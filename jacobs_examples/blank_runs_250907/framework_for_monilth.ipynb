{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "674c6219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovery roots (under jacobs_examples only): 4\n",
      "  • /Users/jacobzwoniarski/Desktop/qae_architectures/jacobs_examples/runs_full_qae_4LT2\n",
      "  • /Users/jacobzwoniarski/Desktop/qae_architectures/jacobs_examples/runs_full_qae_3LT1\n",
      "  • /Users/jacobzwoniarski/Desktop/qae_architectures/jacobs_examples/runs_full_qae_2LT2\n",
      "  • /Users/jacobzwoniarski/Desktop/qae_architectures/jacobs_examples/runs_full_qae_2LT4\n",
      "Monolith models registered: 0 (skipped other JSONs: 0)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0 — Quickstart & discovery (mentor monolith only) ==================\n",
    "import os, glob, json\n",
    "\n",
    "EVAL_SIGMA = 0.10\n",
    "N_EVAL_WINDOWS = 20\n",
    "\n",
    "def _looks_like_monolith_json(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            J = json.load(f)\n",
    "        P = J.get(\"parameters\", {})\n",
    "        return isinstance(P, dict) and any(k in P for k in (\"full_params\",\"weights\",\"params\",\"theta\"))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _discover_monolith(base_dir=\".\", pattern=\"runs_full_qae_*\"):\n",
    "    base_abs = os.path.abspath(base_dir)\n",
    "    roots = [d for d in glob.glob(os.path.join(base_abs, pattern)) if os.path.isdir(d)]\n",
    "    models, skipped = {}, 0\n",
    "    for r in roots:\n",
    "        for p in glob.glob(os.path.join(r, \"**\", \"*.json\"), recursive=True):\n",
    "            if not _looks_like_monolith_json(p):\n",
    "                skipped += 1\n",
    "                continue\n",
    "            tag = os.path.splitext(os.path.basename(p))[0]\n",
    "            k = tag; i = 1\n",
    "            while k in models:\n",
    "                i += 1; k = f\"{tag}_{i}\"\n",
    "            models[k] = os.path.abspath(p)\n",
    "    return roots, models, skipped\n",
    "\n",
    "DISCOVERED_ROOTS, MODEL_PATHS, SKIPPED = _discover_monolith(\".\")\n",
    "\n",
    "print(f\"Discovery roots (under jacobs_examples only): {len(DISCOVERED_ROOTS)}\")\n",
    "for r in DISCOVERED_ROOTS: print(\"  •\", r)\n",
    "print(f\"Monolith models registered: {len(MODEL_PATHS)} (skipped other JSONs: {SKIPPED})\")\n",
    "for name, path in list(MODEL_PATHS.items())[:6]:\n",
    "    print(f\"  ✓ {name:<28s} <- {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20887671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1 — Imports & utilities ===========================================\n",
    "import os, re, json, math, time, hashlib, warnings\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.6, 4.2)\n",
    "\n",
    "# ---- scaling / angles ----\n",
    "def values_to_v01(v, low, high):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    return (v - low) / max(1e-12, (high - low))\n",
    "\n",
    "def v01_to_values(v01, low, high):\n",
    "    v01 = np.asarray(v01, dtype=float)\n",
    "    return low + v01 * (high - low)\n",
    "\n",
    "def readoutZ_to_values(z, low, high):\n",
    "    z = np.clip(np.asarray(z, dtype=float), -0.999999, 0.999999)\n",
    "    v01 = np.arccos(z) / np.pi\n",
    "    return v01_to_values(v01, low, high)\n",
    "\n",
    "def values_to_RY_angles(v, low, high):\n",
    "    return np.pi * values_to_v01(v, low, high)\n",
    "\n",
    "# ---- deterministic noise ----\n",
    "def _stable_seed(tag: str) -> int:\n",
    "    h = hashlib.sha256(tag.encode(\"utf-8\")).digest()\n",
    "    return int.from_bytes(h[:8], \"little\") & 0x7FFFFFFF\n",
    "\n",
    "def add_gaussian_noise_series(series, sigma, low, high, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    noise = rng.normal(0.0, sigma * (high - low), size=series.shape)\n",
    "    return np.clip(series + noise, low, high)\n",
    "\n",
    "# ---- stats ----\n",
    "def bootstrap_ci_mean(x, B=3000, alpha=0.05, rng=None):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0: return (np.nan, np.nan, np.nan)\n",
    "    rng = np.random.default_rng(None if rng is None else rng)\n",
    "    n = x.size\n",
    "    xb = np.empty(B, float)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        xb[b] = np.mean(x[idx])\n",
    "    xb.sort()\n",
    "    lo = xb[int((alpha/2)*B)]\n",
    "    hi = xb[int((1 - alpha/2)*B) - 1]\n",
    "    return float(np.mean(x)), float(lo), float(hi)\n",
    "\n",
    "def sign_test_pvalue(diffs):\n",
    "    diffs = np.asarray(diffs, dtype=float)\n",
    "    wins  = int(np.sum(diffs > 0))\n",
    "    losses= int(np.sum(diffs < 0))\n",
    "    n     = wins + losses\n",
    "    if n == 0: return 1.0\n",
    "    tail = sum(math.comb(n, k) for k in range(0, min(wins, losses)+1)) / (2**n)\n",
    "    return float(min(1.0, 2*tail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebaa6dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready for window sizes: [4, 6]\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2 — Deterministic MG datasets (W=4 & W=6) ==========================\n",
    "NOISE_GRID = (EVAL_SIGMA,)\n",
    "\n",
    "def mackey_glass(length=1300, tau=17, beta=0.2, gamma=0.1, n=10, x0=1.2):\n",
    "    x = np.zeros(length + tau + 1, dtype=float); x[:tau+1] = x0\n",
    "    for t in range(tau, length + tau):\n",
    "        xt = x[t]; xt_tau = x[t - tau]\n",
    "        dx = beta * xt_tau / (1.0 + xt_tau**n) - gamma * xt\n",
    "        x[t+1] = xt + dx\n",
    "    return x[tau+1:]\n",
    "\n",
    "def scale_to_range(y, low=0.2, high=0.8):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    ymin, ymax = float(y.min()), float(y.max())\n",
    "    if ymax - ymin < 1e-12:\n",
    "        return np.full_like(y, (low+high)/2), (low, high)\n",
    "    z = (y - ymin) / (ymax - ymin)\n",
    "    return low + z * (high - low), (low, high)\n",
    "\n",
    "def make_windows(ts, size, step):\n",
    "    ts = np.asarray(ts, dtype=float)\n",
    "    return np.array([ts[i:i+size] for i in range(0, len(ts)-size+1, step)], dtype=float)\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    name: str\n",
    "    clean: np.ndarray\n",
    "    scale_low: float\n",
    "    scale_high: float\n",
    "    windows_clean: np.ndarray\n",
    "    split_idx: int\n",
    "    noisy_series_by_sigma: dict\n",
    "    noisy_windows_by_sigma: dict\n",
    "\n",
    "def _build_for_window_size(window_size=4, window_step=1, noise_grid=NOISE_GRID):\n",
    "    mgA = mackey_glass(length=1300, tau=17)\n",
    "    mgB = mackey_glass(length=1300, tau=30)\n",
    "    sA,(loA,hiA) = scale_to_range(mgA, 0.2, 0.8)\n",
    "    sB,(loB,hiB) = scale_to_range(mgB, 0.2, 0.8)\n",
    "    winA = make_windows(sA, window_size, window_step)\n",
    "    winB = make_windows(sB, window_size, window_step)\n",
    "    splitA = int(0.75 * len(winA)); splitB = int(0.75 * len(winB))\n",
    "\n",
    "    def make_maps(name, scaled, lo, hi):\n",
    "        series_map, win_map = {}, {}\n",
    "        for sigma in noise_grid:\n",
    "            seed = _stable_seed(f\"{name}|sigma={sigma:.3f}|W={window_size}|v1\")\n",
    "            noisy = add_gaussian_noise_series(scaled, sigma, lo, hi, seed)\n",
    "            series_map[round(sigma,3)] = noisy\n",
    "            win_map[round(sigma,3)] = make_windows(noisy, window_size, window_step)\n",
    "        return series_map, win_map\n",
    "\n",
    "    nsA_s, nsA_w = make_maps(\"MG_A\", sA, loA, hiA)\n",
    "    nsB_s, nsB_w = make_maps(\"MG_B\", sB, loB, hiB)\n",
    "\n",
    "    return {\n",
    "        \"MG_A\": Dataset(\"MG_A\", sA, loA, hiA, winA, splitA, nsA_s, nsA_w),\n",
    "        \"MG_B\": Dataset(\"MG_B\", sB, loB, hiB, winB, splitB, nsB_s, nsB_w),\n",
    "    }\n",
    "\n",
    "WINDOW_STEP = 1\n",
    "DATASETS_BY_W = {\n",
    "    4: _build_for_window_size(4, WINDOW_STEP, NOISE_GRID),\n",
    "    6: _build_for_window_size(6, WINDOW_STEP, NOISE_GRID),\n",
    "}\n",
    "print(\"Datasets ready for window sizes:\", list(DATASETS_BY_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e3ef1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3 - Monolith QAE================\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "def values_to_v01(v, low, high):\n",
    "    v = np.asarray(v, dtype=float); return (v - low) / max(1e-12, (high - low))\n",
    "def values_to_RY_angles(v, low, high): return np.pi * values_to_v01(v, low, high)\n",
    "def readoutZ_to_values(z, low, high):\n",
    "    z = np.clip(np.asarray(z, dtype=float), -0.999999, 0.999999)\n",
    "    v01 = np.arccos(z) / np.pi\n",
    "    return low + v01 * (high - low)\n",
    "\n",
    "class MonolithQAE:\n",
    "    def __init__(self, *, n_qubits, n_layers, n_latent, n_trash,\n",
    "                 scale_low, scale_high, full_params, rot=\"Rxyz\"):\n",
    "        self.n_qubits = int(n_qubits)\n",
    "        self.n_layers = int(n_layers)\n",
    "        self.n_latent = int(n_latent)\n",
    "        self.n_trash  = int(n_trash)\n",
    "        self.scale_low  = float(scale_low)\n",
    "        self.scale_high = float(scale_high)\n",
    "        self.rot = str(rot)  # we’ll still infer from param length below if needed\n",
    "\n",
    "        # wires\n",
    "        self.latent_wires = list(range(self.n_latent))\n",
    "        self.trash_wires  = list(range(self.n_latent, self.n_latent + self.n_trash))\n",
    "        self.data_wires   = self.latent_wires + self.trash_wires\n",
    "        self.anz_wires    = self.data_wires  # no extras in our runs\n",
    "\n",
    "        # split params into [encoder | decoder] HALVES (this matches training!)\n",
    "        fp = np.asarray(full_params, dtype=float).ravel()\n",
    "        # try Rxyz split\n",
    "        need_rxyz = 2 * self.n_layers * self.n_qubits * 3\n",
    "        need_ry   = 2 * self.n_layers * self.n_qubits\n",
    "        if fp.size == need_rxyz:\n",
    "            self.rot = \"Rxyz\"\n",
    "            half = fp.size // 2\n",
    "            enc = fp[:half].reshape(self.n_layers, self.n_qubits, 3)\n",
    "            dec = fp[half:].reshape(self.n_layers, self.n_qubits, 3)\n",
    "        elif fp.size == need_ry:\n",
    "            self.rot = \"Ry\"\n",
    "            half = fp.size // 2\n",
    "            enc = fp[:half].reshape(self.n_layers, self.n_qubits)\n",
    "            dec = fp[half:].reshape(self.n_layers, self.n_qubits)\n",
    "        else:\n",
    "            raise ValueError(f\"Param length {fp.size} not equal to 2*L*Q*(3 or 1). \"\n",
    "                             f\"L={self.n_layers}, Q={self.n_qubits}\")\n",
    "\n",
    "        self.enc_weights = pnp.asarray(enc, requires_grad=False)\n",
    "        self.dec_weights = pnp.asarray(dec, requires_grad=False)\n",
    "\n",
    "        self.dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=None)\n",
    "        self._q_fwd = self._q_lat = self._q_tr = None\n",
    "\n",
    "    # --- building blocks (identical to mentor training) ---\n",
    "    def _sequence_encoder(self, wires, angles):\n",
    "        # angles already in radians\n",
    "        n = len(angles)\n",
    "        for w in wires:\n",
    "            th = float(angles[w]) if w < n else 0.0\n",
    "            qml.RY(th, wires=w)\n",
    "\n",
    "    def _entangler(self, weights):\n",
    "        if self.rot == \"Ry\":\n",
    "            qml.BasicEntanglerLayers(weights, wires=self.anz_wires, rotation=qml.RY)\n",
    "        else:\n",
    "            qml.StronglyEntanglingLayers(weights, wires=self.anz_wires)\n",
    "\n",
    "    def _encoder_block(self, enc_w):\n",
    "        self._entangler(enc_w)\n",
    "\n",
    "    def _decoder_block(self, dec_w):\n",
    "        # IMPORTANT: adjoint of the *decoder* block weights (second half),\n",
    "        # exactly like training (not shared with encoder).\n",
    "        qml.adjoint(self._entangler)(dec_w)\n",
    "\n",
    "    def _embed_from_values(self, values_window):\n",
    "        self._sequence_encoder(self.data_wires,\n",
    "                               values_to_RY_angles(values_window, self.scale_low, self.scale_high))\n",
    "\n",
    "    # --- QNodes (no grads needed) ---\n",
    "    def _build_forward(self):\n",
    "        @qml.qnode(self.dev, diff_method=None)\n",
    "        def qnode(values_window):\n",
    "            self._embed_from_values(values_window)\n",
    "            self._encoder_block(self.enc_weights)\n",
    "            # reset trash\n",
    "            for w in self.trash_wires:\n",
    "                qml.measure(w, reset=True)\n",
    "            # decoder = adjoint(dec-block)\n",
    "            self._decoder_block(self.dec_weights)\n",
    "            # no output de-encoder\n",
    "            return [qml.expval(qml.PauliZ(w)) for w in self.data_wires]\n",
    "        return qnode\n",
    "\n",
    "    def _build_latents_after_encoder(self):\n",
    "        @qml.qnode(self.dev, diff_method=None)\n",
    "        def qnode(values_window):\n",
    "            self._embed_from_values(values_window)\n",
    "            self._encoder_block(self.enc_weights)\n",
    "            return [qml.expval(qml.PauliZ(w)) for w in self.latent_wires]\n",
    "        return qnode\n",
    "\n",
    "    def _build_trash_probs_after_encoder(self):\n",
    "        @qml.qnode(self.dev, diff_method=None)\n",
    "        def qnode(values_window):\n",
    "            self._embed_from_values(values_window)\n",
    "            self._encoder_block(self.enc_weights)\n",
    "            return qml.probs(wires=self.trash_wires)\n",
    "        return qnode\n",
    "\n",
    "    # public API\n",
    "    def forward_expZ(self, values_window):\n",
    "        if self._q_fwd is None: self._q_fwd = self._build_forward()\n",
    "        return np.asarray(self._q_fwd(values_window), dtype=float)\n",
    "\n",
    "    def latents_after_encoder_expZ(self, values_window):\n",
    "        if self._q_lat is None: self._q_lat = self._build_latents_after_encoder()\n",
    "        return np.asarray(self._q_lat(values_window), dtype=float)\n",
    "\n",
    "    def trash_probs_after_encoder(self, values_window):\n",
    "        if self._q_tr is None: self._q_tr = self._build_trash_probs_after_encoder()\n",
    "        return np.asarray(self._q_tr(values_window), dtype=float)\n",
    "\n",
    "    def map_expZ_to_values(self, z_all):\n",
    "        return readoutZ_to_values(z_all, self.scale_low, self.scale_high)\n",
    "\n",
    "    def predict_values(self, values_window):\n",
    "        return self.map_expZ_to_values(self.forward_expZ(values_window))\n",
    "\n",
    "    def describe(self):\n",
    "        print(f\"Mentor Monolith QAE — nq={self.n_qubits}, L={self.n_layers}, \"\n",
    "              f\"latent={self.n_latent}, trash={self.n_trash}, rot={self.rot}, \"\n",
    "              f\"decoder=adjoint(dec-block), scale=[{self.scale_low},{self.scale_high}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5975d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4 — Loader for monolith JSON bundles ========================\n",
    "# Expected:\n",
    "# {\n",
    "#   \"dataset\": {\"scale_low\":..., \"scale_high\":..., \"window_size\": nq, \"window_stride\": ...},\n",
    "#   \"architecture\": {\"n_qubits\":..., \"n_layers\":..., \"n_latent\":..., \"n_trash\":..., \"decoder_mode\": \"...\"},\n",
    "#   \"run\": {\"sigma_train\": ...},\n",
    "#   \"parameters\": {\"full_params\": [...]}\n",
    "# }\n",
    "\n",
    "import os, re, json\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "_STD_RE = re.compile(r\"(?P<nq>\\d+)q_(?P<nl>\\d+)l_(?P<nt>\\d+)t_(?P<L>\\d+)ls_(?P<inst>\\d+)\\.json$\", re.IGNORECASE)\n",
    "def parse_std_filename(path):\n",
    "    m=_STD_RE.search(os.path.basename(path))\n",
    "    if not m: return {}\n",
    "    d={k:int(v) for k,v in m.groupdict().items()}\n",
    "    return {\"n_qubits\":d[\"nq\"], \"n_latent\":d[\"nl\"], \"n_trash\":d[\"nt\"], \"n_layers\":d[\"L\"], \"instance\":d[\"inst\"]}\n",
    "\n",
    "def parse_model_json_monolith(path, default_rot=\"Rxyz\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        J = json.load(f)\n",
    "\n",
    "    P = J.get(\"parameters\", {})\n",
    "    full_params = None\n",
    "    for key in (\"full_params\",\"weights\",\"params\",\"theta\"):\n",
    "        if key in P and isinstance(P[key], (list,tuple)):\n",
    "            full_params = np.asarray(P[key], dtype=float).ravel()\n",
    "            break\n",
    "    if full_params is None:\n",
    "        raise ValueError(f\"{os.path.basename(path)}: no parameters.full_params-like array found\")\n",
    "\n",
    "    arch = J.get(\"architecture\", {})\n",
    "    ds   = J.get(\"dataset\", {})\n",
    "    run  = J.get(\"run\", {})\n",
    "\n",
    "    fn = parse_std_filename(path)\n",
    "    n_qubits = int(arch.get(\"n_qubits\", ds.get(\"window_size\", fn.get(\"n_qubits\", 4))))\n",
    "    n_layers = int(arch.get(\"n_layers\", fn.get(\"n_layers\", 1)))\n",
    "    n_latent = int(arch.get(\"n_latent\", fn.get(\"n_latent\", max(1, n_qubits//2))))\n",
    "    n_trash  = int(arch.get(\"n_trash\",  fn.get(\"n_trash\", n_qubits - n_latent)))\n",
    "    rot      = str(arch.get(\"rot\", default_rot))\n",
    "\n",
    "    scale_low  = float(ds.get(\"scale_low\", 0.2))\n",
    "    scale_high = float(ds.get(\"scale_high\", 0.8))\n",
    "    sigma_train = float(run.get(\"sigma_train\", np.nan))\n",
    "\n",
    "    info = dict(n_qubits=n_qubits, n_layers=n_layers, n_latent=n_latent, n_trash=n_trash,\n",
    "                scale_low=scale_low, scale_high=scale_high, rot=rot, sigma_train=sigma_train,\n",
    "                filename=os.path.basename(path))\n",
    "    return {\"full_params\": full_params, \"info\": info}\n",
    "\n",
    "@dataclass\n",
    "class ModelEntry:\n",
    "    name: str\n",
    "    path: str\n",
    "    n_qubits: int\n",
    "    n_latent: int\n",
    "    n_trash: int\n",
    "    scale_low: float\n",
    "    scale_high: float\n",
    "    noise_sigma_train: float\n",
    "    meta: dict\n",
    "    impl: object\n",
    "\n",
    "def instantiate_model_monolith(parsed):\n",
    "    i = parsed[\"info\"]\n",
    "    return MonolithQAE(\n",
    "        n_qubits=i[\"n_qubits\"], n_layers=i[\"n_layers\"],\n",
    "        n_latent=i[\"n_latent\"], n_trash=i[\"n_trash\"],\n",
    "        scale_low=i[\"scale_low\"], scale_high=i[\"scale_high\"],\n",
    "        full_params=parsed[\"full_params\"], rot=i[\"rot\"]\n",
    "    )\n",
    "\n",
    "def load_models_monolith(model_paths: dict):\n",
    "    reg={}\n",
    "    for name, path in model_paths.items():\n",
    "        try:\n",
    "            P = parse_model_json_monolith(path)\n",
    "            impl = instantiate_model_monolith(P)\n",
    "            i = P[\"info\"]\n",
    "            reg[name] = ModelEntry(\n",
    "                name=name, path=path, n_qubits=i[\"n_qubits\"], n_latent=i[\"n_latent\"], n_trash=i[\"n_trash\"],\n",
    "                scale_low=i[\"scale_low\"], scale_high=i[\"scale_high\"],\n",
    "                noise_sigma_train=i[\"sigma_train\"], meta=i, impl=impl\n",
    "            )\n",
    "            print(f\"✓ Loaded {name:<22s} file={i['filename']}  \"\n",
    "                  f\"(nq={i['n_qubits']}, L={i['n_layers']}, latent={i['n_latent']}, \"\n",
    "                  f\"trash={i['n_trash']}, rot={i['rot']})\")\n",
    "            impl.describe()\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {name}: {e}\")\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c2b9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5 — Evaluation core (metrics we need) ==============================\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    noisy_mse: np.ndarray\n",
    "    model_mse: np.ndarray\n",
    "    delta_pct: np.ndarray\n",
    "    delta_pct_sym: np.ndarray\n",
    "    recon_values: np.ndarray\n",
    "    noisy_values: np.ndarray\n",
    "    clean_values: np.ndarray\n",
    "    # --- extras for monolith diagnostics ---\n",
    "    lat_clean: np.ndarray | None = None\n",
    "    lat_noisy: np.ndarray | None = None\n",
    "    p00_trash: np.ndarray | None = None\n",
    "\n",
    "def eval_model_on_dataset(entry, ds, n_eval=20, sigma_override=EVAL_SIGMA):\n",
    "    s_key = float(np.round(float(sigma_override), 3))\n",
    "    if s_key not in ds.noisy_windows_by_sigma:\n",
    "        available = \", \".join([f\"{k:.3f}\" for k in sorted(ds.noisy_windows_by_sigma)])\n",
    "        raise ValueError(f\"{ds.name}: σ={s_key:.3f} not in [{available}]\")\n",
    "    cleanW = ds.windows_clean[ds.split_idx:]\n",
    "    noisyW = ds.noisy_windows_by_sigma[s_key][ds.split_idx:]\n",
    "    N = min(int(n_eval), len(cleanW))\n",
    "    cleanW = cleanW[:N]; noisyW = noisyW[:N]\n",
    "\n",
    "    impl = entry.impl\n",
    "    recon, nmse, dmse, d_pct, d_sym = [], [], [], [], []\n",
    "    lat_c, lat_n, p00 = [], [], []\n",
    "\n",
    "    for w_clean, w_noisy in zip(cleanW, noisyW):\n",
    "        z = impl.forward_expZ(w_noisy)\n",
    "        y = impl.map_expZ_to_values(z)\n",
    "        recon.append(y)\n",
    "\n",
    "        mse_n = float(np.mean((w_clean - w_noisy)**2))\n",
    "        mse_d = float(np.mean((w_clean - y     )**2))\n",
    "        nmse.append(mse_n); dmse.append(mse_d)\n",
    "        d_pct.append(0.0 if mse_n<1e-12 else 100.0*(mse_n - mse_d)/mse_n)\n",
    "        d_sym.append(200.0*(mse_n - mse_d)/max(mse_n + mse_d, 1e-12))\n",
    "\n",
    "        # diagnostics (guarded)\n",
    "        try:\n",
    "            lc = impl.latents_after_encoder_expZ(w_clean)\n",
    "            ln = impl.latents_after_encoder_expZ(w_noisy)\n",
    "            lat_c.append(lc); lat_n.append(ln)\n",
    "        except Exception:\n",
    "            lat_c = lat_n = None\n",
    "        try:\n",
    "            P = impl.trash_probs_after_encoder(w_noisy)  # length = 2^(n_trash)\n",
    "            p00.append(float(P[0]))  # |00..0> probability\n",
    "        except Exception:\n",
    "            p00 = None\n",
    "\n",
    "    return EvalResult(\n",
    "        noisy_mse=np.asarray(nmse), model_mse=np.asarray(dmse),\n",
    "        delta_pct=np.asarray(d_pct), delta_pct_sym=np.asarray(d_sym),\n",
    "        recon_values=np.asarray(recon), noisy_values=noisyW, clean_values=cleanW,\n",
    "        lat_clean=None if lat_c is None else np.asarray(lat_c),\n",
    "        lat_noisy=None if lat_n is None else np.asarray(lat_n),\n",
    "        p00_trash=None if p00 is None else np.asarray(p00),\n",
    "    )\n",
    "\n",
    "def summarize_eval(er: EvalResult):\n",
    "    mean_imp, lo_imp, hi_imp = bootstrap_ci_mean(er.delta_pct)\n",
    "    mean_sym, lo_sym, hi_sym = bootstrap_ci_mean(er.delta_pct_sym)\n",
    "    p_val = sign_test_pvalue(er.noisy_mse - er.model_mse)\n",
    "    succ  = 100.0 * float(np.mean(er.delta_pct > 0))\n",
    "    return {\n",
    "        \"noisy_MSE_mean\": float(np.mean(er.noisy_mse)),\n",
    "        \"model_MSE_mean\": float(np.mean(er.model_mse)),\n",
    "        \"delta_pct_mean\": float(mean_imp),\n",
    "        \"delta_pct_CI95\": [float(lo_imp), float(hi_imp)],\n",
    "        \"delta_pct_sym_mean\": float(mean_sym),\n",
    "        \"delta_pct_sym_CI95\": [float(lo_sym), float(hi_sym)],\n",
    "        \"sign_test_p\": float(p_val),\n",
    "        \"success_rate_pct\": float(succ),\n",
    "        \"n_windows\": int(er.noisy_mse.size)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "590e9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6 — Run benchmark (window-size aware) ==============================\n",
    "REG = load_models_monolith(MODEL_PATHS)\n",
    "\n",
    "def run_benchmark(model_registry, n_eval=N_EVAL_WINDOWS, sigma_override=EVAL_SIGMA):\n",
    "    all_results, all_summaries = {}, {}\n",
    "    for name, entry in model_registry.items():\n",
    "        w = int(entry.n_qubits)\n",
    "        if w not in DATASETS_BY_W:\n",
    "            warnings.warn(f\"No dataset for window_size={w}; skipping {name}.\"); continue\n",
    "        for dsname, ds in DATASETS_BY_W[w].items():\n",
    "            er = eval_model_on_dataset(entry, ds, n_eval=n_eval, sigma_override=sigma_override)\n",
    "            sm = summarize_eval(er)\n",
    "            all_results.setdefault(dsname, {})[name]  = er\n",
    "            all_summaries.setdefault(dsname, {})[name]= sm\n",
    "            print(f\"{dsname} | {name:24s} Δ%={sm['delta_pct_mean']:+5.1f} \"\n",
    "                  f\"(CI {sm['delta_pct_CI95'][0]:+.1f},{sm['delta_pct_CI95'][1]:+.1f})  \"\n",
    "                  f\"succ={sm['success_rate_pct']:4.1f}%  p={sm['sign_test_p']:.4f}\")\n",
    "    return all_results, all_summaries\n",
    "\n",
    "ALL_RESULTS, ALL_SUMMARIES = run_benchmark(REG, n_eval=N_EVAL_WINDOWS, sigma_override=EVAL_SIGMA)\n",
    "\n",
    "# quick table\n",
    "if ALL_SUMMARIES:\n",
    "    rows=[]\n",
    "    for dsname, d in ALL_SUMMARIES.items():\n",
    "        for name, sm in d.items():\n",
    "            e = REG[name]\n",
    "            rows.append({\n",
    "                \"dataset\": dsname, \"model\": name,\n",
    "                \"n_qubits\": e.n_qubits, \"n_latent\": e.n_latent, \"n_trash\": e.n_trash,\n",
    "                \"layers\": e.meta.get(\"n_layers\", None),\n",
    "                \"delta_pct_mean\": sm[\"delta_pct_mean\"],\n",
    "                \"delta_pct_lo\": sm[\"delta_pct_CI95\"][0],\n",
    "                \"delta_pct_hi\": sm[\"delta_pct_CI95\"][1],\n",
    "                \"success_rate_pct\": sm[\"success_rate_pct\"],\n",
    "                \"p\": sm[\"sign_test_p\"],\n",
    "            })\n",
    "    df = pd.DataFrame(rows).sort_values([\"dataset\",\"n_qubits\",\"layers\",\"model\"])\n",
    "    print(\"\\n== Summary (head) ==\"); print(df.head(12).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16abe379",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No eval summaries collected. Did models load & run?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m eval_long \u001b[38;5;241m=\u001b[39m _eval_long_df(ALL_SUMMARIES, REG)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_long\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo eval summaries collected. Did models load & run?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# pivot to columns per dataset\u001b[39;00m\n\u001b[1;32m     42\u001b[0m pivot \u001b[38;5;241m=\u001b[39m eval_long\u001b[38;5;241m.\u001b[39mpivot_table(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m                               values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_pct_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess_rate_pct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msign_test_p\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     44\u001b[0m                               aggfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No eval summaries collected. Did models load & run?"
     ]
    }
   ],
   "source": [
    "# === Cell 7 — Merge eval columns into your training CSVs =====================\n",
    "import os, glob\n",
    "\n",
    "# Find a common root to search CSVs\n",
    "if DISCOVERED_ROOTS:\n",
    "    try:\n",
    "        JACOB_ROOT = os.path.commonpath([os.path.dirname(r) for r in DISCOVERED_ROOTS])\n",
    "    except ValueError:\n",
    "        JACOB_ROOT = os.path.dirname(DISCOVERED_ROOTS[0])\n",
    "else:\n",
    "    JACOB_ROOT = os.path.abspath(\".\")\n",
    "\n",
    "def find_many(root, pattern):\n",
    "    return sorted(glob.glob(os.path.join(root, \"**\", pattern), recursive=True))\n",
    "\n",
    "# Build a long eval frame keyed by filename (your saver writes that)\n",
    "def _eval_long_df(all_summaries, registry):\n",
    "    rows=[]\n",
    "    for dsname, bymodel in all_summaries.items():\n",
    "        for name, sm in bymodel.items():\n",
    "            e = registry[name]\n",
    "            rows.append({\n",
    "                \"filename\": os.path.basename(e.path),\n",
    "                \"model\": name,\n",
    "                \"dataset\": dsname,\n",
    "                \"n_qubits\": e.n_qubits,\n",
    "                \"n_layers\": e.meta.get(\"n_layers\", None),\n",
    "                \"n_latent\": e.n_latent,\n",
    "                \"n_trash\": e.n_trash,\n",
    "                \"delta_pct_mean\": sm[\"delta_pct_mean\"],\n",
    "                \"success_rate_pct\": sm[\"success_rate_pct\"],\n",
    "                \"sign_test_p\": sm[\"sign_test_p\"],\n",
    "                \"n_windows\": sm[\"n_windows\"],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "eval_long = _eval_long_df(ALL_SUMMARIES, REG)\n",
    "if eval_long.empty:\n",
    "    raise RuntimeError(\"No eval summaries collected. Did models load & run?\")\n",
    "\n",
    "# pivot to columns per dataset\n",
    "pivot = eval_long.pivot_table(index=\"filename\", columns=\"dataset\",\n",
    "                              values=[\"delta_pct_mean\",\"success_rate_pct\",\"sign_test_p\"],\n",
    "                              aggfunc=\"first\")\n",
    "pivot.columns = [f\"{a}_{b}\" for a,b in pivot.columns.to_flat_index()]\n",
    "pivot = pivot.reset_index()\n",
    "\n",
    "# add AVG columns\n",
    "for base in [\"delta_pct_mean\",\"success_rate_pct\",\"sign_test_p\"]:\n",
    "    cols = [c for c in pivot.columns if c.startswith(base+\"_\")]\n",
    "    if cols:\n",
    "        pivot[f\"{base}_AVG\"] = pivot[cols].mean(axis=1, skipna=True)\n",
    "\n",
    "# locate CSVs\n",
    "train_csvs  = find_many(JACOB_ROOT, \"all_training_instances_*.csv\")\n",
    "layer_csvs  = find_many(JACOB_ROOT, \"summary_by_layers_*.csv\")\n",
    "print(f\"\\nFound {len(train_csvs)} training-instance CSV(s) and {len(layer_csvs)} layer-summary CSV(s).\")\n",
    "\n",
    "# merge into every training CSV (write *_with_eval.csv)\n",
    "train_outputs = []\n",
    "for csv_in in train_csvs:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_in, low_memory=False)\n",
    "        if \"filename\" not in df.columns:\n",
    "            print(f\"  ! Skipping (no 'filename' column): {csv_in}\")\n",
    "            continue\n",
    "        merged = df.merge(pivot, on=\"filename\", how=\"left\")\n",
    "        # coerce new numeric columns\n",
    "        for c in merged.columns:\n",
    "            if any(c.startswith(k) for k in [\"delta_pct_mean_\",\"success_rate_pct_\",\"sign_test_p_\"]) or c.endswith(\"_AVG\"):\n",
    "                merged[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n",
    "        csv_out = csv_in.replace(\".csv\", \"_with_eval.csv\")\n",
    "        merged.to_csv(csv_out, index=False)\n",
    "        train_outputs.append(csv_out)\n",
    "        print(f\"  ✓ Wrote training+eval → {csv_out}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {csv_in}: {e}\")\n",
    "\n",
    "# Build layer-level aggregates (fallback from eval_long if needed)\n",
    "def _mean_std_str(s):\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    return \"n/a\" if v.size == 0 else f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "def build_layer_eval_from_eval_long(eval_long_df):\n",
    "    info = (eval_long_df[[\"filename\",\"n_layers\"]]\n",
    "            .drop_duplicates()\n",
    "            .set_index(\"filename\"))\n",
    "    wide = eval_long_df.pivot_table(index=\"filename\",\n",
    "                                    columns=\"dataset\",\n",
    "                                    values=[\"delta_pct_mean\",\"success_rate_pct\",\"sign_test_p\"],\n",
    "                                    aggfunc=\"first\")\n",
    "    wide.columns = [f\"{a}_{b}\" for a,b in wide.columns.to_flat_index()]\n",
    "    wide = wide.reset_index().merge(info, on=\"filename\", how=\"left\")\n",
    "    def grp_stat(s): return _mean_std_str(s)\n",
    "    layer_eval = (wide.groupby(\"n_layers\", dropna=False)\n",
    "                       .apply(lambda t: pd.Series({\n",
    "                           \"delta_MG_A\": grp_stat(t.get(\"delta_pct_mean_MG_A\")),\n",
    "                           \"delta_MG_B\": grp_stat(t.get(\"delta_pct_mean_MG_B\")),\n",
    "                           \"succ_MG_A\":  grp_stat(t.get(\"success_rate_pct_MG_A\")),\n",
    "                           \"succ_MG_B\":  grp_stat(t.get(\"success_rate_pct_MG_B\")),\n",
    "                           \"p50_MG_A\":   f\"{pd.to_numeric(t.get('sign_test_p_MG_A'), errors='coerce').median():.6f}\",\n",
    "                           \"p50_MG_B\":   f\"{pd.to_numeric(t.get('sign_test_p_MG_B'), errors='coerce').median():.6f}\",\n",
    "                       }))\n",
    "                       .reset_index())\n",
    "    return layer_eval\n",
    "\n",
    "layer_outputs = []\n",
    "layer_eval_fallback = build_layer_eval_from_eval_long(eval_long)\n",
    "for csv_in in layer_csvs:\n",
    "    try:\n",
    "        layer_df = pd.read_csv(csv_in)\n",
    "        if \"n_layers\" not in layer_df.columns and layer_df.index.name == \"n_layers\":\n",
    "            layer_df = layer_df.reset_index()\n",
    "        merged_layer = layer_df.merge(layer_eval_fallback, on=\"n_layers\", how=\"left\")\n",
    "        csv_out = csv_in.replace(\".csv\", \"_with_eval.csv\")\n",
    "        merged_layer.to_csv(csv_out, index=False)\n",
    "        layer_outputs.append(csv_out)\n",
    "        print(f\"  ✓ Wrote layer+eval → {csv_out}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {csv_in}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4529584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8 — Minimal visuals (optional) =====================================\n",
    "if REG:\n",
    "    # pick the first loaded model and show one example per matching dataset\n",
    "    name0, entry0 = next(iter(REG.items()))\n",
    "    w0 = entry0.n_qubits\n",
    "    dsA = DATASETS_BY_W[w0][\"MG_A\"]; dsB = DATASETS_BY_W[w0][\"MG_B\"]\n",
    "    def plot_example(er, title):\n",
    "        c,n,d = er.clean_values[0], er.noisy_values[0], er.recon_values[0]\n",
    "        xs = np.arange(len(c))\n",
    "        plt.figure(); plt.plot(xs,c,label=\"clean\"); plt.plot(xs,n,label=\"noisy\"); plt.plot(xs,d,label=\"deno\")\n",
    "        plt.title(title); plt.xlabel(\"t\"); plt.ylabel(\"value\"); plt.legend(); plt.show()\n",
    "    erA = eval_model_on_dataset(entry0, dsA, n_eval=N_EVAL_WINDOWS, sigma_override=EVAL_SIGMA)\n",
    "    erB = eval_model_on_dataset(entry0, dsB, n_eval=N_EVAL_WINDOWS, sigma_override=EVAL_SIGMA)\n",
    "    plot_example(erA, f\"{name0} — MG_A (σ={EVAL_SIGMA:g})\")\n",
    "    plot_example(erB, f\"{name0} — MG_B (σ={EVAL_SIGMA:g})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4be17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9 — Plotting & extra analytics (monolith) ==========================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_reconstruction_example(er, idx=0, title=\"Reconstruction\"):\n",
    "    c, n, d = er.clean_values[idx], er.noisy_values[idx], er.recon_values[idx]\n",
    "    xs = np.arange(len(c))\n",
    "    plt.figure()\n",
    "    plt.plot(xs, c, label=\"clean\")\n",
    "    plt.plot(xs, n, label=\"noisy\")\n",
    "    plt.plot(xs, d, label=\"denoised\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_delta_distributions(er_by_model, title=\"ΔMSE% (per-window)\"):\n",
    "    plt.figure()\n",
    "    for name, er in er_by_model.items():\n",
    "        plt.hist(er.delta_pct, bins=12, alpha=0.5, label=name)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"improvement %\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_robustness(entry, ds, sigmas=(0.10,), n_eval=N_EVAL_WINDOWS):\n",
    "    means, lows, highs, xs = [], [], [], []\n",
    "    avail = set(ds.noisy_windows_by_sigma.keys())\n",
    "    for s in sigmas:\n",
    "        s_key = float(np.round(float(s), 3))\n",
    "        if s_key not in avail:\n",
    "            print(f\"[robustness] skip σ={s_key:.3f} (available: {sorted(avail)})\")\n",
    "            continue\n",
    "        er = eval_model_on_dataset(entry, ds, n_eval=n_eval, sigma_override=s_key)\n",
    "        m, lo, hi = bootstrap_ci_mean(er.delta_pct)\n",
    "        xs.append(float(s_key)); means.append(m); lows.append(lo); highs.append(hi)\n",
    "    if not xs:\n",
    "        return\n",
    "    plt.figure()\n",
    "    plt.plot(xs, means, marker=\"o\")\n",
    "    plt.fill_between(xs, lows, highs, alpha=0.2)\n",
    "    plt.xlabel(\"σ\")\n",
    "    plt.ylabel(\"ΔMSE% (mean, 95% CI)\")\n",
    "    plt.title(f\"Robustness on {ds.name} — {entry.name}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_latent_trash(er, entry, dsname):\n",
    "    if getattr(er, \"lat_clean\", None) is not None and getattr(er, \"lat_noisy\", None) is not None:\n",
    "        diffs = np.mean((er.lat_clean - er.lat_noisy)**2, axis=1)\n",
    "        plt.figure()\n",
    "        plt.hist(diffs, bins=12)\n",
    "        plt.title(f\"Latent drift MSE — {entry.name} on {dsname}\")\n",
    "        plt.xlabel(\"MSE\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.show()\n",
    "    if getattr(er, \"p00_trash\", None) is not None:\n",
    "        plt.figure()\n",
    "        plt.hist(er.p00_trash, bins=12)\n",
    "        plt.title(f\"Trash P(|00⟩) after encoder — {entry.name} on {dsname}\")\n",
    "        plt.xlabel(\"P00\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.show()\n",
    "\n",
    "# Tables & heatmaps\n",
    "def summaries_to_df(all_summaries, registry):\n",
    "    rows = []\n",
    "    for dsname, bymodel in all_summaries.items():\n",
    "        for name, sm in bymodel.items():\n",
    "            e = registry[name]\n",
    "            rows.append({\n",
    "                \"dataset\": dsname,\n",
    "                \"model\": name,\n",
    "                \"n_qubits\": e.n_qubits,\n",
    "                \"n_latent\": e.n_latent,\n",
    "                \"n_trash\": e.n_trash,\n",
    "                \"layers\": e.meta.get(\"n_layers\", getattr(e.impl, \"n_layers\", None)),\n",
    "                \"delta_pct_mean\": sm[\"delta_pct_mean\"],\n",
    "                \"delta_pct_lo\": sm[\"delta_pct_CI95\"][0],\n",
    "                \"delta_pct_hi\": sm[\"delta_pct_CI95\"][1],\n",
    "                \"success_rate_pct\": sm[\"success_rate_pct\"],\n",
    "                \"p\": sm[\"sign_test_p\"],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def heatmap_by_arch(df, dataset, value_col=\"delta_pct_mean\"):\n",
    "    sub = df[df[\"dataset\"] == dataset]\n",
    "    for (nl, nt), group in sub.groupby([\"n_latent\", \"n_trash\"]):\n",
    "        piv = group.pivot_table(index=\"n_qubits\", columns=\"layers\", values=value_col, aggfunc=\"mean\")\n",
    "        if piv.size == 0:\n",
    "            continue\n",
    "        plt.figure(figsize=(5.2, 3.6))\n",
    "        plt.imshow(piv.values, aspect=\"auto\")\n",
    "        plt.xticks(range(piv.shape[1]), piv.columns)\n",
    "        plt.yticks(range(piv.shape[0]), piv.index)\n",
    "        plt.colorbar(label=value_col)\n",
    "        plt.title(f\"{dataset} — mean {value_col} (latent={nl}, trash={nt})\")\n",
    "        plt.xlabel(\"layers\")\n",
    "        plt.ylabel(\"n_qubits\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Full-series tools\n",
    "def _flatten_avg(windows, step):\n",
    "    N, W = windows.shape\n",
    "    L = (N - 1) * step + W\n",
    "    acc = np.zeros(L)\n",
    "    cnt = np.zeros(L)\n",
    "    for i in range(N):\n",
    "        s = i * step\n",
    "        e = s + W\n",
    "        acc[s:e] += windows[i]\n",
    "        cnt[s:e] += 1\n",
    "    return acc / np.maximum(cnt, 1e-12)\n",
    "\n",
    "def reconstruct_full_series(entry, ds, sigma_override=EVAL_SIGMA, step=1):\n",
    "    s_key = float(np.round(float(sigma_override), 3))\n",
    "    if s_key not in ds.noisy_windows_by_sigma:\n",
    "        avail = \", \".join([f\"{k:.3f}\" for k in sorted(ds.noisy_windows_by_sigma)])\n",
    "        raise ValueError(f\"{ds.name}: σ={s_key:.3f} not in [{avail}]\")\n",
    "    cleanW = np.asarray(ds.windows_clean, dtype=float)\n",
    "    noisyW = np.asarray(ds.noisy_windows_by_sigma[s_key], dtype=float)\n",
    "    preds = [entry.impl.map_expZ_to_values(entry.impl.forward_expZ(w)) for w in noisyW]\n",
    "    preds = np.asarray(preds)\n",
    "    flat_c = _flatten_avg(cleanW, step)\n",
    "    flat_n = _flatten_avg(noisyW, step)\n",
    "    flat_d = _flatten_avg(preds, step)\n",
    "    mse_n = float(np.mean((flat_c - flat_n)**2))\n",
    "    mse_d = float(np.mean((flat_c - flat_d)**2))\n",
    "    d_pct = 0.0 if mse_n < 1e-12 else 100.0 * (mse_n - mse_d) / mse_n\n",
    "    return {\n",
    "        \"clean\": flat_c,\n",
    "        \"noisy\": flat_n,\n",
    "        \"deno\": flat_d,\n",
    "        \"mse_noisy\": mse_n,\n",
    "        \"mse_deno\": mse_d,\n",
    "        \"delta_pct\": d_pct,\n",
    "        \"sigma\": s_key,\n",
    "    }\n",
    "\n",
    "def plot_full_series(rec, title):\n",
    "    L = len(rec[\"clean\"])\n",
    "    xs = np.arange(L)\n",
    "    plt.figure(figsize=(9.8, 4.0))\n",
    "    plt.plot(xs, rec[\"clean\"], label=\"clean\")\n",
    "    plt.plot(xs, rec[\"noisy\"], label=f\"noisy (MSE={rec['mse_noisy']:.5f})\")\n",
    "    plt.plot(xs, rec[\"deno\"],  label=f\"deno (MSE={rec['mse_deno']:.5f}, Δ%={rec['delta_pct']:+.1f})\")\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"value\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_full_series_panels(entry, ds, sigma_override=EVAL_SIGMA,\n",
    "                            panel_len=180, ncols=3, nrows=2, start=0, sharey=True):\n",
    "    rec = reconstruct_full_series(entry, ds, sigma_override)\n",
    "    c, n, d = rec[\"clean\"], rec[\"noisy\"], rec[\"deno\"]\n",
    "    L = len(c)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4.5*ncols, 2.75*nrows), sharey=sharey)\n",
    "    axes = np.ravel(axes)\n",
    "    for k, ax in enumerate(axes):\n",
    "        s = start + k * panel_len\n",
    "        e = min(s + panel_len, L)\n",
    "        if s >= L:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        xs = np.arange(s, e)\n",
    "        cc, nn, dd = c[s:e], n[s:e], d[s:e]\n",
    "        mse_n = float(np.mean((cc - nn)**2))\n",
    "        mse_d = float(np.mean((cc - dd)**2))\n",
    "        d_pct = 0.0 if mse_n < 1e-12 else 100.0 * (mse_n - mse_d) / mse_n\n",
    "        ax.plot(xs, cc, label=\"clean\")\n",
    "        ax.plot(xs, nn, label=f\"noisy (MSE={mse_n:.5f})\")\n",
    "        ax.plot(xs, dd, label=f\"deno (MSE={mse_d:.5f}, Δ%={d_pct:+.1f})\")\n",
    "        if k == 0:\n",
    "            ax.legend()\n",
    "            ax.set_ylabel(\"value\")\n",
    "        ax.set_title(f\"{s}–{e}\")\n",
    "        ax.set_xlabel(\"t\")\n",
    "    fig.suptitle(f\"{entry.name} — {ds.name} (σ={rec['sigma']:.3f}), panels of {panel_len}\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def error_lens(entry, ds, sigma_override=EVAL_SIGMA, seg=150, start=0, length=None):\n",
    "    rec = reconstruct_full_series(entry, ds, sigma_override)\n",
    "    c, n, d = rec[\"clean\"], rec[\"noisy\"], rec[\"deno\"]\n",
    "    L = len(c)\n",
    "    if length is None:\n",
    "        length = L - start\n",
    "    s, e = int(start), int(min(start + length, L))\n",
    "    se_n = (n - c)**2\n",
    "    se_d = (d - c)**2\n",
    "    imp = se_n - se_d\n",
    "    mean_n = float(se_n.mean())\n",
    "    mean_d = float(se_d.mean())\n",
    "    d_pct = 100.0 * (mean_n - mean_d) / max(mean_n, 1e-12)\n",
    "    frac = 100.0 * float(np.mean(imp > 0))\n",
    "    print(f\"{ds.name} | {entry.name} | σ={rec['sigma']:.3f}\\n\"\n",
    "          f\"Global MSE: noisy={mean_n:.5f} deno={mean_d:.5f} Δ%={d_pct:+.1f}\\n\"\n",
    "          f\"Samples improved: {frac:.1f}%\")\n",
    "    xs = np.arange(s, e)\n",
    "    plt.figure(figsize=(10, 3.1))\n",
    "    plt.plot(xs, se_n[s:e], label=\"(clean-noisy)^2\")\n",
    "    plt.plot(xs, se_d[s:e], label=\"(clean-deno)^2\")\n",
    "    plt.title(f\"Per-sample squared errors — {ds.name} [{s}:{e}]\")\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"squared error\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 2.5))\n",
    "    imp_seg = imp[s:e]\n",
    "    plt.plot(xs, imp_seg, lw=0.8)\n",
    "    plt.fill_between(xs, 0, imp_seg, where=(imp_seg >= 0), alpha=0.35)\n",
    "    plt.fill_between(xs, 0, imp_seg, where=(imp_seg < 0), alpha=0.25)\n",
    "    plt.axhline(0, color=\"k\", lw=0.8)\n",
    "    plt.title(f\"Improvement per sample — {ds.name} [{s}:{e}]\")\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"ΔSE\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    bins = list(range(0, L, seg))\n",
    "    contrib = [imp[i:i+seg].sum() for i in bins]\n",
    "    centers = [i + seg/2 for i in bins]\n",
    "    plt.figure(figsize=(10, 3.0))\n",
    "    plt.bar(centers, contrib, width=0.8*seg)\n",
    "    plt.axhline(0, color=\"k\", lw=0.8)\n",
    "    plt.title(f\"Contribution to total improvement by {seg}-sample segments — {ds.name}\")\n",
    "    plt.xlabel(\"segment center\")\n",
    "    plt.ylabel(\"Σ ΔSE\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76312ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10 — Run monolith analyses & heatmaps ==============================\n",
    "if REG and ALL_SUMMARIES:\n",
    "    df = summaries_to_df(ALL_SUMMARIES, REG)\n",
    "    print(\"\\n== Summary (head) ==\")\n",
    "    print(df.head(12).to_string(index=False))\n",
    "\n",
    "    for dsname in sorted(ALL_SUMMARIES):\n",
    "        heatmap_by_arch(df, dsname, value_col=\"delta_pct_mean\")\n",
    "\n",
    "    # Show a few visuals per window size (first model hitting that size)\n",
    "    shown=set()\n",
    "    for name, entry in REG.items():\n",
    "        w = entry.n_qubits\n",
    "        if w in shown: continue\n",
    "        dsname = \"MG_A\"\n",
    "        er = ALL_RESULTS[dsname][name]\n",
    "        plot_reconstruction_example(er, idx=0, title=f\"{name} — {dsname} (σ={EVAL_SIGMA})\")\n",
    "        plot_robustness(entry, DATASETS_BY_W[w][dsname], sigmas=(EVAL_SIGMA,), n_eval=N_EVAL_WINDOWS)\n",
    "        # Full series & panels on MG_B\n",
    "        rec = reconstruct_full_series(entry, DATASETS_BY_W[w][\"MG_B\"], sigma_override=EVAL_SIGMA)\n",
    "        plot_full_series(rec, title=f\"{name} — MG_B (σ={rec['sigma']:.3f})\")\n",
    "        plot_full_series_panels(entry, DATASETS_BY_W[w][\"MG_B\"], sigma_override=EVAL_SIGMA,\n",
    "                                panel_len=180, ncols=3, nrows=2, start=0, sharey=True)\n",
    "        error_lens(entry, DATASETS_BY_W[w][\"MG_B\"], sigma_override=EVAL_SIGMA, seg=150, start=0, length=900)\n",
    "        shown.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "736c41b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No representatives selected (insufficient eval columns or empty df).\n",
      "[Cell 10] Missing Δ% columns (delta_pct_mean_MG_A / MG_B) — skipping scatter.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10 — Representatives & MG_A vs MG_B scatter (monolith, robust) ====\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Expect df_all from your CSVs earlier in the cell; if not present, bail out.\n",
    "if \"df_all\" not in globals():\n",
    "    print(\"[Cell 10] df_all not found — skipping representatives & scatter.\")\n",
    "else:\n",
    "    df_all = df_all.copy()\n",
    "\n",
    "    # ---- A) Ensure architecture columns exist (infer from filename if needed)\n",
    "    _STD_RE = re.compile(r\"(?P<nq>\\d+)q_(?P<nl>\\d+)l_(?P<nt>\\d+)t_(?P<L>\\d+)ls(?:_[0-9]+)?\\.json$\", re.IGNORECASE)\n",
    "\n",
    "    def _infer_arch_from_filename(df, col=\"filename\"):\n",
    "        if col not in df.columns:\n",
    "            return df\n",
    "        # extract parts; this yields columns nq, nl, nt, L when pattern matches\n",
    "        ext = df[col].astype(str).str.extract(_STD_RE)\n",
    "        for src, dst in [(\"nq\", \"n_qubits\"), (\"nl\", \"n_latent\"), (\"nt\", \"n_trash\"), (\"L\", \"n_layers\")]:\n",
    "            if dst not in df.columns or df[dst].isna().all():\n",
    "                if src in ext:\n",
    "                    df[dst] = df.get(dst, pd.Series(index=df.index, dtype=\"float64\"))\n",
    "                    df.loc[ext[src].notna(), dst] = pd.to_numeric(ext[src], errors=\"coerce\")\n",
    "        return df\n",
    "\n",
    "    df_all = _infer_arch_from_filename(df_all, col=\"filename\")\n",
    "\n",
    "    # coerce to Int64 safely (keeps NaN)\n",
    "    for c in [\"n_qubits\", \"n_latent\", \"n_trash\", \"n_layers\"]:\n",
    "        if c in df_all.columns:\n",
    "            df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # ---- B) Build config_name without apply (avoid your ValueError)\n",
    "    need = {\"n_qubits\", \"n_latent\", \"n_trash\", \"n_layers\"}\n",
    "    if not need.issubset(df_all.columns):\n",
    "        missing = sorted(need - set(df_all.columns))\n",
    "        print(f\"[Cell 10] Missing columns {missing}; cannot form config_name. Skipping.\")\n",
    "    else:\n",
    "        # Fill any remaining NaNs with -1 markers so the string comp works\n",
    "        tmp = df_all[[\"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\"]].copy()\n",
    "        tmp = tmp.fillna(-1).astype(int)\n",
    "        df_all[\"config_name\"] = (\n",
    "            tmp[\"n_qubits\"].astype(str) + \"q_\" +\n",
    "            tmp[\"n_latent\"].astype(str) + \"l_\" +\n",
    "            tmp[\"n_trash\"].astype(str)  + \"t_\" +\n",
    "            tmp[\"n_layers\"].astype(str) + \"ls\"\n",
    "        )\n",
    "\n",
    "        # ---- C) Compute AVGs if not present\n",
    "        if {\"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\"} <= set(df_all.columns) and \"delta_pct_mean_AVG\" not in df_all.columns:\n",
    "            df_all[\"delta_pct_mean_AVG\"] = pd.to_numeric(df_all[\"delta_pct_mean_MG_A\"], errors=\"coerce\") \\\n",
    "                                         .combine(pd.to_numeric(df_all[\"delta_pct_mean_MG_B\"], errors=\"coerce\"),\n",
    "                                                  func=lambda a,b: np.nanmean([a,b], axis=0))\n",
    "        if {\"success_rate_pct_MG_A\",\"success_rate_pct_MG_B\"} <= set(df_all.columns) and \"success_rate_pct_AVG\" not in df_all.columns:\n",
    "            df_all[\"success_rate_pct_AVG\"] = pd.to_numeric(df_all[\"success_rate_pct_MG_A\"], errors=\"coerce\") \\\n",
    "                                            .combine(pd.to_numeric(df_all[\"success_rate_pct_MG_B\"], errors=\"coerce\"),\n",
    "                                                     func=lambda a,b: np.nanmean([a,b], axis=0))\n",
    "        if \"sign_test_p_AVG\" not in df_all.columns:\n",
    "            df_all[\"sign_test_p_AVG\"] = pd.NA  # leave blank; combining p-values properly is separate\n",
    "\n",
    "        # ---- D) Pick “middle” representative per config\n",
    "        perf_col = \"delta_pct_mean_AVG\"\n",
    "        p_col    = \"sign_test_p_AVG\"\n",
    "        succ_col = \"success_rate_pct_AVG\"\n",
    "\n",
    "        middle_rows = []\n",
    "        if perf_col in df_all.columns:\n",
    "            for cfg, g in df_all.groupby(\"config_name\", dropna=False):\n",
    "                g2 = g.dropna(subset=[perf_col]).copy()\n",
    "                if g2.empty:\n",
    "                    continue\n",
    "                med = g2[perf_col].median()\n",
    "                g2[\"_dist\"] = (g2[perf_col] - med).abs()\n",
    "                if succ_col in g2.columns:\n",
    "                    g2[\"_succ_dist\"] = (pd.to_numeric(g2[succ_col], errors=\"coerce\") - \n",
    "                                        pd.to_numeric(g2[succ_col], errors=\"coerce\").median()).abs()\n",
    "                sort_cols, ascending = [\"_dist\"], [True]\n",
    "                if p_col in g2.columns and p_col in g2:\n",
    "                    sort_cols.append(p_col); ascending.append(True)\n",
    "                if \"_succ_dist\" in g2.columns:\n",
    "                    sort_cols.append(\"_succ_dist\"); ascending.append(True)\n",
    "                sort_cols.append(\"filename\" if \"filename\" in g2.columns else g2.columns[0]); ascending.append(True)\n",
    "                pick = g2.sort_values(sort_cols, ascending=ascending).iloc[0]\n",
    "                middle_rows.append(pick.drop(labels=[c for c in [\"_dist\",\"_succ_dist\"] if c in g2.columns]))\n",
    "        middle_df = pd.DataFrame(middle_rows).reset_index(drop=True)\n",
    "\n",
    "        # ---- E) Save representatives near your project root (jacobs_examples/)\n",
    "        out_dir = Path(\"jacobs_examples\") if Path(\"jacobs_examples\").exists() else Path(\".\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        OUT_CSV = out_dir / \"representative_middle_models_with_eval.csv\"\n",
    "        if not middle_df.empty:\n",
    "            middle_df.to_csv(OUT_CSV, index=False)\n",
    "            print(f\"\\nSaved {len(middle_df)} representatives → {OUT_CSV}\")\n",
    "        else:\n",
    "            print(\"\\nNo representatives selected (insufficient eval columns or empty df).\")\n",
    "\n",
    "        # ---- F) Scatter: MG_A vs MG_B (all points + representatives if any)\n",
    "        if {\"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\"} <= set(df_all.columns):\n",
    "            x = pd.to_numeric(df_all[\"delta_pct_mean_MG_A\"], errors=\"coerce\").values\n",
    "            y = pd.to_numeric(df_all[\"delta_pct_mean_MG_B\"], errors=\"coerce\").values\n",
    "\n",
    "            plt.figure(figsize=(6.8, 5.2))\n",
    "            plt.scatter(x, y, alpha=0.55, label=\"All models\")\n",
    "\n",
    "            finite = np.isfinite(x) & np.isfinite(y)\n",
    "            if finite.any():\n",
    "                xmin = float(np.min(np.concatenate([x[finite], y[finite]])))\n",
    "                xmax = float(np.max(np.concatenate([x[finite], y[finite]])))\n",
    "                plt.plot([xmin, xmax], [xmin, xmax], linestyle=\"--\", linewidth=1.0, label=\"y = x\")\n",
    "\n",
    "            plt.xlabel(\"Δ% mean on MG_A\")\n",
    "            plt.ylabel(\"Δ% mean on MG_B\")\n",
    "            plt.title(\"Model performance: MG_A vs MG_B\")\n",
    "\n",
    "            if not middle_df.empty and {\"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\"} <= set(middle_df.columns):\n",
    "                xr = pd.to_numeric(middle_df[\"delta_pct_mean_MG_A\"], errors=\"coerce\").values\n",
    "                yr = pd.to_numeric(middle_df[\"delta_pct_mean_MG_B\"], errors=\"coerce\").values\n",
    "                plt.scatter(xr, yr, s=80, marker=\"D\", label=\"Representatives\")\n",
    "                if \"config_name\" in middle_df.columns:\n",
    "                    for _, r in middle_df.dropna(subset=[\"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\"]).iterrows():\n",
    "                        plt.annotate(str(r[\"config_name\"]),\n",
    "                                     (float(r[\"delta_pct_mean_MG_A\"]), float(r[\"delta_pct_mean_MG_B\"])),\n",
    "                                     xytext=(4, 4), textcoords=\"offset points\", fontsize=8)\n",
    "\n",
    "            plt.legend(loc=\"best\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Numeric summary\n",
    "            def _safe_corr(a, b):\n",
    "                a = np.asarray(a, float); b = np.asarray(b, float)\n",
    "                m = np.isfinite(a) & np.isfinite(b)\n",
    "                return np.corrcoef(a[m], b[m])[0,1] if m.sum() >= 3 else np.nan\n",
    "\n",
    "            corr_all = _safe_corr(x, y)\n",
    "            above = int(np.nansum(y > x))\n",
    "            below = int(np.nansum(y < x))\n",
    "            equal = int(np.nansum(np.isclose(y, x, atol=1e-9)))\n",
    "            print(f\"Correlation (Δ% MG_A vs MG_B, all models): r = {corr_all:.3f}\")\n",
    "            print(f\"Points above diagonal (MG_B > MG_A): {above}\")\n",
    "            print(f\"Points below diagonal (MG_B < MG_A): {below}\")\n",
    "            print(f\"On diagonal: {equal}\")\n",
    "\n",
    "            # Per-config comparison table (representatives if available, else medians)\n",
    "            if not middle_df.empty and {\"config_name\",\"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\"} <= set(middle_df.columns):\n",
    "                comp = (middle_df[[\"config_name\",\"filename\",\"run_tag\",\n",
    "                                   \"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\",\"delta_pct_mean_AVG\"]]\n",
    "                        .dropna(subset=[\"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\"], how=\"any\")\n",
    "                        .sort_values(\"config_name\", kind=\"stable\"))\n",
    "                print(\"\\nRepresentatives — MG_A vs MG_B:\")\n",
    "                try:\n",
    "                    display(comp)\n",
    "                except Exception:\n",
    "                    print(comp.to_string(index=False))\n",
    "            else:\n",
    "                # medians per config if we can\n",
    "                if \"config_name\" in df_all.columns:\n",
    "                    med = (df_all.groupby(\"config_name\")[[\"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\"]]\n",
    "                                 .median().reset_index())\n",
    "                    med[\"delta_pct_mean_AVG\"] = med[[\"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\"]].mean(axis=1)\n",
    "                    print(\"\\nGroup medians (no representatives found in scope):\")\n",
    "                    try:\n",
    "                        display(med.sort_values(\"config_name\"))\n",
    "                    except Exception:\n",
    "                        print(med.sort_values(\"config_name\").to_string(index=False))\n",
    "        else:\n",
    "            print(\"[Cell 10] Missing Δ% columns (delta_pct_mean_MG_A / MG_B) — skipping scatter.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
