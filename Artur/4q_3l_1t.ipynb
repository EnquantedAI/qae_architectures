{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed3bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# We'll run 5 instances (same across all notebooks) and two depths: 1 and 3 layers.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # used in filenames as ..._ls_01.json, ..._ls_02.json, ...\n",
    "LAYER_OPTIONS  = [1, 3]            # train 1-layer first, then 3-layers\n",
    "EVAL_SIGMA     = 0.10              # fixed noise everywhere (train & eval)\n",
    "\n",
    "# where to save artifacts (JSON bundles, instance records, CSV summary)\n",
    "OUT_BASE = \"./runs_halfqae\"        # change if you like; subfolders will be created automatically\n",
    "CSV_PATH = f\"{OUT_BASE}/results_instances.csv\"  # will be appended-to if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d179c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== IMPORTY ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys, json, math, random, time, hashlib\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1410fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== USTAWIENIA OGÓLNE ====\n",
    "np.random.seed(42)\n",
    "\n",
    "# symulator (szybki)\n",
    "BACKEND = \"lightning.qubit\"\n",
    "\n",
    "# Half-QAE\n",
    "n_qubits = 4\n",
    "n_latent = 3\n",
    "n_trash  = n_qubits - n_latent  # 1\n",
    "\n",
    "# dane Mackey-Glass \n",
    "beta=0.25\n",
    "gamma=0.1\n",
    "n=10\n",
    "tau=15\n",
    "dt=1.0\n",
    "T=300\n",
    "\n",
    "# skalowanie\n",
    "margin = 0.2\n",
    "scale_low, scale_high = 0.0+margin, 1.0-margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b35303bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== POMOCNICZE ====\n",
    "def scale_values(x, new_min=0.0, new_max=1.0):\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    return new_min + (x - x_min) * (new_max - new_min) / max(1e-12, (x_max - x_min))\n",
    "\n",
    "def mackey_glass(beta=0.2, gamma=0.1, n=10, tau=17, dt=0.1, T=1000):\n",
    "    N = int(T/dt)\n",
    "    delay_steps = int(tau/dt)\n",
    "    x = np.zeros(N+delay_steps)\n",
    "    x[0:delay_steps] = 1.2\n",
    "    for t in range(delay_steps, N+delay_steps-1):\n",
    "        x_tau = x[t-delay_steps]\n",
    "        dxdt = beta * x_tau / (1 + x_tau**n) - gamma * x[t]\n",
    "        x[t+1] = x[t] + dxdt * dt\n",
    "    return x[delay_steps:]\n",
    "\n",
    "def ts_add_noise(X, noise=0.1, low=0.0, high=1.0):\n",
    "    Z = X + np.random.normal(0.0, noise, size=X.shape)\n",
    "    return np.clip(Z, low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f7597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== GENEROWANIE DANYCH ====\n",
    "y_raw = mackey_glass(beta=beta, gamma=gamma, n=n, tau=tau, dt=dt, T=T)\n",
    "y_raw = scale_values(y_raw, new_min=scale_low, new_max=scale_high)\n",
    "y = y_raw[2::3]                               # subsampling\n",
    "X_idx = np.arange(len(y))\n",
    "\n",
    "# okna (sliding window)\n",
    "window_size = n_qubits\n",
    "stride = 1\n",
    "X_windows = np.stack([y[i:i+window_size] for i in range(0, len(y)-window_size+1, stride)])\n",
    "\n",
    "# podziały: train/val/test = 0.6 / 0.2 / 0.2\n",
    "X_temp, X_test = train_test_split(X_windows, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_temp,   test_size=0.25, random_state=42)\n",
    "\n",
    "# czyste zbiory (na Stage 1 i Stage 2 generujemy szum dynamicznie)\n",
    "X_train_clean = X_train.copy()\n",
    "X_val_clean   = X_val.copy()\n",
    "X_test_clean  = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d426eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ANSATZE  ====\n",
    "def encoder_ansatz(params, x, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    # angle encoding\n",
    "    for w, val in zip(W, x[:len(W)]):\n",
    "        qml.RY(val*np.pi, wires=w)\n",
    "    # warstwy rotacji\n",
    "    n_block = len(W)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in range(n_layers):\n",
    "        for j, w in enumerate(W):\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RX(params[idx],     wires=w)\n",
    "            qml.RY(params[idx + 1], wires=w)\n",
    "            qml.RZ(params[idx + 2], wires=w)\n",
    "        for a, b in zip(W, W[1:]):\n",
    "            qml.CNOT(wires=[a, b])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n",
    "\n",
    "def decoder_ansatz(params, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    n_block = len(W)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in range(n_layers):\n",
    "        for a, b in zip(W, W[1:]):\n",
    "            qml.CNOT(wires=[a, b])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n",
    "        for j, w in enumerate(W):\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RZ(params[idx + 2], wires=w)\n",
    "            qml.RY(params[idx + 1], wires=w)\n",
    "            qml.RX(params[idx],     wires=w)\n",
    "\n",
    "def adjoint_decoder_ansatz(params, x, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    n_block = len(W)\n",
    "    for w, val in zip(W, x[:n_block]):\n",
    "        qml.RY(val*np.pi, wires=w)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in reversed(range(n_layers)):\n",
    "        for j in reversed(range(n_block)):\n",
    "            w = W[j]\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RX(-params[idx],     wires=w)\n",
    "            qml.RY(-params[idx + 1], wires=w)\n",
    "            qml.RZ(-params[idx + 2], wires=w)\n",
    "        for j in reversed(range(n_block - 1)):\n",
    "            qml.CNOT(wires=[W[j+1], W[j]])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc13de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STAGE 1: swap-test na adjoint_decoder (uczenie „trash->czysto”) ====\n",
    "# inicjalizacje\n",
    "#n_layers = 2\n",
    "#param_shape = n_layers * n_qubits * 3\n",
    "\n",
    "trash2_start = n_qubits               # 4\n",
    "ancilla      = n_qubits + n_trash     # 6\n",
    "num_total_s1 = n_qubits + n_trash + 1 # 7\n",
    "\n",
    "dev_s1 = qml.device(BACKEND, wires=num_total_s1, shots=None)\n",
    "\n",
    "@qml.qnode(dev_s1, interface=\"autograd\")\n",
    "def swap_test_on_adj_decoder(x, params):\n",
    "    main   = list(range(n_qubits))                                # [0,1,2,3]\n",
    "    trash1 = list(range(n_latent, n_latent + n_trash))            # [2,3]\n",
    "    trash2 = list(range(trash2_start, trash2_start + n_trash))    # [4,5]\n",
    "\n",
    "    adjoint_decoder_ansatz(params, x)\n",
    "\n",
    "    for i in trash2:\n",
    "        qml.Hadamard(wires=i)\n",
    "    qml.Hadamard(wires=ancilla)\n",
    "    for i in range(n_trash):\n",
    "        qml.CSWAP(wires=[ancilla, trash1[i], trash2[i]])\n",
    "    qml.Hadamard(wires=ancilla)\n",
    "\n",
    "    return qml.probs(wires=ancilla)\n",
    "\n",
    "def s1_cost(params, X_batch):\n",
    "    acc = pnp.array(0.0)\n",
    "    for x in X_batch:\n",
    "        p0 = swap_test_on_adj_decoder(x, params)[0]\n",
    "        acc = acc + (1.0 - p0)\n",
    "    return acc / len(X_batch)\n",
    "\n",
    "def train_adjoint_decoder(params_init, X_train, n_epochs=80, batch_size=8, lr=0.01, X_val_clean=None, seed=0):  # <-- ZMIANA\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    params = pnp.array(params_init, requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr)\n",
    "    hist = []\n",
    "    hist_val = []\n",
    "    lr_hist = []\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    t0 = time.time()\n",
    "    for ep in range(n_epochs):\n",
    "        idx = rng.permutation(len(X_train))  # <-- ZMIANA\n",
    "        s=0.0; nb=0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            Xb = X_train[idx[i:i+batch_size]]\n",
    "            params, c = opt.step_and_cost(lambda p: s1_cost(p, Xb), params)\n",
    "            s += float(c); nb += 1\n",
    "        train_cost = s/nb\n",
    "        hist.append(train_cost)\n",
    "        lr_hist.append(opt.stepsize)\n",
    "        # --- walidacja ---\n",
    "        if X_val_clean is not None:\n",
    "            val_cost = float(s1_cost(params, X_val_clean))\n",
    "            hist_val.append(val_cost)\n",
    "        else:\n",
    "            val_cost = train_cost\n",
    "            hist_val.append(train_cost)\n",
    "        print(f\"[Stage1] L={len(params_init)//(n_qubits*3)} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {opt.stepsize:.5f}\")\n",
    "        if train_cost < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = train_cost, pnp.array(params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= 10:\n",
    "            break\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    return dict(\n",
    "        phi=best_params if best_params is not None else params,\n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=len(hist),\n",
    "        hist_train=list(map(float, hist)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64ed0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STAGE 2: trenujemy ENCODER latent↔latent (noisy vs clean_ref) ====\n",
    "dev_s2 = qml.device(BACKEND, wires=2*n_qubits + 1, shots=None)\n",
    "\n",
    "@qml.qnode(dev_s2, interface=\"autograd\")\n",
    "def swap_test_encoder_latent(x_noisy, x_clean, enc_params, dec_dagger_params):\n",
    "    encoder = list(range(n_qubits))                  # blok encodera\n",
    "    ref     = list(range(n_qubits, 2*n_qubits))      # blok referencyjny\n",
    "    anc     = 2*n_qubits\n",
    "\n",
    "    encoder_ansatz(enc_params, x_noisy, wires=encoder)\n",
    "    adjoint_decoder_ansatz(dec_dagger_params, x_clean, wires=ref)\n",
    "\n",
    "    qml.Hadamard(wires=anc)\n",
    "    for i in range(n_latent):\n",
    "        qml.CSWAP(wires=[anc, encoder[i], ref[i]])   # SWAP tylko latentów\n",
    "    qml.Hadamard(wires=anc)\n",
    "    return qml.probs(wires=anc)\n",
    "\n",
    "def s2_cost(enc_params, X_noisy_b, X_clean_b, dec_dagger_params):\n",
    "    acc = pnp.array(0.0)\n",
    "    for xn, xc in zip(X_noisy_b, X_clean_b):\n",
    "        p0 = swap_test_encoder_latent(xn, xc, enc_params, dec_dagger_params)[0]\n",
    "        acc = acc + (1.0 - p0)\n",
    "    return acc / len(X_noisy_b)\n",
    "\n",
    "# skala szumu czerpana z realnego skalowania danych\n",
    "info = {\"scale_low\": scale_low, \"scale_high\": scale_high}\n",
    "\n",
    "def s2_cost_dataset(enc_params, X_clean_set, dec_params, noise_level):\n",
    "    \"\"\"Metryka testowa S2: generujemy szum o danym poziomie i liczymy koszt.\"\"\"\n",
    "    low, high = info[\"scale_low\"], info[\"scale_high\"]\n",
    "    sigma = noise_level * (high - low)\n",
    "    X_noisy_set = ts_add_noise(X_clean_set, noise=sigma, low=low, high=high)\n",
    "    return float(s2_cost(enc_params, X_noisy_set, X_clean_set, dec_params))\n",
    "\n",
    "def train_encoder_with_sidekick_dyn_noise(enc_params_init, X_clean,\n",
    "                                         dec_dagger_params, noise_level,\n",
    "                                         n_epochs=80, batch_size=8, lr=0.01, seed=0,\n",
    "                                         X_val_clean=None):\n",
    "    params = pnp.array(enc_params_init, requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr)\n",
    "    hist = []\n",
    "    hist_val = []\n",
    "    lr_hist = []\n",
    "    hist_noisy = []\n",
    "    hist_delta = []\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    t0 = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    low, high = info[\"scale_low\"], info[\"scale_high\"]\n",
    "    sigma = noise_level * (high - low)\n",
    "    for ep in range(n_epochs):\n",
    "        idx = rng.permutation(len(X_clean))\n",
    "        Xc = X_clean[idx]\n",
    "        s = 0.0; nb = 0\n",
    "        for i in range(0, len(Xc), batch_size):\n",
    "            Xcb = Xc[i:i+batch_size]\n",
    "            Xnb = ts_add_noise(Xcb, noise=sigma, low=low, high=high)\n",
    "            params, c = opt.step_and_cost(\n",
    "                lambda p: s2_cost(p, Xnb, Xcb, dec_dagger_params), params\n",
    "            )\n",
    "            s += float(c); nb += 1\n",
    "        train_cost = s/nb\n",
    "        hist.append(train_cost)\n",
    "        lr_hist.append(opt.stepsize)\n",
    "        # --- walidacja ---\n",
    "        if X_val_clean is not None:\n",
    "            val_cost = float(s2_cost_dataset(params, X_val_clean, dec_dagger_params, noise_level))\n",
    "            # --- baseline/noise-only na walidacji ---\n",
    "            X_noisy_val = ts_add_noise(X_val_clean, noise=sigma, low=low, high=high)\n",
    "            mse_noisy = float(s2_cost(params*0, X_noisy_val, X_val_clean, dec_dagger_params))  # params*0 = brak uczenia\n",
    "            hist_noisy.append(mse_noisy)\n",
    "            if mse_noisy > 1e-12:\n",
    "                delta = 100.0 * (1.0 - val_cost / mse_noisy)\n",
    "            else:\n",
    "                delta = 0.0\n",
    "            hist_delta.append(delta)\n",
    "            hist_val.append(val_cost)\n",
    "        else:\n",
    "            val_cost = train_cost\n",
    "            hist_val.append(train_cost)\n",
    "            hist_noisy.append(np.nan)\n",
    "            hist_delta.append(np.nan)\n",
    "        print(f\"[Stage3] L={len(enc_params_init)//(n_qubits*3)} ep {ep:03d} | train {train_cost:.5f} | \"\n",
    "              f\"val {val_cost:.5f} | noisy {mse_noisy:.5f} | Δ {delta:+.1f}% | LR {opt.stepsize:.5f}\")\n",
    "        if train_cost < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = train_cost, pnp.array(params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= 10:\n",
    "            break\n",
    "    \n",
    "    train_seconds = float(time.time() - t0)\n",
    "    return dict(\n",
    "        psi=best_params if best_params is not None else params,\n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=len(hist),\n",
    "        hist_train=list(map(float, hist)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d4cb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(X_train, X_val, n_layers=2, instance_id=1, n_epochs=120, batch_size=32, lr_init=0.01, patience=10, lr_patience=8, min_delta=1e-6, seed=0):  # <-- ZMIANA\n",
    "    param_shape = n_layers * n_qubits * 3\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    params_init = rng.uniform(-0.01, 0.01, param_shape)  # <-- ZMIANA\n",
    "    result = train_adjoint_decoder(\n",
    "        params_init, X_train,\n",
    "        n_epochs=n_epochs, batch_size=batch_size, lr=lr_init,\n",
    "        X_val_clean=X_val,\n",
    "        seed=seed  # <-- ZMIANA\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def train_stage3(X_train, X_val, dec_dagger_params, n_layers=2, instance_id=1, n_epochs=120, batch_size=32, lr_init=0.01, noise_level=0.10, seed=0):\n",
    "    param_shape = n_layers * n_qubits * 3\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    enc_params_init = rng.uniform(-0.01, 0.01, param_shape)  # <-- ZMIANA\n",
    "    result = train_encoder_with_sidekick_dyn_noise(\n",
    "        enc_params_init, X_train,\n",
    "        dec_dagger_params, noise_level,\n",
    "        n_epochs=n_epochs, batch_size=batch_size, lr=lr_init, seed=seed,\n",
    "        X_val_clean=X_val\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83879ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.063804 | val 0.056745 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.063256 | val 0.055486 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.062227 | val 0.054336 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.060651 | val 0.053375 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.060709 | val 0.052591 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.060193 | val 0.051948 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.059144 | val 0.051482 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.059238 | val 0.051172 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.058978 | val 0.050959 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.058752 | val 0.050866 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.057992 | val 0.050800 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.059229 | val 0.050820 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.059281 | val 0.050848 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.058646 | val 0.050913 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.060230 | val 0.050952 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.058482 | val 0.051046 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.059052 | val 0.051089 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.058920 | val 0.051121 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.059611 | val 0.051141 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.059484 | val 0.051138 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.059629 | val 0.051159 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.11424 | val 0.10287 | noisy 0.10181 | Δ -1.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11443 | val 0.10001 | noisy 0.09382 | Δ -6.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.11492 | val 0.10243 | noisy 0.10304 | Δ +0.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.11397 | val 0.08541 | noisy 0.09350 | Δ +8.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.10179 | val 0.08801 | noisy 0.08888 | Δ +1.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.11101 | val 0.09248 | noisy 0.10323 | Δ +10.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10769 | val 0.08934 | noisy 0.10167 | Δ +12.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10556 | val 0.07999 | noisy 0.09607 | Δ +16.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10683 | val 0.08798 | noisy 0.09936 | Δ +11.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.11140 | val 0.08782 | noisy 0.09533 | Δ +7.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10540 | val 0.08618 | noisy 0.09832 | Δ +12.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10643 | val 0.08206 | noisy 0.09318 | Δ +11.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10703 | val 0.09314 | noisy 0.10886 | Δ +14.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10440 | val 0.10042 | noisy 0.10614 | Δ +5.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.11531 | val 0.09389 | noisy 0.09514 | Δ +1.3% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.064784 | val 0.057382 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.064213 | val 0.056052 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.063027 | val 0.054906 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.060574 | val 0.053859 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.059840 | val 0.052967 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.060168 | val 0.052243 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.059357 | val 0.051660 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.059618 | val 0.051226 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.057447 | val 0.050947 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.060159 | val 0.050775 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.058515 | val 0.050702 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.057310 | val 0.050672 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.059216 | val 0.050689 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.059160 | val 0.050740 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.059660 | val 0.050822 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.058900 | val 0.050897 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.058677 | val 0.050940 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.058770 | val 0.051006 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.059701 | val 0.051050 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.060143 | val 0.051081 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.059240 | val 0.051070 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.058956 | val 0.051101 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.12443 | val 0.09108 | noisy 0.09531 | Δ +4.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.10844 | val 0.09211 | noisy 0.10996 | Δ +16.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10683 | val 0.08715 | noisy 0.09967 | Δ +12.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10434 | val 0.09457 | noisy 0.09937 | Δ +4.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.10980 | val 0.09479 | noisy 0.09123 | Δ -3.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10338 | val 0.09288 | noisy 0.09978 | Δ +6.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10914 | val 0.09000 | noisy 0.09469 | Δ +5.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10749 | val 0.08321 | noisy 0.09219 | Δ +9.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10467 | val 0.08917 | noisy 0.09449 | Δ +5.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10612 | val 0.08672 | noisy 0.09429 | Δ +8.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10951 | val 0.09037 | noisy 0.09736 | Δ +7.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10409 | val 0.09236 | noisy 0.08877 | Δ -4.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10448 | val 0.08722 | noisy 0.09256 | Δ +5.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10591 | val 0.09029 | noisy 0.09081 | Δ +0.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.10887 | val 0.08955 | noisy 0.09033 | Δ +0.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.10894 | val 0.09199 | noisy 0.08952 | Δ -2.8% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.064964 | val 0.057355 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.064507 | val 0.055988 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.062636 | val 0.054793 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.063031 | val 0.053806 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.061262 | val 0.053023 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.060478 | val 0.052336 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.060480 | val 0.051798 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.060095 | val 0.051388 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.059371 | val 0.051091 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.059856 | val 0.050885 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.058165 | val 0.050793 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.059310 | val 0.050761 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.057830 | val 0.050805 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.059215 | val 0.050862 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.059156 | val 0.050897 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.059013 | val 0.050949 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.059138 | val 0.051004 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.058203 | val 0.051044 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.058625 | val 0.051072 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.057890 | val 0.051071 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.059551 | val 0.051081 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.059272 | val 0.051094 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.059056 | val 0.051063 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.11895 | val 0.09743 | noisy 0.10670 | Δ +8.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11134 | val 0.09227 | noisy 0.09384 | Δ +1.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10268 | val 0.09735 | noisy 0.09407 | Δ -3.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.11123 | val 0.09186 | noisy 0.10922 | Δ +15.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.09888 | val 0.08570 | noisy 0.10095 | Δ +15.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10375 | val 0.08748 | noisy 0.09347 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10539 | val 0.08402 | noisy 0.09954 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10307 | val 0.08880 | noisy 0.10007 | Δ +11.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10542 | val 0.08794 | noisy 0.09671 | Δ +9.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.11019 | val 0.08910 | noisy 0.08736 | Δ -2.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.11365 | val 0.09632 | noisy 0.08874 | Δ -8.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10560 | val 0.08584 | noisy 0.08943 | Δ +4.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10642 | val 0.09034 | noisy 0.10439 | Δ +13.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10895 | val 0.09158 | noisy 0.11045 | Δ +17.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.10425 | val 0.08235 | noisy 0.09832 | Δ +16.2% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.065003 | val 0.056883 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.062039 | val 0.055484 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.060632 | val 0.054411 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.061873 | val 0.053543 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.062165 | val 0.052778 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.059824 | val 0.052168 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.060674 | val 0.051701 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.059534 | val 0.051354 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.060618 | val 0.051117 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.058619 | val 0.051008 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.059833 | val 0.050951 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.058386 | val 0.050965 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.058979 | val 0.050965 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.058069 | val 0.050998 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.059845 | val 0.051019 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.058577 | val 0.051042 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.057154 | val 0.051045 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.059466 | val 0.051085 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.058881 | val 0.051107 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.059022 | val 0.051104 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.058360 | val 0.051084 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.057956 | val 0.051072 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.059720 | val 0.051074 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.059290 | val 0.051058 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.057710 | val 0.051066 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.058069 | val 0.051062 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.059045 | val 0.051027 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.11853 | val 0.10427 | noisy 0.09564 | Δ -9.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.10895 | val 0.09493 | noisy 0.10443 | Δ +9.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10785 | val 0.09103 | noisy 0.10734 | Δ +15.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10224 | val 0.08855 | noisy 0.09798 | Δ +9.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.11311 | val 0.08632 | noisy 0.09986 | Δ +13.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10504 | val 0.08638 | noisy 0.10462 | Δ +17.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.11114 | val 0.08852 | noisy 0.10453 | Δ +15.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10873 | val 0.09106 | noisy 0.09732 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.11486 | val 0.09244 | noisy 0.09651 | Δ +4.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10550 | val 0.08872 | noisy 0.10318 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10962 | val 0.09064 | noisy 0.09792 | Δ +7.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10485 | val 0.08383 | noisy 0.09852 | Δ +14.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10664 | val 0.08711 | noisy 0.10254 | Δ +15.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10227 | val 0.09499 | noisy 0.10929 | Δ +13.1% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.064829 | val 0.057371 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.063176 | val 0.056368 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.062851 | val 0.055313 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.061687 | val 0.054447 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.061194 | val 0.053653 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.060748 | val 0.052940 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.060210 | val 0.052365 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.059905 | val 0.051885 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.060050 | val 0.051511 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.058276 | val 0.051277 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.058289 | val 0.051077 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.058250 | val 0.050930 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.059185 | val 0.050848 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.058193 | val 0.050823 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.059229 | val 0.050837 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.059373 | val 0.050851 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.058363 | val 0.050854 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.059802 | val 0.050883 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.058597 | val 0.050950 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.057735 | val 0.051034 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.059090 | val 0.051053 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.059522 | val 0.051090 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.059696 | val 0.051123 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.058229 | val 0.051139 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.059269 | val 0.051155 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.059482 | val 0.051142 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.057654 | val 0.051117 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.058571 | val 0.051073 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.058666 | val 0.051048 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.059884 | val 0.051030 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.058369 | val 0.051009 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.059037 | val 0.051008 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.059090 | val 0.050978 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.060338 | val 0.050971 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.059192 | val 0.050975 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.058934 | val 0.050940 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.059824 | val 0.050949 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.11911 | val 0.09936 | noisy 0.09248 | Δ -7.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11686 | val 0.08460 | noisy 0.09810 | Δ +13.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10743 | val 0.09703 | noisy 0.09849 | Δ +1.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10521 | val 0.08881 | noisy 0.10240 | Δ +13.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.10487 | val 0.08365 | noisy 0.10103 | Δ +17.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10193 | val 0.08877 | noisy 0.10682 | Δ +16.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10282 | val 0.09105 | noisy 0.09814 | Δ +7.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10741 | val 0.08542 | noisy 0.09805 | Δ +12.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10984 | val 0.09868 | noisy 0.09106 | Δ -8.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10447 | val 0.08360 | noisy 0.10347 | Δ +19.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.11018 | val 0.09375 | noisy 0.09904 | Δ +5.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10653 | val 0.07693 | noisy 0.09858 | Δ +22.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10831 | val 0.09207 | noisy 0.09846 | Δ +6.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10099 | val 0.08274 | noisy 0.10163 | Δ +18.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.10872 | val 0.09255 | noisy 0.09111 | Δ -1.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.10903 | val 0.09882 | noisy 0.09368 | Δ -5.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.10259 | val 0.10211 | noisy 0.09316 | Δ -9.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.10185 | val 0.09614 | noisy 0.09704 | Δ +0.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.10797 | val 0.10150 | noisy 0.10113 | Δ -0.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.10313 | val 0.08275 | noisy 0.10712 | Δ +22.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.10382 | val 0.08480 | noisy 0.10278 | Δ +17.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.10789 | val 0.08704 | noisy 0.09523 | Δ +8.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.10116 | val 0.08803 | noisy 0.09528 | Δ +7.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.10980 | val 0.08980 | noisy 0.09548 | Δ +5.9% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 1 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.061388 | val 0.056512 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.060350 | val 0.054243 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.059310 | val 0.052617 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.057486 | val 0.051719 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.057802 | val 0.051264 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.058172 | val 0.051021 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.057037 | val 0.051021 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.057148 | val 0.051192 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.056801 | val 0.051416 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.056718 | val 0.051601 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.055453 | val 0.051558 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.056850 | val 0.051445 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.056618 | val 0.051235 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.056084 | val 0.051180 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.057626 | val 0.051031 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.055783 | val 0.051069 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.056456 | val 0.051049 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.055945 | val 0.050954 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.056429 | val 0.050855 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.056394 | val 0.050848 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.056139 | val 0.050796 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.11479 | val 0.10900 | noisy 0.10646 | Δ -2.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.11572 | val 0.10079 | noisy 0.11451 | Δ +12.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.11110 | val 0.09204 | noisy 0.10881 | Δ +15.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.10685 | val 0.08971 | noisy 0.10723 | Δ +16.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.11130 | val 0.09278 | noisy 0.11404 | Δ +18.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.11371 | val 0.09114 | noisy 0.11377 | Δ +19.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.11066 | val 0.08659 | noisy 0.11423 | Δ +24.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.10193 | val 0.09524 | noisy 0.11336 | Δ +16.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.10943 | val 0.09301 | noisy 0.11369 | Δ +18.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.10406 | val 0.09634 | noisy 0.11340 | Δ +15.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.11215 | val 0.10118 | noisy 0.10902 | Δ +7.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.10561 | val 0.09440 | noisy 0.10833 | Δ +12.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.11228 | val 0.08659 | noisy 0.10335 | Δ +16.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.10384 | val 0.09828 | noisy 0.10416 | Δ +5.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.10083 | val 0.09608 | noisy 0.10721 | Δ +10.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.10243 | val 0.09572 | noisy 0.12301 | Δ +22.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.10948 | val 0.09818 | noisy 0.11410 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.10214 | val 0.09873 | noisy 0.10133 | Δ +2.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.11066 | val 0.09712 | noisy 0.10869 | Δ +10.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.11637 | val 0.09507 | noisy 0.10659 | Δ +10.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.10092 | val 0.09011 | noisy 0.11000 | Δ +18.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.11113 | val 0.08925 | noisy 0.10657 | Δ +16.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.10696 | val 0.08716 | noisy 0.09980 | Δ +12.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.10395 | val 0.10235 | noisy 0.11231 | Δ +8.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.11164 | val 0.09404 | noisy 0.11651 | Δ +19.3% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.062306 | val 0.056535 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.060733 | val 0.054383 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.059468 | val 0.052957 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.057379 | val 0.051972 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.057371 | val 0.051359 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.057369 | val 0.051074 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.056848 | val 0.051031 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.057469 | val 0.051098 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.054631 | val 0.051299 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.057981 | val 0.051387 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.056109 | val 0.051364 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.054726 | val 0.051182 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.056984 | val 0.050990 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.056390 | val 0.050786 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.056962 | val 0.050671 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.055609 | val 0.050597 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.055273 | val 0.050508 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.055803 | val 0.050545 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.056059 | val 0.050406 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.11914 | val 0.10299 | noisy 0.10548 | Δ +2.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.11852 | val 0.10237 | noisy 0.10693 | Δ +4.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.10700 | val 0.09111 | noisy 0.11311 | Δ +19.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.11331 | val 0.09075 | noisy 0.11139 | Δ +18.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.10990 | val 0.09103 | noisy 0.11851 | Δ +23.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.10556 | val 0.09361 | noisy 0.11033 | Δ +15.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.10837 | val 0.09392 | noisy 0.11719 | Δ +19.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.10609 | val 0.10340 | noisy 0.10278 | Δ -0.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.10672 | val 0.08894 | noisy 0.10297 | Δ +13.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.10642 | val 0.08479 | noisy 0.10939 | Δ +22.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.10825 | val 0.08994 | noisy 0.10276 | Δ +12.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.10445 | val 0.08717 | noisy 0.12484 | Δ +30.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.10534 | val 0.09175 | noisy 0.10764 | Δ +14.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.11552 | val 0.09532 | noisy 0.11803 | Δ +19.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.10975 | val 0.09155 | noisy 0.10935 | Δ +16.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.10928 | val 0.09603 | noisy 0.10054 | Δ +4.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.10861 | val 0.09271 | noisy 0.10744 | Δ +13.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.10328 | val 0.09779 | noisy 0.10408 | Δ +6.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.10282 | val 0.09663 | noisy 0.11733 | Δ +17.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.11293 | val 0.09162 | noisy 0.09918 | Δ +7.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.10509 | val 0.09208 | noisy 0.10598 | Δ +13.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.10386 | val 0.09791 | noisy 0.11003 | Δ +11.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.10780 | val 0.10014 | noisy 0.10686 | Δ +6.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.10784 | val 0.09284 | noisy 0.10484 | Δ +11.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.11376 | val 0.09117 | noisy 0.09786 | Δ +6.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.10973 | val 0.09505 | noisy 0.10624 | Δ +10.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.11572 | val 0.09544 | noisy 0.10623 | Δ +10.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 027 | train 0.10273 | val 0.09671 | noisy 0.10871 | Δ +11.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 028 | train 0.10465 | val 0.09108 | noisy 0.10625 | Δ +14.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 029 | train 0.11003 | val 0.10554 | noisy 0.10653 | Δ +0.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 030 | train 0.11693 | val 0.09786 | noisy 0.10872 | Δ +10.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 031 | train 0.10278 | val 0.10284 | noisy 0.10682 | Δ +3.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 032 | train 0.11153 | val 0.08666 | noisy 0.10147 | Δ +14.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 033 | train 0.10893 | val 0.10253 | noisy 0.10380 | Δ +1.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 034 | train 0.10031 | val 0.09575 | noisy 0.09720 | Δ +1.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 035 | train 0.10380 | val 0.09791 | noisy 0.10254 | Δ +4.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 036 | train 0.11102 | val 0.08710 | noisy 0.10589 | Δ +17.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 037 | train 0.11583 | val 0.10583 | noisy 0.10637 | Δ +0.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 038 | train 0.10325 | val 0.09381 | noisy 0.10173 | Δ +7.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 039 | train 0.09921 | val 0.09205 | noisy 0.10638 | Δ +13.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 040 | train 0.11278 | val 0.09573 | noisy 0.10146 | Δ +5.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 041 | train 0.10780 | val 0.09105 | noisy 0.10689 | Δ +14.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 042 | train 0.11359 | val 0.09578 | noisy 0.10282 | Δ +6.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 043 | train 0.10890 | val 0.09459 | noisy 0.09807 | Δ +3.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 044 | train 0.10923 | val 0.08895 | noisy 0.11156 | Δ +20.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 045 | train 0.11088 | val 0.08975 | noisy 0.09026 | Δ +0.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 046 | train 0.11458 | val 0.09314 | noisy 0.11658 | Δ +20.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 047 | train 0.10970 | val 0.08312 | noisy 0.10068 | Δ +17.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 048 | train 0.10345 | val 0.09173 | noisy 0.10770 | Δ +14.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 049 | train 0.10018 | val 0.08849 | noisy 0.11488 | Δ +23.0% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.062373 | val 0.057352 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.061119 | val 0.054689 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.059491 | val 0.052967 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.060258 | val 0.051928 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.058038 | val 0.051409 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.058131 | val 0.051055 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.057448 | val 0.050937 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.057492 | val 0.051024 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.057191 | val 0.051163 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.057570 | val 0.051181 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.055203 | val 0.051233 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.056367 | val 0.051241 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.054767 | val 0.051290 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.056354 | val 0.051300 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.055587 | val 0.051065 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.055627 | val 0.050836 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.055546 | val 0.050674 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.054106 | val 0.050357 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.054437 | val 0.050084 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.053462 | val 0.049824 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.054763 | val 0.049655 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.054687 | val 0.049516 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.054189 | val 0.049312 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.052400 | val 0.049255 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.053897 | val 0.049144 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.052800 | val 0.048959 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.053041 | val 0.048986 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.052544 | val 0.048724 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.052262 | val 0.048684 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.051973 | val 0.048501 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.050749 | val 0.048313 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.050440 | val 0.047979 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.050825 | val 0.047541 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.049530 | val 0.047444 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.050195 | val 0.047201 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.050739 | val 0.046463 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.049954 | val 0.045702 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.048270 | val 0.045297 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.048148 | val 0.044694 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.046569 | val 0.044132 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.046434 | val 0.043540 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.046132 | val 0.043011 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.044810 | val 0.042335 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.044698 | val 0.041659 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.043090 | val 0.040772 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.042941 | val 0.040154 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.042497 | val 0.039611 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.041357 | val 0.039400 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.040176 | val 0.038954 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.039181 | val 0.037702 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.038157 | val 0.036747 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.037834 | val 0.036133 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.037232 | val 0.036235 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.036248 | val 0.035901 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.034928 | val 0.035364 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.034466 | val 0.033819 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.033766 | val 0.032864 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.033181 | val 0.031944 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.032829 | val 0.031575 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.031922 | val 0.031270 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.030942 | val 0.030277 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.030533 | val 0.029821 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.030066 | val 0.028812 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.029019 | val 0.027830 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.028429 | val 0.027117 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.027453 | val 0.026994 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.027272 | val 0.026952 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.026615 | val 0.026430 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.026573 | val 0.025753 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.025633 | val 0.025234 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.025895 | val 0.024668 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.025707 | val 0.024386 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.025312 | val 0.024446 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.024888 | val 0.024801 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.025217 | val 0.024697 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.024587 | val 0.024130 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.024993 | val 0.023552 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.024619 | val 0.023423 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.024390 | val 0.023321 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.024693 | val 0.023607 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.024196 | val 0.023624 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.023903 | val 0.023546 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.023354 | val 0.022966 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.023277 | val 0.022797 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.022897 | val 0.022892 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.022894 | val 0.023013 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.023065 | val 0.022943 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.022641 | val 0.023012 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.022493 | val 0.022599 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.022574 | val 0.022202 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.021723 | val 0.021892 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.021923 | val 0.021888 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.021919 | val 0.021830 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.021537 | val 0.022142 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.021456 | val 0.021808 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.021076 | val 0.021614 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.020869 | val 0.021246 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.020660 | val 0.021099 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.020729 | val 0.021197 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.019924 | val 0.020868 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.019769 | val 0.020765 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.019296 | val 0.020368 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.018676 | val 0.020060 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.018719 | val 0.019829 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.018529 | val 0.019342 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.018010 | val 0.018908 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.018019 | val 0.018624 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.017128 | val 0.018622 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.016742 | val 0.018149 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.016653 | val 0.017656 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.016323 | val 0.017032 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.015688 | val 0.016531 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.015351 | val 0.016102 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.014551 | val 0.015656 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.013873 | val 0.015023 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.013658 | val 0.014410 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.013084 | val 0.013795 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.012403 | val 0.013272 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.012156 | val 0.012877 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.011698 | val 0.012382 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.30261 | val 0.26288 | noisy 0.29733 | Δ +11.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.27785 | val 0.24628 | noisy 0.28674 | Δ +14.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.26344 | val 0.23035 | noisy 0.29082 | Δ +20.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.24355 | val 0.20245 | noisy 0.28840 | Δ +29.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.23220 | val 0.19899 | noisy 0.29682 | Δ +33.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.22357 | val 0.18611 | noisy 0.29566 | Δ +37.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.20553 | val 0.17549 | noisy 0.28774 | Δ +39.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.19477 | val 0.16585 | noisy 0.27883 | Δ +40.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.18863 | val 0.15864 | noisy 0.29373 | Δ +46.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.18652 | val 0.15562 | noisy 0.28712 | Δ +45.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.17394 | val 0.14873 | noisy 0.29135 | Δ +49.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.17220 | val 0.14208 | noisy 0.30055 | Δ +52.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.16338 | val 0.13791 | noisy 0.28752 | Δ +52.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.15125 | val 0.13899 | noisy 0.28929 | Δ +52.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.15482 | val 0.13383 | noisy 0.29164 | Δ +54.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.14758 | val 0.12629 | noisy 0.28151 | Δ +55.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.14371 | val 0.12525 | noisy 0.28088 | Δ +55.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.14259 | val 0.12770 | noisy 0.28085 | Δ +54.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.14134 | val 0.12604 | noisy 0.29781 | Δ +57.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.13847 | val 0.12679 | noisy 0.28919 | Δ +56.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.13924 | val 0.12262 | noisy 0.27920 | Δ +56.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.13764 | val 0.12353 | noisy 0.28788 | Δ +57.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.13128 | val 0.12146 | noisy 0.28293 | Δ +57.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.12912 | val 0.11424 | noisy 0.29033 | Δ +60.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.12854 | val 0.11487 | noisy 0.28824 | Δ +60.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.12616 | val 0.11401 | noisy 0.28869 | Δ +60.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.12109 | val 0.11119 | noisy 0.28608 | Δ +61.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 027 | train 0.11704 | val 0.10726 | noisy 0.29519 | Δ +63.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 028 | train 0.11637 | val 0.10673 | noisy 0.29134 | Δ +63.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 029 | train 0.11176 | val 0.10327 | noisy 0.29304 | Δ +64.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 030 | train 0.10810 | val 0.10053 | noisy 0.29701 | Δ +66.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 031 | train 0.10221 | val 0.10133 | noisy 0.28831 | Δ +64.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 032 | train 0.10342 | val 0.09366 | noisy 0.28956 | Δ +67.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 033 | train 0.09881 | val 0.08967 | noisy 0.29060 | Δ +69.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 034 | train 0.09445 | val 0.08219 | noisy 0.28624 | Δ +71.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 035 | train 0.09295 | val 0.08380 | noisy 0.29583 | Δ +71.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 036 | train 0.08716 | val 0.07610 | noisy 0.29482 | Δ +74.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 037 | train 0.08312 | val 0.07009 | noisy 0.29853 | Δ +76.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 038 | train 0.08408 | val 0.07274 | noisy 0.28632 | Δ +74.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 039 | train 0.07579 | val 0.06274 | noisy 0.28401 | Δ +77.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 040 | train 0.07867 | val 0.06525 | noisy 0.28460 | Δ +77.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 041 | train 0.06898 | val 0.05927 | noisy 0.27682 | Δ +78.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 042 | train 0.06675 | val 0.06584 | noisy 0.28548 | Δ +76.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 043 | train 0.06743 | val 0.05688 | noisy 0.29344 | Δ +80.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 044 | train 0.06649 | val 0.05489 | noisy 0.29035 | Δ +81.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 045 | train 0.07026 | val 0.05858 | noisy 0.29718 | Δ +80.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 046 | train 0.06195 | val 0.04991 | noisy 0.29037 | Δ +82.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 047 | train 0.06060 | val 0.05203 | noisy 0.29838 | Δ +82.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 048 | train 0.05593 | val 0.05297 | noisy 0.29019 | Δ +81.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 049 | train 0.06425 | val 0.04989 | noisy 0.29124 | Δ +82.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 050 | train 0.05805 | val 0.05436 | noisy 0.28827 | Δ +81.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 051 | train 0.06202 | val 0.05061 | noisy 0.29699 | Δ +83.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 052 | train 0.05600 | val 0.05157 | noisy 0.28235 | Δ +81.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 053 | train 0.05766 | val 0.05724 | noisy 0.28723 | Δ +80.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 054 | train 0.05382 | val 0.05410 | noisy 0.28576 | Δ +81.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 055 | train 0.05738 | val 0.05154 | noisy 0.29972 | Δ +82.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 056 | train 0.05739 | val 0.05098 | noisy 0.28722 | Δ +82.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 057 | train 0.05253 | val 0.04589 | noisy 0.28973 | Δ +84.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 058 | train 0.05707 | val 0.04412 | noisy 0.29722 | Δ +85.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 059 | train 0.05359 | val 0.04407 | noisy 0.29394 | Δ +85.0% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.062508 | val 0.056290 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.058765 | val 0.053879 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.056982 | val 0.052519 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.058101 | val 0.051705 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.059560 | val 0.051112 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.056988 | val 0.050815 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.058643 | val 0.050767 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.056721 | val 0.050838 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.058149 | val 0.050917 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.055858 | val 0.051166 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.057245 | val 0.051333 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.055973 | val 0.051560 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.056075 | val 0.051579 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.055427 | val 0.051516 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.056923 | val 0.051307 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.055683 | val 0.051085 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.053626 | val 0.050715 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.056203 | val 0.050441 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.055154 | val 0.050219 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.055053 | val 0.049957 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.054424 | val 0.049850 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.053432 | val 0.049700 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.055113 | val 0.049540 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.054355 | val 0.049392 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.052829 | val 0.049351 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.052532 | val 0.049132 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.053559 | val 0.048836 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.052373 | val 0.048651 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.053615 | val 0.048544 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.051759 | val 0.048385 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.050170 | val 0.048183 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.051698 | val 0.047864 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.051321 | val 0.047592 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.051039 | val 0.047463 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.050201 | val 0.047331 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.049862 | val 0.047176 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.050667 | val 0.046779 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.049540 | val 0.046257 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.049390 | val 0.045930 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.048551 | val 0.045699 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.048016 | val 0.045206 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.047223 | val 0.044710 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.047067 | val 0.044200 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.046184 | val 0.043314 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.046195 | val 0.042487 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.044294 | val 0.041979 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.043980 | val 0.041415 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.042555 | val 0.040592 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.041498 | val 0.039661 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.040127 | val 0.038722 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.039366 | val 0.037715 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.037359 | val 0.036648 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.036483 | val 0.035518 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.035312 | val 0.034509 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.034360 | val 0.033455 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.033068 | val 0.032320 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.031157 | val 0.031492 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.030460 | val 0.030455 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.029121 | val 0.029421 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.028142 | val 0.028561 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.027176 | val 0.027843 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.026270 | val 0.026963 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.025513 | val 0.026022 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.024421 | val 0.025225 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.023922 | val 0.024328 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.023042 | val 0.023780 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.022410 | val 0.023063 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.021865 | val 0.022849 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.021717 | val 0.022577 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.020871 | val 0.022253 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.020697 | val 0.021927 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.020390 | val 0.021456 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.020320 | val 0.021081 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.020029 | val 0.020982 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.019851 | val 0.021244 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.019680 | val 0.020936 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.019716 | val 0.020694 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.019645 | val 0.020413 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.019442 | val 0.020032 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.019378 | val 0.019883 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.018904 | val 0.020259 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.019180 | val 0.020110 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.018862 | val 0.019723 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.019127 | val 0.019479 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.018562 | val 0.019917 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.019031 | val 0.020066 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.018924 | val 0.020041 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.018686 | val 0.019787 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.018106 | val 0.019279 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.018924 | val 0.019187 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.018324 | val 0.019510 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.018880 | val 0.019900 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.018239 | val 0.019508 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.018111 | val 0.019039 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.018267 | val 0.019251 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.018162 | val 0.019416 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.018148 | val 0.019540 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.017961 | val 0.019449 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.018071 | val 0.019269 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.017824 | val 0.018937 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.017921 | val 0.018716 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.017718 | val 0.018621 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.017887 | val 0.018818 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.017547 | val 0.019077 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.017530 | val 0.019286 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.017625 | val 0.019324 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.017939 | val 0.018848 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.017640 | val 0.018538 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.017289 | val 0.018580 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.017462 | val 0.018722 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.017131 | val 0.018930 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.016936 | val 0.018903 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.017301 | val 0.019023 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.016901 | val 0.018580 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.017001 | val 0.018173 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.016897 | val 0.018058 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.016676 | val 0.018153 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.017005 | val 0.018231 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.016659 | val 0.018169 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.016195 | val 0.018134 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.34659 | val 0.33540 | noisy 0.35953 | Δ +6.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.32598 | val 0.29762 | noisy 0.35422 | Δ +16.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.29856 | val 0.27584 | noisy 0.35728 | Δ +22.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.27876 | val 0.26258 | noisy 0.35662 | Δ +26.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.25759 | val 0.24674 | noisy 0.35721 | Δ +30.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.24646 | val 0.22620 | noisy 0.35402 | Δ +36.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.23590 | val 0.21667 | noisy 0.35660 | Δ +39.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.22582 | val 0.21110 | noisy 0.35566 | Δ +40.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.21528 | val 0.19791 | noisy 0.35244 | Δ +43.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.20565 | val 0.19630 | noisy 0.34905 | Δ +43.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.19902 | val 0.18770 | noisy 0.35379 | Δ +46.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.18517 | val 0.18214 | noisy 0.35990 | Δ +49.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.18135 | val 0.17421 | noisy 0.35560 | Δ +51.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.17659 | val 0.16737 | noisy 0.35672 | Δ +53.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.17191 | val 0.16008 | noisy 0.35800 | Δ +55.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.16591 | val 0.16115 | noisy 0.35814 | Δ +55.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.16510 | val 0.14867 | noisy 0.35844 | Δ +58.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.15822 | val 0.15090 | noisy 0.35651 | Δ +57.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.15612 | val 0.13949 | noisy 0.36109 | Δ +61.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.14516 | val 0.13869 | noisy 0.35660 | Δ +61.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.13923 | val 0.13046 | noisy 0.34473 | Δ +62.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.13329 | val 0.13097 | noisy 0.35359 | Δ +63.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.12899 | val 0.11884 | noisy 0.35724 | Δ +66.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.12429 | val 0.10981 | noisy 0.34813 | Δ +68.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.11820 | val 0.11285 | noisy 0.35201 | Δ +67.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.11108 | val 0.10774 | noisy 0.35958 | Δ +70.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.10451 | val 0.10667 | noisy 0.35832 | Δ +70.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 027 | train 0.10606 | val 0.10389 | noisy 0.35858 | Δ +71.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 028 | train 0.10219 | val 0.09669 | noisy 0.35855 | Δ +73.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 029 | train 0.09601 | val 0.09478 | noisy 0.35972 | Δ +73.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 030 | train 0.09845 | val 0.09454 | noisy 0.35701 | Δ +73.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 031 | train 0.08931 | val 0.07937 | noisy 0.35085 | Δ +77.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 032 | train 0.08999 | val 0.09800 | noisy 0.35971 | Δ +72.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 033 | train 0.09284 | val 0.08849 | noisy 0.35914 | Δ +75.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 034 | train 0.08706 | val 0.08261 | noisy 0.36020 | Δ +77.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 035 | train 0.08426 | val 0.08185 | noisy 0.36258 | Δ +77.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 036 | train 0.08569 | val 0.07804 | noisy 0.35406 | Δ +78.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 037 | train 0.08070 | val 0.08047 | noisy 0.35601 | Δ +77.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 038 | train 0.07628 | val 0.08117 | noisy 0.35522 | Δ +77.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 039 | train 0.07500 | val 0.07374 | noisy 0.35874 | Δ +79.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 040 | train 0.07456 | val 0.07498 | noisy 0.35738 | Δ +79.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 041 | train 0.07736 | val 0.07196 | noisy 0.35417 | Δ +79.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 042 | train 0.07520 | val 0.08259 | noisy 0.35501 | Δ +76.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 043 | train 0.07646 | val 0.07252 | noisy 0.36408 | Δ +80.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 044 | train 0.07218 | val 0.06628 | noisy 0.35434 | Δ +81.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 045 | train 0.06765 | val 0.06870 | noisy 0.35588 | Δ +80.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 046 | train 0.06491 | val 0.06626 | noisy 0.35833 | Δ +81.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 047 | train 0.06517 | val 0.06773 | noisy 0.35259 | Δ +80.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 048 | train 0.06717 | val 0.07189 | noisy 0.35496 | Δ +79.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 049 | train 0.06333 | val 0.06326 | noisy 0.35659 | Δ +82.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 050 | train 0.06344 | val 0.07193 | noisy 0.35681 | Δ +79.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 051 | train 0.06142 | val 0.06747 | noisy 0.36120 | Δ +81.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 052 | train 0.06224 | val 0.05969 | noisy 0.35328 | Δ +83.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 053 | train 0.05940 | val 0.06481 | noisy 0.35592 | Δ +81.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 054 | train 0.06118 | val 0.06921 | noisy 0.35427 | Δ +80.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 055 | train 0.06011 | val 0.06743 | noisy 0.35507 | Δ +81.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 056 | train 0.05610 | val 0.06157 | noisy 0.35396 | Δ +82.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 057 | train 0.06528 | val 0.06390 | noisy 0.36171 | Δ +82.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 058 | train 0.05898 | val 0.05981 | noisy 0.35495 | Δ +83.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 059 | train 0.06244 | val 0.07096 | noisy 0.35020 | Δ +79.7% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.062656 | val 0.057301 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.059985 | val 0.055453 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.059780 | val 0.053697 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.057892 | val 0.052534 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.057308 | val 0.051696 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.057858 | val 0.051064 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.057326 | val 0.050768 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.057685 | val 0.050651 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.057828 | val 0.050662 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.055638 | val 0.050835 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.055627 | val 0.050922 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.055743 | val 0.050985 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.056191 | val 0.051021 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.055722 | val 0.051099 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.056356 | val 0.051116 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.056098 | val 0.051086 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.055525 | val 0.050956 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.056200 | val 0.050714 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.055189 | val 0.050675 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.054314 | val 0.050666 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.055482 | val 0.050422 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.055610 | val 0.050235 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.055583 | val 0.050100 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.053600 | val 0.050003 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.054706 | val 0.049977 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.054379 | val 0.049686 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.052207 | val 0.049378 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.052788 | val 0.049068 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.052863 | val 0.048862 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.053394 | val 0.048610 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.051287 | val 0.048306 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.051472 | val 0.048053 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.050706 | val 0.047500 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.051175 | val 0.047072 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.049853 | val 0.046683 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.049008 | val 0.045980 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.048669 | val 0.045625 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.047513 | val 0.044992 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.046082 | val 0.044281 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.045046 | val 0.043429 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.044411 | val 0.042861 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.043045 | val 0.041831 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.042590 | val 0.040809 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.041511 | val 0.039926 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.040599 | val 0.038979 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.039543 | val 0.037752 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.038181 | val 0.036569 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.037910 | val 0.035168 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.036579 | val 0.034474 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.035361 | val 0.033831 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.034537 | val 0.032812 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.033726 | val 0.031694 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.032514 | val 0.030601 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.031981 | val 0.029967 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.030915 | val 0.029073 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.030708 | val 0.028158 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.029677 | val 0.027758 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.029086 | val 0.027324 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.028832 | val 0.026688 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.028167 | val 0.026044 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.028324 | val 0.025116 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.027646 | val 0.025032 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.027052 | val 0.024365 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.026134 | val 0.024001 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.026346 | val 0.023428 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.025961 | val 0.023386 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.026008 | val 0.022760 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.025110 | val 0.022567 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.024899 | val 0.022410 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.025245 | val 0.021791 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.024965 | val 0.021226 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.024839 | val 0.021343 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.023869 | val 0.021915 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.024055 | val 0.021967 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.024060 | val 0.021098 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.023176 | val 0.020699 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.023441 | val 0.020365 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.023080 | val 0.020341 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.022600 | val 0.020359 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.022780 | val 0.020480 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.022398 | val 0.020207 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.022157 | val 0.019882 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.022309 | val 0.019464 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.021623 | val 0.019539 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.021774 | val 0.019699 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.021335 | val 0.020496 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.020995 | val 0.019994 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.021473 | val 0.019191 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.020630 | val 0.018658 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.020720 | val 0.018955 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.020250 | val 0.019501 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.020225 | val 0.019484 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.019728 | val 0.019102 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.019752 | val 0.018337 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.019486 | val 0.017713 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.019499 | val 0.017783 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.019071 | val 0.017893 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.018347 | val 0.018131 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.018775 | val 0.018029 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.018468 | val 0.018054 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.018328 | val 0.017561 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.017747 | val 0.017725 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.018104 | val 0.017391 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.017737 | val 0.016710 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.017547 | val 0.016438 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.017382 | val 0.016526 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.017182 | val 0.016551 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.016732 | val 0.016712 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.016417 | val 0.016761 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.016682 | val 0.016522 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.016127 | val 0.015808 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.016381 | val 0.015415 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.015848 | val 0.014971 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.016164 | val 0.015182 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.015438 | val 0.015547 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.015755 | val 0.015988 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.015704 | val 0.015597 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.015294 | val 0.015273 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.014915 | val 0.014810 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.014931 | val 0.014285 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.30343 | val 0.30365 | noisy 0.32762 | Δ +7.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.28713 | val 0.29145 | noisy 0.31668 | Δ +8.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.26581 | val 0.25884 | noisy 0.34161 | Δ +24.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.25199 | val 0.23886 | noisy 0.33719 | Δ +29.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.23440 | val 0.23249 | noisy 0.32473 | Δ +28.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.22149 | val 0.21546 | noisy 0.32630 | Δ +34.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.21002 | val 0.20452 | noisy 0.33258 | Δ +38.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.20398 | val 0.19565 | noisy 0.32838 | Δ +40.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.19395 | val 0.18373 | noisy 0.31965 | Δ +42.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.18562 | val 0.17264 | noisy 0.32090 | Δ +46.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.18122 | val 0.16645 | noisy 0.31599 | Δ +47.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.16751 | val 0.15736 | noisy 0.31845 | Δ +50.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.15986 | val 0.15025 | noisy 0.32090 | Δ +53.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.15423 | val 0.14515 | noisy 0.33758 | Δ +57.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.14767 | val 0.14192 | noisy 0.33459 | Δ +57.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.14082 | val 0.13135 | noisy 0.33503 | Δ +60.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.13093 | val 0.12318 | noisy 0.33132 | Δ +62.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.13122 | val 0.11760 | noisy 0.33046 | Δ +64.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.12912 | val 0.11359 | noisy 0.32686 | Δ +65.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.12343 | val 0.11033 | noisy 0.31950 | Δ +65.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.11656 | val 0.11073 | noisy 0.33084 | Δ +66.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.11167 | val 0.10014 | noisy 0.33084 | Δ +69.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.10753 | val 0.10614 | noisy 0.33039 | Δ +67.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.10904 | val 0.09125 | noisy 0.32281 | Δ +71.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.10494 | val 0.09459 | noisy 0.32612 | Δ +71.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.09992 | val 0.09237 | noisy 0.33032 | Δ +72.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.10354 | val 0.08818 | noisy 0.32705 | Δ +73.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 027 | train 0.09424 | val 0.08315 | noisy 0.32466 | Δ +74.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 028 | train 0.09486 | val 0.08404 | noisy 0.31523 | Δ +73.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 029 | train 0.09721 | val 0.07716 | noisy 0.33046 | Δ +76.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 030 | train 0.09331 | val 0.08679 | noisy 0.33662 | Δ +74.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 031 | train 0.09086 | val 0.07782 | noisy 0.32307 | Δ +75.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 032 | train 0.08980 | val 0.07137 | noisy 0.33491 | Δ +78.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 033 | train 0.08748 | val 0.07299 | noisy 0.32661 | Δ +77.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 034 | train 0.08204 | val 0.07552 | noisy 0.32757 | Δ +76.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 035 | train 0.08511 | val 0.06961 | noisy 0.34058 | Δ +79.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 036 | train 0.07872 | val 0.07125 | noisy 0.32719 | Δ +78.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 037 | train 0.08669 | val 0.06313 | noisy 0.33419 | Δ +81.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 038 | train 0.07739 | val 0.06451 | noisy 0.33499 | Δ +80.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 039 | train 0.07784 | val 0.06141 | noisy 0.32372 | Δ +81.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 040 | train 0.07706 | val 0.06528 | noisy 0.32625 | Δ +80.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 041 | train 0.07463 | val 0.06194 | noisy 0.33174 | Δ +81.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 042 | train 0.06859 | val 0.06232 | noisy 0.32888 | Δ +81.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 043 | train 0.07075 | val 0.05945 | noisy 0.33345 | Δ +82.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 044 | train 0.07168 | val 0.05599 | noisy 0.32413 | Δ +82.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 045 | train 0.06353 | val 0.05975 | noisy 0.31849 | Δ +81.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 046 | train 0.06187 | val 0.05632 | noisy 0.32879 | Δ +82.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 047 | train 0.06034 | val 0.04862 | noisy 0.32698 | Δ +85.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 048 | train 0.05869 | val 0.05790 | noisy 0.33093 | Δ +82.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 049 | train 0.06041 | val 0.04916 | noisy 0.31658 | Δ +84.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 050 | train 0.05498 | val 0.05488 | noisy 0.33014 | Δ +83.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 051 | train 0.06351 | val 0.04418 | noisy 0.32341 | Δ +86.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 052 | train 0.05517 | val 0.05016 | noisy 0.31921 | Δ +84.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 053 | train 0.05368 | val 0.04467 | noisy 0.32249 | Δ +86.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 054 | train 0.05475 | val 0.05099 | noisy 0.32671 | Δ +84.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 055 | train 0.05750 | val 0.04382 | noisy 0.32840 | Δ +86.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 056 | train 0.05531 | val 0.05042 | noisy 0.32691 | Δ +84.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 057 | train 0.05410 | val 0.04366 | noisy 0.32596 | Δ +86.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 058 | train 0.04964 | val 0.04424 | noisy 0.32326 | Δ +86.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 059 | train 0.05519 | val 0.04998 | noisy 0.31065 | Δ +83.9% | LR 0.01000\n",
      "\n",
      "Completed 10 runs.\n"
     ]
    }
   ],
   "source": [
    "RUNS = []\n",
    "\n",
    "for L in LAYER_OPTIONS:\n",
    "    for inst in INSTANCE_IDS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Instance {inst} | Layers {L}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        s1 = train_stage1(\n",
    "            X_train, X_val,\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=120, batch_size=32,\n",
    "            lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6,\n",
    "            seed=inst  # <-- ZMIANA\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        s3 = train_stage3(\n",
    "            X_train, X_val,\n",
    "            dec_dagger_params=s1[\"phi\"],\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=60, batch_size=16,\n",
    "            lr_init=0.010, noise_level=EVAL_SIGMA, seed=inst  # <-- już było\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        RUNS.append({\n",
    "            \"instance_id\": inst,\n",
    "            \"n_layers\": L,\n",
    "            \"stage1\": {\n",
    "                \"phi\": s1[\"phi\"],\n",
    "                \"best_val\": s1[\"best_val\"],\n",
    "                \"hist_train\": s1[\"hist_train\"],\n",
    "                \"hist_val\": s1[\"hist_val\"],\n",
    "                \"hist_lr\": s1[\"hist_lr\"],\n",
    "                \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                \"epochs\": s1.get(\"epochs\"),\n",
    "                \"train_seconds\": float(t1 - t0),\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"psi\": s3[\"psi\"],\n",
    "                \"best_val\": s3[\"best_val\"],\n",
    "                \"hist_train\": s3[\"hist_train\"],\n",
    "                \"hist_val\": s3[\"hist_val\"],\n",
    "                \"hist_lr\": s3[\"hist_lr\"],\n",
    "                \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                \"epochs\": s3.get(\"epochs\"),\n",
    "                \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": float(t2 - t1),\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a537537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle → ./runs_halfqae/q4_l3t1/L1\\4q_3l_1t_1ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l3t1/L1\\4q_3l_1t_1ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l3t1/L1\\4q_3l_1t_1ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l3t1/L1\\4q_3l_1t_1ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l3t1/L1\\4q_3l_1t_1ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l3t1/L3\\4q_3l_1t_3ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l3t1/L3\\4q_3l_1t_3ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l3t1/L3\\4q_3l_1t_3ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l3t1/L3\\4q_3l_1t_3ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l3t1/L3\\4q_3l_1t_3ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v3.csv\n",
      "\n",
      "All runs saved and recorded in CSV.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 7 — Save artifacts (JSON) and append a paper-ready CSV per run\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams logged (keep in sync with training cells) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.010\n",
    "S3_MAX_EPOCHS    = 60\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v3\"  # bump if you change columns\n",
    "\n",
    "# --- ensure dirs ---\n",
    "def ensure_dir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "ensure_dir(OUT_BASE)\n",
    "subroot = ensure_dir(f\"{OUT_BASE}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "\n",
    "# --- CSV path (versioned) ---\n",
    "CSV_PATH = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "\n",
    "# --- header for the full, paper-friendly table ---\n",
    "CSV_HEADER = [\n",
    "    # id / naming\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    # architecture\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    # noise & window\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    # stage-1 hyperparams + outcomes\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    # stage-3 hyperparams + outcomes\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    # params (JSON)\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    # totals\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    needs_header = True\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                first_line = f.readline().rstrip(\"\\n\")\n",
    "            needs_header = (first_line != \",\".join(header))\n",
    "        except Exception:\n",
    "            needs_header = True\n",
    "    if needs_header:\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(header)\n",
    "\n",
    "ensure_csv(CSV_PATH, CSV_HEADER)\n",
    "\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def save_one_run(run):\n",
    "    inst = int(run[\"instance_id\"])\n",
    "    L    = int(run[\"n_layers\"])\n",
    "    seed = int(run.get(\"seed\", inst))\n",
    "\n",
    "    # standardized filename: 4q_3l_1t_{L}ls_{inst:02d}.json\n",
    "    fname = f\"4q_3l_1t_{L}ls_{inst:02d}.json\"\n",
    "    out_dir = ensure_dir(f\"{subroot}/L{L}\")\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    # pull stage results (robust to missing keys)\n",
    "    s1 = run[\"stage1\"]\n",
    "    s3 = run[\"stage3\"]\n",
    "\n",
    "    # Stage-1 metrics\n",
    "    s1_hist_val = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val = float(s1.get(\"best_val\", np.nan))\n",
    "    s1_final_val = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = int(s1.get(\"best_epoch\", _safe_argmin(s1_hist_val)))\n",
    "    s1_epochs = int(s1.get(\"epochs\", len(s1_hist_val)))\n",
    "    s1_seconds = float(s1.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # Stage-3 metrics\n",
    "    s3_hist_val = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "    s3_best_val = float(s3.get(\"best_val\", np.nan))\n",
    "    s3_final_val = _safe_last(s3_hist_val)\n",
    "    s3_best_epoch = int(s3.get(\"best_epoch\", _safe_argmin(s3_hist_val)))\n",
    "    s3_epochs = int(s3.get(\"epochs\", len(s3_hist_val)))\n",
    "    s3_seconds = float(s3.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # --- compute metrics with FALLBACKS if curves are missing ---\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = (float(np.nanmax(s3_hist_delta)) if (len(s3_hist_delta) and np.isfinite(np.nanmax(s3_hist_delta)))\n",
    "                      else np.nan)\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    # bundle JSON (parameters + training curves)\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"scale_low\":  float(scale_low),\n",
    "            \"scale_high\": float(scale_high),\n",
    "            \"window_size\": int(n_qubits),\n",
    "            \"window_stride\": 1,\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(EVAL_SIGMA),\n",
    "            \"sigma_eval\":  float(EVAL_SIGMA),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(n_qubits),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(n_latent),\n",
    "            \"n_trash\":  int(n_trash),\n",
    "            \"latent_wires\": list(range(n_latent)),\n",
    "            \"trash_wires\":  list(range(n_latent, n_qubits)),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val, \"lr_curve\": s3.get(\"hist_lr\", []),\n",
    "                \"noisy_curve\": s3_hist_noisy, \"delta_curve\": s3_hist_delta,\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # assemble CSV row\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        OUT_BASE,\n",
    "        inst, seed,\n",
    "        int(n_qubits), int(n_latent), int(n_trash), int(L),\n",
    "        f\"{EVAL_SIGMA:.3f}\", f\"{EVAL_SIGMA:.3f}\", 1,\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS),\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert row into CSV\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(CSV_PATH).exists():\n",
    "        df_old = pd.read_csv(CSV_PATH)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(CSV_PATH, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Upserted CSV row  → {CSV_PATH}\")\n",
    "\n",
    "# ---- save all runs from Cell 7 ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved and recorded in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b94c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae/all_training_instances_v3.csv\n",
      "Saved per-layer summary → ./runs_halfqae/summary_by_layers_v3.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4q_3l_1t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093889</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>122.600948</td>\n",
       "      <td>0.098557</td>\n",
       "      <td>16.734282</td>\n",
       "      <td>1.314132</td>\n",
       "      <td>[0.07920300593268884, 0.1741352513847527, -0.0...</td>\n",
       "      <td>[0.0366161477584314, -0.13505197325611903, -0....</td>\n",
       "      <td>223.291317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4q_3l_1t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091986</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>148.427447</td>\n",
       "      <td>0.095022</td>\n",
       "      <td>16.231979</td>\n",
       "      <td>-2.753940</td>\n",
       "      <td>[-0.1419920046602157, 0.17041320656823505, 0.0...</td>\n",
       "      <td>[-0.04059580305207794, -0.1550606613555467, 0....</td>\n",
       "      <td>314.273312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4q_3l_1t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082347</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>87.759136</td>\n",
       "      <td>0.098217</td>\n",
       "      <td>17.082315</td>\n",
       "      <td>16.247384</td>\n",
       "      <td>[-0.09213537167300323, 0.17792055054259284, 0....</td>\n",
       "      <td>[0.16011008854084957, -0.12812737228335272, -0...</td>\n",
       "      <td>200.461899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4q_3l_1t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094989</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>107.617503</td>\n",
       "      <td>0.101407</td>\n",
       "      <td>17.429752</td>\n",
       "      <td>13.082997</td>\n",
       "      <td>[0.01920366015605288, 0.19949546744272428, -0....</td>\n",
       "      <td>[-0.00714619063878673, -0.132966307835624, 0.0...</td>\n",
       "      <td>234.036083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4q_3l_1t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089805</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>156.167812</td>\n",
       "      <td>0.098323</td>\n",
       "      <td>22.750509</td>\n",
       "      <td>5.944734</td>\n",
       "      <td>[-0.006951072014653309, 0.19005931260571107, 0...</td>\n",
       "      <td>[0.05768951136773057, -0.23974031577343044, -0...</td>\n",
       "      <td>309.764705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4q_3l_1t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094036</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>907.662600</td>\n",
       "      <td>0.110019</td>\n",
       "      <td>24.199576</td>\n",
       "      <td>19.290737</td>\n",
       "      <td>[0.004985296223276446, 0.00476188496708803, -0...</td>\n",
       "      <td>[-0.053674728899125806, -0.23733318852153798, ...</td>\n",
       "      <td>1360.208073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4q_3l_1t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088488</td>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>1242.876786</td>\n",
       "      <td>0.106812</td>\n",
       "      <td>30.178496</td>\n",
       "      <td>22.973731</td>\n",
       "      <td>[0.005943542272704687, -0.005901366808950797, ...</td>\n",
       "      <td>[-0.06415235865224381, -0.19645577673833928, 0...</td>\n",
       "      <td>1626.111450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4q_3l_1t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044067</td>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>2711.642654</td>\n",
       "      <td>0.289907</td>\n",
       "      <td>85.154858</td>\n",
       "      <td>85.007987</td>\n",
       "      <td>[-0.16021938627037816, 0.16815956481981176, 0....</td>\n",
       "      <td>[0.06863906304281042, -0.18745406220578748, -0...</td>\n",
       "      <td>4682.675478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4q_3l_1t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070960</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>2035.343617</td>\n",
       "      <td>0.356201</td>\n",
       "      <td>83.150482</td>\n",
       "      <td>79.737196</td>\n",
       "      <td>[0.26651923602831495, 0.26923452972840145, -0....</td>\n",
       "      <td>[-0.14633881274005284, -0.020503136254279193, ...</td>\n",
       "      <td>4803.325313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4q_3l_1t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049984</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "      <td>1068.343299</td>\n",
       "      <td>0.327131</td>\n",
       "      <td>86.655977</td>\n",
       "      <td>83.909849</td>\n",
       "      <td>[0.010133178674081058, 0.16217964084610895, 0....</td>\n",
       "      <td>[0.2433202582659158, -0.4172195315114159, -0.2...</td>\n",
       "      <td>2664.366669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag  dataset_folder  instance_id  rng_seed  \\\n",
       "0  4q_3l_1t_1ls_01.json  inst1_L1  ./runs_halfqae            1         1   \n",
       "1  4q_3l_1t_1ls_02.json  inst2_L1  ./runs_halfqae            2         2   \n",
       "2  4q_3l_1t_1ls_03.json  inst3_L1  ./runs_halfqae            3         3   \n",
       "3  4q_3l_1t_1ls_04.json  inst4_L1  ./runs_halfqae            4         4   \n",
       "4  4q_3l_1t_1ls_05.json  inst5_L1  ./runs_halfqae            5         5   \n",
       "5  4q_3l_1t_3ls_01.json  inst1_L3  ./runs_halfqae            1         1   \n",
       "6  4q_3l_1t_3ls_02.json  inst2_L3  ./runs_halfqae            2         2   \n",
       "7  4q_3l_1t_3ls_03.json  inst3_L3  ./runs_halfqae            3         3   \n",
       "8  4q_3l_1t_3ls_04.json  inst4_L3  ./runs_halfqae            4         4   \n",
       "9  4q_3l_1t_3ls_05.json  inst5_L3  ./runs_halfqae            5         5   \n",
       "\n",
       "   n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  s3_final_val_mse  \\\n",
       "0         4         3        1         1          0.1  ...          0.093889   \n",
       "1         4         3        1         1          0.1  ...          0.091986   \n",
       "2         4         3        1         1          0.1  ...          0.082347   \n",
       "3         4         3        1         1          0.1  ...          0.094989   \n",
       "4         4         3        1         1          0.1  ...          0.089805   \n",
       "5         4         3        1         3          0.1  ...          0.094036   \n",
       "6         4         3        1         3          0.1  ...          0.088488   \n",
       "7         4         3        1         3          0.1  ...          0.044067   \n",
       "8         4         3        1         3          0.1  ...          0.070960   \n",
       "9         4         3        1         3          0.1  ...          0.049984   \n",
       "\n",
       "   s3_best_epoch  s3_epochs  s3_train_seconds  s3_noisy_baseline_mse  \\\n",
       "0              4         15        122.600948               0.098557   \n",
       "1              5         16        148.427447               0.095022   \n",
       "2              4         15         87.759136               0.098217   \n",
       "3              3         14        107.617503               0.101407   \n",
       "4             13         24        156.167812               0.098323   \n",
       "5             14         25        907.662600               0.110019   \n",
       "6             39         50       1242.876786               0.106812   \n",
       "7             57         60       2711.642654               0.289907   \n",
       "8             56         60       2035.343617               0.356201   \n",
       "9             58         60       1068.343299               0.327131   \n",
       "\n",
       "   s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0          16.734282            1.314132   \n",
       "1          16.231979           -2.753940   \n",
       "2          17.082315           16.247384   \n",
       "3          17.429752           13.082997   \n",
       "4          22.750509            5.944734   \n",
       "5          24.199576           19.290737   \n",
       "6          30.178496           22.973731   \n",
       "7          85.154858           85.007987   \n",
       "8          83.150482           79.737196   \n",
       "9          86.655977           83.909849   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [0.07920300593268884, 0.1741352513847527, -0.0...   \n",
       "1  [-0.1419920046602157, 0.17041320656823505, 0.0...   \n",
       "2  [-0.09213537167300323, 0.17792055054259284, 0....   \n",
       "3  [0.01920366015605288, 0.19949546744272428, -0....   \n",
       "4  [-0.006951072014653309, 0.19005931260571107, 0...   \n",
       "5  [0.004985296223276446, 0.00476188496708803, -0...   \n",
       "6  [0.005943542272704687, -0.005901366808950797, ...   \n",
       "7  [-0.16021938627037816, 0.16815956481981176, 0....   \n",
       "8  [0.26651923602831495, 0.26923452972840145, -0....   \n",
       "9  [0.010133178674081058, 0.16217964084610895, 0....   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [0.0366161477584314, -0.13505197325611903, -0....           223.291317  \n",
       "1  [-0.04059580305207794, -0.1550606613555467, 0....           314.273312  \n",
       "2  [0.16011008854084957, -0.12812737228335272, -0...           200.461899  \n",
       "3  [-0.00714619063878673, -0.132966307835624, 0.0...           234.036083  \n",
       "4  [0.05768951136773057, -0.23974031577343044, -0...           309.764705  \n",
       "5  [-0.053674728899125806, -0.23733318852153798, ...          1360.208073  \n",
       "6  [-0.06415235865224381, -0.19645577673833928, 0...          1626.111450  \n",
       "7  [0.06863906304281042, -0.18745406220578748, -0...          4682.675478  \n",
       "8  [-0.14633881274005284, -0.020503136254279193, ...          4803.325313  \n",
       "9  [0.2433202582659158, -0.4172195315114159, -0.2...          2664.366669  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.098305 ± 0.002023</td>\n",
       "      <td>0.101456 ± 0.001502</td>\n",
       "      <td>0.090603 ± 0.004489</td>\n",
       "      <td>18.045767 ± 2.385462</td>\n",
       "      <td>6.767062 ± 7.082763</td>\n",
       "      <td>0.057588 ± 0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.238014 ± 0.107888</td>\n",
       "      <td>0.071661 ± 0.023251</td>\n",
       "      <td>0.069507 ± 0.019962</td>\n",
       "      <td>61.867878 ± 28.400002</td>\n",
       "      <td>58.183900 ± 30.326011</td>\n",
       "      <td>0.030579 ± 0.020030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.098305 ± 0.002023     0.101456 ± 0.001502   \n",
       "3            5           0.238014 ± 0.107888     0.071661 ± 0.023251   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.090603 ± 0.004489      18.045767 ± 2.385462   \n",
       "3             0.069507 ± 0.019962     61.867878 ± 28.400002   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1               6.767062 ± 7.082763    0.057588 ± 0.000314  \n",
       "3             58.183900 ± 30.326011    0.030579 ± 0.020030  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results table\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(CSV_PATH).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Run Cell 8 first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- NEW: drop duplicate runs; keep the newest copy (with baseline/delta)\n",
    "if \"filename\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "# Typical numeric casts (safe)\n",
    "for col in [\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\n",
    "    \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "    \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\"\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "\n",
    "clean_path = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "Path(OUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved training-only table → {clean_path}\")\n",
    "\n",
    "# A compact per-layer summary (mean±std); guards against all-NaN\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "metrics = [\n",
    "    (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "    (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "    (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "    (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "    (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "    (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "]\n",
    "\n",
    "grp = df.groupby(\"n_layers\", dropna=False)\n",
    "summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "for col, label in metrics:\n",
    "    if col in df.columns and np.isfinite(df[col]).any():\n",
    "        summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "summary_path = f\"{OUT_BASE}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "summary.to_csv(summary_path, index=True)\n",
    "print(f\"Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82c991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
