{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46802b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# We'll run 5 instances (same across all notebooks) and two depths: 1 and 3 layers.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # used in filenames as ..._ls_01.json, ..._ls_02.json, ...\n",
    "LAYER_OPTIONS  = [1, 3]            # train 1-layer first, then 3-layers\n",
    "EVAL_SIGMA     = 0.10              # fixed noise everywhere (train & eval)\n",
    "\n",
    "# where to save artifacts (JSON bundles, instance records, CSV summary)\n",
    "OUT_BASE = \"./runs_halfqae\"        # change if you like; subfolders will be created automatically\n",
    "CSV_PATH = f\"{OUT_BASE}/results_instances.csv\"  # will be appended-to if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467f1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== IMPORTY ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys, json, math, random, time, hashlib\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8937b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== USTAWIENIA OGÓLNE ====\n",
    "np.random.seed(42)\n",
    "\n",
    "# symulator (szybki)\n",
    "BACKEND = \"lightning.qubit\"\n",
    "\n",
    "# Half-QAE\n",
    "n_qubits = 6\n",
    "n_latent = 2\n",
    "n_trash  = n_qubits - n_latent  # 4\n",
    "\n",
    "# dane Mackey-Glass \n",
    "beta=0.25\n",
    "gamma=0.1\n",
    "n=10\n",
    "tau=15\n",
    "dt=1.0\n",
    "T=300\n",
    "\n",
    "# skalowanie\n",
    "margin = 0.2\n",
    "scale_low, scale_high = 0.0+margin, 1.0-margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc17b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== POMOCNICZE ====\n",
    "def scale_values(x, new_min=0.0, new_max=1.0):\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    return new_min + (x - x_min) * (new_max - new_min) / max(1e-12, (x_max - x_min))\n",
    "\n",
    "def mackey_glass(beta=0.2, gamma=0.1, n=10, tau=17, dt=0.1, T=1000):\n",
    "    N = int(T/dt)\n",
    "    delay_steps = int(tau/dt)\n",
    "    x = np.zeros(N+delay_steps)\n",
    "    x[0:delay_steps] = 1.2\n",
    "    for t in range(delay_steps, N+delay_steps-1):\n",
    "        x_tau = x[t-delay_steps]\n",
    "        dxdt = beta * x_tau / (1 + x_tau**n) - gamma * x[t]\n",
    "        x[t+1] = x[t] + dxdt * dt\n",
    "    return x[delay_steps:]\n",
    "\n",
    "def ts_add_noise(X, noise=0.1, low=0.0, high=1.0):\n",
    "    Z = X + np.random.normal(0.0, noise, size=X.shape)\n",
    "    return np.clip(Z, low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8405a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== GENEROWANIE DANYCH ====\n",
    "y_raw = mackey_glass(beta=beta, gamma=gamma, n=n, tau=tau, dt=dt, T=T)\n",
    "y_raw = scale_values(y_raw, new_min=scale_low, new_max=scale_high)\n",
    "y = y_raw[2::3]                               # subsampling\n",
    "X_idx = np.arange(len(y))\n",
    "\n",
    "# okna (sliding window)\n",
    "window_size = n_qubits\n",
    "stride = 1\n",
    "X_windows = np.stack([y[i:i+window_size] for i in range(0, len(y)-window_size+1, stride)])\n",
    "\n",
    "# podziały: train/val/test = 0.6 / 0.2 / 0.2\n",
    "X_temp, X_test = train_test_split(X_windows, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_temp,   test_size=0.25, random_state=42)\n",
    "\n",
    "# czyste zbiory (na Stage 1 i Stage 2 generujemy szum dynamicznie)\n",
    "X_train_clean = X_train.copy()\n",
    "X_val_clean   = X_val.copy()\n",
    "X_test_clean  = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27f848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ANSATZE  ====\n",
    "def encoder_ansatz(params, x, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    # angle encoding\n",
    "    for w, val in zip(W, x[:len(W)]):\n",
    "        qml.RY(val*np.pi, wires=w)\n",
    "    # warstwy rotacji\n",
    "    n_block = len(W)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in range(n_layers):\n",
    "        for j, w in enumerate(W):\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RX(params[idx],     wires=w)\n",
    "            qml.RY(params[idx + 1], wires=w)\n",
    "            qml.RZ(params[idx + 2], wires=w)\n",
    "        for a, b in zip(W, W[1:]):\n",
    "            qml.CNOT(wires=[a, b])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n",
    "\n",
    "def decoder_ansatz(params, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    n_block = len(W)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in range(n_layers):\n",
    "        for a, b in zip(W, W[1:]):\n",
    "            qml.CNOT(wires=[a, b])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n",
    "        for j, w in enumerate(W):\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RZ(params[idx + 2], wires=w)\n",
    "            qml.RY(params[idx + 1], wires=w)\n",
    "            qml.RX(params[idx],     wires=w)\n",
    "\n",
    "def adjoint_decoder_ansatz(params, x, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    n_block = len(W)\n",
    "    for w, val in zip(W, x[:n_block]):\n",
    "        qml.RY(val*np.pi, wires=w)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in reversed(range(n_layers)):\n",
    "        for j in reversed(range(n_block)):\n",
    "            w = W[j]\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RX(-params[idx],     wires=w)\n",
    "            qml.RY(-params[idx + 1], wires=w)\n",
    "            qml.RZ(-params[idx + 2], wires=w)\n",
    "        for j in reversed(range(n_block - 1)):\n",
    "            qml.CNOT(wires=[W[j+1], W[j]])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee33b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STAGE 1: swap-test na adjoint_decoder (uczenie „trash->czysto”) ====\n",
    "# inicjalizacje\n",
    "#n_layers = 2\n",
    "#param_shape = n_layers * n_qubits * 3\n",
    "\n",
    "trash2_start = n_qubits               # 4\n",
    "ancilla      = n_qubits + n_trash     # 6\n",
    "num_total_s1 = n_qubits + n_trash + 1 # 7\n",
    "\n",
    "dev_s1 = qml.device(BACKEND, wires=num_total_s1, shots=None)\n",
    "\n",
    "@qml.qnode(dev_s1, interface=\"autograd\")\n",
    "def swap_test_on_adj_decoder(x, params):\n",
    "    main   = list(range(n_qubits))                                # [0,1,2,3]\n",
    "    trash1 = list(range(n_latent, n_latent + n_trash))            # [2,3]\n",
    "    trash2 = list(range(trash2_start, trash2_start + n_trash))    # [4,5]\n",
    "\n",
    "    adjoint_decoder_ansatz(params, x)\n",
    "\n",
    "    for i in trash2:\n",
    "        qml.Hadamard(wires=i)\n",
    "    qml.Hadamard(wires=ancilla)\n",
    "    for i in range(n_trash):\n",
    "        qml.CSWAP(wires=[ancilla, trash1[i], trash2[i]])\n",
    "    qml.Hadamard(wires=ancilla)\n",
    "\n",
    "    return qml.probs(wires=ancilla)\n",
    "\n",
    "def s1_cost(params, X_batch):\n",
    "    acc = pnp.array(0.0)\n",
    "    for x in X_batch:\n",
    "        p0 = swap_test_on_adj_decoder(x, params)[0]\n",
    "        acc = acc + (1.0 - p0)\n",
    "    return acc / len(X_batch)\n",
    "\n",
    "def train_adjoint_decoder(params_init, X_train, n_epochs=80, batch_size=8, lr=0.01, X_val_clean=None, seed=0):  # <-- ZMIANA\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    params = pnp.array(params_init, requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr)\n",
    "    hist = []\n",
    "    hist_val = []\n",
    "    lr_hist = []\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    t0 = time.time()\n",
    "    for ep in range(n_epochs):\n",
    "        idx = rng.permutation(len(X_train))  # <-- ZMIANA\n",
    "        s=0.0; nb=0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            Xb = X_train[idx[i:i+batch_size]]\n",
    "            params, c = opt.step_and_cost(lambda p: s1_cost(p, Xb), params)\n",
    "            s += float(c); nb += 1\n",
    "        train_cost = s/nb\n",
    "        hist.append(train_cost)\n",
    "        lr_hist.append(opt.stepsize)\n",
    "        # --- walidacja ---\n",
    "        if X_val_clean is not None:\n",
    "            val_cost = float(s1_cost(params, X_val_clean))\n",
    "            hist_val.append(val_cost)\n",
    "        else:\n",
    "            val_cost = train_cost\n",
    "            hist_val.append(train_cost)\n",
    "        print(f\"[Stage1] L={len(params_init)//(n_qubits*3)} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {opt.stepsize:.5f}\")\n",
    "        if train_cost < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = train_cost, pnp.array(params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= 10:\n",
    "            break\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    return dict(\n",
    "        phi=best_params if best_params is not None else params,\n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=len(hist),\n",
    "        hist_train=list(map(float, hist)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d34c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STAGE 2: trenujemy ENCODER latent↔latent (noisy vs clean_ref) ====\n",
    "dev_s2 = qml.device(BACKEND, wires=2*n_qubits + 1, shots=None)\n",
    "\n",
    "@qml.qnode(dev_s2, interface=\"autograd\")\n",
    "def swap_test_encoder_latent(x_noisy, x_clean, enc_params, dec_dagger_params):\n",
    "    encoder = list(range(n_qubits))                  # blok encodera\n",
    "    ref     = list(range(n_qubits, 2*n_qubits))      # blok referencyjny\n",
    "    anc     = 2*n_qubits\n",
    "\n",
    "    encoder_ansatz(enc_params, x_noisy, wires=encoder)\n",
    "    adjoint_decoder_ansatz(dec_dagger_params, x_clean, wires=ref)\n",
    "\n",
    "    qml.Hadamard(wires=anc)\n",
    "    for i in range(n_latent):\n",
    "        qml.CSWAP(wires=[anc, encoder[i], ref[i]])   # SWAP tylko latentów\n",
    "    qml.Hadamard(wires=anc)\n",
    "    return qml.probs(wires=anc)\n",
    "\n",
    "def s2_cost(enc_params, X_noisy_b, X_clean_b, dec_dagger_params):\n",
    "    acc = pnp.array(0.0)\n",
    "    for xn, xc in zip(X_noisy_b, X_clean_b):\n",
    "        p0 = swap_test_encoder_latent(xn, xc, enc_params, dec_dagger_params)[0]\n",
    "        acc = acc + (1.0 - p0)\n",
    "    return acc / len(X_noisy_b)\n",
    "\n",
    "# skala szumu czerpana z realnego skalowania danych\n",
    "info = {\"scale_low\": scale_low, \"scale_high\": scale_high}\n",
    "\n",
    "def s2_cost_dataset(enc_params, X_clean_set, dec_params, noise_level):\n",
    "    \"\"\"Metryka testowa S2: generujemy szum o danym poziomie i liczymy koszt.\"\"\"\n",
    "    low, high = info[\"scale_low\"], info[\"scale_high\"]\n",
    "    sigma = noise_level * (high - low)\n",
    "    X_noisy_set = ts_add_noise(X_clean_set, noise=sigma, low=low, high=high)\n",
    "    return float(s2_cost(enc_params, X_noisy_set, X_clean_set, dec_params))\n",
    "\n",
    "def train_encoder_with_sidekick_dyn_noise(enc_params_init, X_clean,\n",
    "                                         dec_dagger_params, noise_level,\n",
    "                                         n_epochs=80, batch_size=8, lr=0.01, seed=0,\n",
    "                                         X_val_clean=None):\n",
    "    params = pnp.array(enc_params_init, requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr)\n",
    "    hist = []\n",
    "    hist_val = []\n",
    "    lr_hist = []\n",
    "    hist_noisy = []\n",
    "    hist_delta = []\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    t0 = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    low, high = info[\"scale_low\"], info[\"scale_high\"]\n",
    "    sigma = noise_level * (high - low)\n",
    "    for ep in range(n_epochs):\n",
    "        idx = rng.permutation(len(X_clean))\n",
    "        Xc = X_clean[idx]\n",
    "        s = 0.0; nb = 0\n",
    "        for i in range(0, len(Xc), batch_size):\n",
    "            Xcb = Xc[i:i+batch_size]\n",
    "            Xnb = ts_add_noise(Xcb, noise=sigma, low=low, high=high)\n",
    "            params, c = opt.step_and_cost(\n",
    "                lambda p: s2_cost(p, Xnb, Xcb, dec_dagger_params), params\n",
    "            )\n",
    "            s += float(c); nb += 1\n",
    "        train_cost = s/nb\n",
    "        hist.append(train_cost)\n",
    "        lr_hist.append(opt.stepsize)\n",
    "        # --- walidacja ---\n",
    "        if X_val_clean is not None:\n",
    "            val_cost = float(s2_cost_dataset(params, X_val_clean, dec_dagger_params, noise_level))\n",
    "            # --- baseline/noise-only na walidacji ---\n",
    "            X_noisy_val = ts_add_noise(X_val_clean, noise=sigma, low=low, high=high)\n",
    "            mse_noisy = float(s2_cost(params*0, X_noisy_val, X_val_clean, dec_dagger_params))  # params*0 = brak uczenia\n",
    "            hist_noisy.append(mse_noisy)\n",
    "            if mse_noisy > 1e-12:\n",
    "                delta = 100.0 * (1.0 - val_cost / mse_noisy)\n",
    "            else:\n",
    "                delta = 0.0\n",
    "            hist_delta.append(delta)\n",
    "            hist_val.append(val_cost)\n",
    "        else:\n",
    "            val_cost = train_cost\n",
    "            hist_val.append(train_cost)\n",
    "            hist_noisy.append(np.nan)\n",
    "            hist_delta.append(np.nan)\n",
    "        print(f\"[Stage3] L={len(enc_params_init)//(n_qubits*3)} ep {ep:03d} | train {train_cost:.5f} | \"\n",
    "              f\"val {val_cost:.5f} | noisy {mse_noisy:.5f} | Δ {delta:+.1f}% | LR {opt.stepsize:.5f}\")\n",
    "        if train_cost < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = train_cost, pnp.array(params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= 10:\n",
    "            break\n",
    "    \n",
    "    train_seconds = float(time.time() - t0)\n",
    "    return dict(\n",
    "        psi=best_params if best_params is not None else params,\n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=len(hist),\n",
    "        hist_train=list(map(float, hist)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7562aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(X_train, X_val, n_layers=2, instance_id=1, n_epochs=120, batch_size=32, lr_init=0.01, patience=10, lr_patience=8, min_delta=1e-6, seed=0):  # <-- ZMIANA\n",
    "    param_shape = n_layers * n_qubits * 3\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    params_init = rng.uniform(-0.01, 0.01, param_shape)  # <-- ZMIANA\n",
    "    result = train_adjoint_decoder(\n",
    "        params_init, X_train,\n",
    "        n_epochs=n_epochs, batch_size=batch_size, lr=lr_init,\n",
    "        X_val_clean=X_val,\n",
    "        seed=seed  # <-- ZMIANA\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def train_stage3(X_train, X_val, dec_dagger_params, n_layers=2, instance_id=1, n_epochs=120, batch_size=32, lr_init=0.01, noise_level=0.10, seed=0):\n",
    "    param_shape = n_layers * n_qubits * 3\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    enc_params_init = rng.uniform(-0.01, 0.01, param_shape)  # <-- ZMIANA\n",
    "    result = train_encoder_with_sidekick_dyn_noise(\n",
    "        enc_params_init, X_train,\n",
    "        dec_dagger_params, noise_level,\n",
    "        n_epochs=n_epochs, batch_size=batch_size, lr=lr_init, seed=seed,\n",
    "        X_val_clean=X_val\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f45bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.127050 | val 0.109520 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.121815 | val 0.106276 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.118588 | val 0.103180 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.118516 | val 0.100359 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.115090 | val 0.097915 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.117418 | val 0.095698 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.116246 | val 0.093824 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.114898 | val 0.092275 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.112591 | val 0.091053 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.111259 | val 0.090005 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.110050 | val 0.089216 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.111071 | val 0.088651 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.109853 | val 0.088246 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.110442 | val 0.087959 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.111140 | val 0.087712 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.111700 | val 0.087566 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.110006 | val 0.087511 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.111508 | val 0.087505 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.111824 | val 0.087508 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.108916 | val 0.087543 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.110438 | val 0.087558 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.112197 | val 0.087616 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.111364 | val 0.087661 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.110895 | val 0.087713 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.109907 | val 0.087738 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.112190 | val 0.087774 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.110289 | val 0.087851 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.110599 | val 0.087900 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.111062 | val 0.087968 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.108812 | val 0.088036 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.110451 | val 0.088082 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.110746 | val 0.088153 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.111268 | val 0.088174 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.111227 | val 0.088161 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.111357 | val 0.088197 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.110519 | val 0.088237 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.110906 | val 0.088238 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.110327 | val 0.088248 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.111285 | val 0.088252 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.109515 | val 0.088224 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.11525 | val 0.09356 | noisy 0.08994 | Δ -4.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.10624 | val 0.09075 | noisy 0.10172 | Δ +10.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.09787 | val 0.09455 | noisy 0.09747 | Δ +3.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10365 | val 0.07498 | noisy 0.10037 | Δ +25.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.09483 | val 0.09019 | noisy 0.10316 | Δ +12.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10448 | val 0.08159 | noisy 0.10427 | Δ +21.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10297 | val 0.07954 | noisy 0.09898 | Δ +19.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.09868 | val 0.08501 | noisy 0.08993 | Δ +5.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.11003 | val 0.07182 | noisy 0.09842 | Δ +27.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10077 | val 0.09284 | noisy 0.09077 | Δ -2.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.09692 | val 0.08725 | noisy 0.10696 | Δ +18.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.09444 | val 0.07627 | noisy 0.09002 | Δ +15.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10313 | val 0.09087 | noisy 0.08866 | Δ -2.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.09631 | val 0.08172 | noisy 0.09678 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.09784 | val 0.09348 | noisy 0.08893 | Δ -5.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.09916 | val 0.08707 | noisy 0.10383 | Δ +16.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.10093 | val 0.08655 | noisy 0.09465 | Δ +8.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.10198 | val 0.08986 | noisy 0.09900 | Δ +9.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.10562 | val 0.08842 | noisy 0.09980 | Δ +11.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.09435 | val 0.08396 | noisy 0.09956 | Δ +15.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.10343 | val 0.09130 | noisy 0.08993 | Δ -1.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.09783 | val 0.08120 | noisy 0.09368 | Δ +13.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.09908 | val 0.08613 | noisy 0.09204 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.09981 | val 0.08058 | noisy 0.09578 | Δ +15.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 024 | train 0.10279 | val 0.08425 | noisy 0.09769 | Δ +13.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 025 | train 0.09450 | val 0.07714 | noisy 0.10235 | Δ +24.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 026 | train 0.09605 | val 0.09141 | noisy 0.08994 | Δ -1.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 027 | train 0.10315 | val 0.08554 | noisy 0.09414 | Δ +9.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 028 | train 0.09637 | val 0.08587 | noisy 0.09613 | Δ +10.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 029 | train 0.09058 | val 0.08200 | noisy 0.09532 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 030 | train 0.09938 | val 0.09486 | noisy 0.10401 | Δ +8.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 031 | train 0.10101 | val 0.09477 | noisy 0.09677 | Δ +2.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 032 | train 0.10133 | val 0.09237 | noisy 0.10062 | Δ +8.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 033 | train 0.10566 | val 0.08272 | noisy 0.10145 | Δ +18.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 034 | train 0.10280 | val 0.08182 | noisy 0.09347 | Δ +12.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 035 | train 0.10057 | val 0.08280 | noisy 0.09630 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 036 | train 0.10349 | val 0.09430 | noisy 0.10480 | Δ +10.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 037 | train 0.09439 | val 0.08785 | noisy 0.09524 | Δ +7.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 038 | train 0.10150 | val 0.08115 | noisy 0.09285 | Δ +12.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 039 | train 0.09896 | val 0.08458 | noisy 0.09813 | Δ +13.8% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.126426 | val 0.109232 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.121685 | val 0.105648 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.118435 | val 0.102494 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.119079 | val 0.099645 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.117622 | val 0.097178 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.114532 | val 0.095093 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.114964 | val 0.093294 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.114057 | val 0.091799 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.112017 | val 0.090655 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.112123 | val 0.089708 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.112894 | val 0.089030 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.112788 | val 0.088506 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.111729 | val 0.088121 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.109289 | val 0.087860 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.111086 | val 0.087674 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.113154 | val 0.087577 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.111276 | val 0.087528 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.109619 | val 0.087516 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.111174 | val 0.087526 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.114020 | val 0.087553 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.109322 | val 0.087621 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.112718 | val 0.087687 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.108665 | val 0.087802 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.109820 | val 0.087930 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.112704 | val 0.087983 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.109631 | val 0.088077 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.112276 | val 0.088156 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.110862 | val 0.088210 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.109623 | val 0.088248 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.112795 | val 0.088237 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.111451 | val 0.088290 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.110000 | val 0.088311 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.109460 | val 0.088330 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.10643 | val 0.09417 | noisy 0.09893 | Δ +4.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.10433 | val 0.08616 | noisy 0.09837 | Δ +12.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10023 | val 0.09270 | noisy 0.09555 | Δ +3.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10700 | val 0.08220 | noisy 0.09812 | Δ +16.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.10368 | val 0.07750 | noisy 0.08737 | Δ +11.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10149 | val 0.08025 | noisy 0.09539 | Δ +15.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10672 | val 0.08945 | noisy 0.09465 | Δ +5.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10055 | val 0.08598 | noisy 0.09773 | Δ +12.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10047 | val 0.08529 | noisy 0.08987 | Δ +5.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.09890 | val 0.08949 | noisy 0.10690 | Δ +16.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10070 | val 0.08326 | noisy 0.08936 | Δ +6.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10101 | val 0.07348 | noisy 0.08949 | Δ +17.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10095 | val 0.08488 | noisy 0.10040 | Δ +15.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10262 | val 0.08323 | noisy 0.09836 | Δ +15.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.10038 | val 0.09715 | noisy 0.09499 | Δ -2.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.09737 | val 0.08514 | noisy 0.09586 | Δ +11.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.09210 | val 0.09027 | noisy 0.07858 | Δ -14.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.09561 | val 0.08928 | noisy 0.09458 | Δ +5.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.09905 | val 0.07839 | noisy 0.09180 | Δ +14.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.10500 | val 0.08356 | noisy 0.09313 | Δ +10.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.09830 | val 0.08303 | noisy 0.10277 | Δ +19.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.10059 | val 0.08453 | noisy 0.09175 | Δ +7.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.09501 | val 0.08379 | noisy 0.10463 | Δ +19.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.09693 | val 0.08226 | noisy 0.10354 | Δ +20.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 024 | train 0.10998 | val 0.08284 | noisy 0.10758 | Δ +23.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 025 | train 0.10079 | val 0.08626 | noisy 0.09529 | Δ +9.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 026 | train 0.09577 | val 0.08311 | noisy 0.10228 | Δ +18.7% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.125705 | val 0.110224 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.120673 | val 0.106650 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.122664 | val 0.103300 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.119594 | val 0.100341 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.115357 | val 0.097697 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.114492 | val 0.095499 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.113535 | val 0.093581 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.111693 | val 0.091977 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.113135 | val 0.090749 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.111388 | val 0.089812 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.111650 | val 0.089128 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.108399 | val 0.088621 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.112127 | val 0.088202 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.111699 | val 0.087913 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.110051 | val 0.087717 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.111667 | val 0.087576 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.112192 | val 0.087479 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.112800 | val 0.087439 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.109975 | val 0.087428 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.111142 | val 0.087433 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.109563 | val 0.087466 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.111499 | val 0.087460 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.11328 | val 0.08366 | noisy 0.09537 | Δ +12.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11006 | val 0.09702 | noisy 0.09270 | Δ -4.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10637 | val 0.08984 | noisy 0.08882 | Δ -1.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.11069 | val 0.08258 | noisy 0.09199 | Δ +10.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.09903 | val 0.08690 | noisy 0.09175 | Δ +5.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10293 | val 0.09786 | noisy 0.09309 | Δ -5.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10420 | val 0.09085 | noisy 0.09376 | Δ +3.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.09931 | val 0.08321 | noisy 0.09783 | Δ +14.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10792 | val 0.08365 | noisy 0.09171 | Δ +8.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10297 | val 0.08195 | noisy 0.09543 | Δ +14.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10065 | val 0.08223 | noisy 0.09324 | Δ +11.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10028 | val 0.07543 | noisy 0.10579 | Δ +28.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.09827 | val 0.08700 | noisy 0.09489 | Δ +8.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10013 | val 0.08417 | noisy 0.10074 | Δ +16.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.09714 | val 0.08010 | noisy 0.09868 | Δ +18.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.10595 | val 0.08495 | noisy 0.10107 | Δ +16.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.09522 | val 0.07440 | noisy 0.09924 | Δ +25.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.10210 | val 0.08805 | noisy 0.09626 | Δ +8.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.09607 | val 0.08561 | noisy 0.09378 | Δ +8.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.09656 | val 0.08269 | noisy 0.10206 | Δ +19.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.09285 | val 0.08503 | noisy 0.09539 | Δ +10.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.10374 | val 0.10062 | noisy 0.09879 | Δ -1.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.10243 | val 0.08587 | noisy 0.10022 | Δ +14.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.10457 | val 0.08482 | noisy 0.10070 | Δ +15.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 024 | train 0.10441 | val 0.08870 | noisy 0.09323 | Δ +4.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 025 | train 0.10100 | val 0.08413 | noisy 0.10061 | Δ +16.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 026 | train 0.09181 | val 0.08779 | noisy 0.08966 | Δ +2.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 027 | train 0.10138 | val 0.09206 | noisy 0.10843 | Δ +15.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 028 | train 0.09703 | val 0.08284 | noisy 0.09209 | Δ +10.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 029 | train 0.10275 | val 0.07851 | noisy 0.09769 | Δ +19.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 030 | train 0.10652 | val 0.08798 | noisy 0.08881 | Δ +0.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 031 | train 0.10316 | val 0.08142 | noisy 0.09995 | Δ +18.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 032 | train 0.09854 | val 0.08691 | noisy 0.09604 | Δ +9.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 033 | train 0.10241 | val 0.07899 | noisy 0.09591 | Δ +17.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 034 | train 0.09622 | val 0.08925 | noisy 0.10566 | Δ +15.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 035 | train 0.09815 | val 0.08230 | noisy 0.10092 | Δ +18.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 036 | train 0.09866 | val 0.07817 | noisy 0.09209 | Δ +15.1% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.122525 | val 0.108883 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.123230 | val 0.105413 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.119718 | val 0.102194 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.116276 | val 0.099345 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.116745 | val 0.097077 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.113969 | val 0.095083 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.113747 | val 0.093326 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.111820 | val 0.091888 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.112896 | val 0.090725 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.111415 | val 0.089780 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.111049 | val 0.089076 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.110132 | val 0.088529 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.111226 | val 0.088140 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.110954 | val 0.087880 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.110621 | val 0.087691 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.111893 | val 0.087559 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.112398 | val 0.087467 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.110867 | val 0.087457 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.112090 | val 0.087438 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.111489 | val 0.087473 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.110975 | val 0.087512 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.111822 | val 0.087599 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.10131 | val 0.08106 | noisy 0.09906 | Δ +18.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.10196 | val 0.09045 | noisy 0.09668 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10113 | val 0.08250 | noisy 0.10241 | Δ +19.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.09931 | val 0.08898 | noisy 0.09066 | Δ +1.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.10553 | val 0.08287 | noisy 0.09326 | Δ +11.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.09794 | val 0.09079 | noisy 0.08356 | Δ -8.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.09818 | val 0.09105 | noisy 0.09073 | Δ -0.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.09708 | val 0.08452 | noisy 0.09942 | Δ +15.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.09859 | val 0.08887 | noisy 0.10173 | Δ +12.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10296 | val 0.08271 | noisy 0.09517 | Δ +13.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10320 | val 0.08822 | noisy 0.08698 | Δ -1.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.09251 | val 0.08345 | noisy 0.08706 | Δ +4.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10409 | val 0.08565 | noisy 0.08345 | Δ -2.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10296 | val 0.07900 | noisy 0.10295 | Δ +23.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.10191 | val 0.08682 | noisy 0.10222 | Δ +15.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.10120 | val 0.09209 | noisy 0.09579 | Δ +3.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.10394 | val 0.08498 | noisy 0.09952 | Δ +14.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.10076 | val 0.07761 | noisy 0.09491 | Δ +18.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.09880 | val 0.08922 | noisy 0.09015 | Δ +1.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.09808 | val 0.08317 | noisy 0.09645 | Δ +13.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.09023 | val 0.08655 | noisy 0.09513 | Δ +9.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.10566 | val 0.08261 | noisy 0.10358 | Δ +20.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.09547 | val 0.08077 | noisy 0.09704 | Δ +16.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.09692 | val 0.07877 | noisy 0.09109 | Δ +13.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 024 | train 0.09946 | val 0.08046 | noisy 0.09639 | Δ +16.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 025 | train 0.09521 | val 0.07823 | noisy 0.08767 | Δ +10.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 026 | train 0.09487 | val 0.08673 | noisy 0.09003 | Δ +3.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 027 | train 0.09754 | val 0.07876 | noisy 0.09093 | Δ +13.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 028 | train 0.10318 | val 0.08072 | noisy 0.08906 | Δ +9.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 029 | train 0.10736 | val 0.08084 | noisy 0.09583 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 030 | train 0.10064 | val 0.08253 | noisy 0.09333 | Δ +11.6% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.125362 | val 0.109531 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.122622 | val 0.105997 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.119743 | val 0.102863 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.117413 | val 0.100150 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.114069 | val 0.097767 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.116015 | val 0.095660 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.112395 | val 0.093987 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.113737 | val 0.092477 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.111565 | val 0.091269 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.111189 | val 0.090289 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.111910 | val 0.089489 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.112495 | val 0.088863 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.111809 | val 0.088461 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.110775 | val 0.088171 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.112889 | val 0.087928 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.108170 | val 0.087795 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.109009 | val 0.087673 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.112171 | val 0.087604 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.109861 | val 0.087574 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.112658 | val 0.087555 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.111142 | val 0.087569 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.110965 | val 0.087601 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.111327 | val 0.087629 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.113087 | val 0.087656 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.110441 | val 0.087748 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.110086 | val 0.087841 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.10594 | val 0.08953 | noisy 0.09802 | Δ +8.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.10361 | val 0.09301 | noisy 0.10111 | Δ +8.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.09957 | val 0.10019 | noisy 0.10051 | Δ +0.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10458 | val 0.08721 | noisy 0.09812 | Δ +11.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.10324 | val 0.08355 | noisy 0.10715 | Δ +22.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10348 | val 0.08278 | noisy 0.10465 | Δ +20.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.09906 | val 0.08450 | noisy 0.10303 | Δ +18.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10469 | val 0.08501 | noisy 0.09979 | Δ +14.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10119 | val 0.08567 | noisy 0.09034 | Δ +5.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10408 | val 0.08833 | noisy 0.09745 | Δ +9.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10365 | val 0.08501 | noisy 0.09986 | Δ +14.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.09792 | val 0.09642 | noisy 0.09753 | Δ +1.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.09687 | val 0.08447 | noisy 0.09007 | Δ +6.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.09646 | val 0.08864 | noisy 0.09933 | Δ +10.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.10093 | val 0.08191 | noisy 0.08935 | Δ +8.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.09621 | val 0.08344 | noisy 0.09154 | Δ +8.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.09921 | val 0.08486 | noisy 0.09576 | Δ +11.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.10293 | val 0.08214 | noisy 0.08885 | Δ +7.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.09587 | val 0.08484 | noisy 0.09396 | Δ +9.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.10292 | val 0.08552 | noisy 0.09430 | Δ +9.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.10172 | val 0.08834 | noisy 0.09526 | Δ +7.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.09788 | val 0.08029 | noisy 0.09120 | Δ +12.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.10504 | val 0.08419 | noisy 0.09502 | Δ +11.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.10263 | val 0.08077 | noisy 0.10779 | Δ +25.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 024 | train 0.09441 | val 0.08397 | noisy 0.10270 | Δ +18.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 025 | train 0.09752 | val 0.09453 | noisy 0.09258 | Δ -2.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 026 | train 0.10180 | val 0.07862 | noisy 0.09708 | Δ +19.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 027 | train 0.09855 | val 0.08024 | noisy 0.09435 | Δ +15.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 028 | train 0.09954 | val 0.08259 | noisy 0.10240 | Δ +19.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 029 | train 0.09782 | val 0.08503 | noisy 0.08979 | Δ +5.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 030 | train 0.10277 | val 0.08194 | noisy 0.11158 | Δ +26.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 031 | train 0.09781 | val 0.09361 | noisy 0.09971 | Δ +6.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 032 | train 0.09374 | val 0.08734 | noisy 0.09685 | Δ +9.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 033 | train 0.10023 | val 0.09203 | noisy 0.10189 | Δ +9.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 034 | train 0.09426 | val 0.08530 | noisy 0.08699 | Δ +1.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 035 | train 0.09587 | val 0.09302 | noisy 0.09843 | Δ +5.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 036 | train 0.10087 | val 0.08676 | noisy 0.08808 | Δ +1.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 037 | train 0.10286 | val 0.08740 | noisy 0.09189 | Δ +4.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 038 | train 0.09250 | val 0.08345 | noisy 0.10507 | Δ +20.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 039 | train 0.09856 | val 0.09187 | noisy 0.09409 | Δ +2.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 040 | train 0.10100 | val 0.08604 | noisy 0.09641 | Δ +10.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 041 | train 0.08998 | val 0.08674 | noisy 0.10383 | Δ +16.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 042 | train 0.10230 | val 0.08508 | noisy 0.09662 | Δ +11.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 043 | train 0.09304 | val 0.08408 | noisy 0.09886 | Δ +15.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 044 | train 0.09607 | val 0.09102 | noisy 0.08641 | Δ -5.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 045 | train 0.10175 | val 0.08967 | noisy 0.09924 | Δ +9.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 046 | train 0.09995 | val 0.08592 | noisy 0.09230 | Δ +6.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 047 | train 0.09102 | val 0.08811 | noisy 0.08762 | Δ -0.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 048 | train 0.09429 | val 0.08636 | noisy 0.09639 | Δ +10.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 049 | train 0.10484 | val 0.08588 | noisy 0.09893 | Δ +13.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 050 | train 0.09997 | val 0.08609 | noisy 0.09903 | Δ +13.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 051 | train 0.09557 | val 0.08571 | noisy 0.09261 | Δ +7.5% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 1 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.127349 | val 0.107361 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.119690 | val 0.102538 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.115993 | val 0.098602 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.115431 | val 0.095762 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.112376 | val 0.093847 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.115269 | val 0.092372 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.114776 | val 0.091229 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.114142 | val 0.090242 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.112161 | val 0.089444 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.111037 | val 0.088741 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.109817 | val 0.088280 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.110786 | val 0.088043 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.109590 | val 0.087994 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.110285 | val 0.088045 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.110975 | val 0.088034 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.111520 | val 0.087999 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.109921 | val 0.087985 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.111395 | val 0.087959 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.111569 | val 0.087873 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.108798 | val 0.087873 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.110248 | val 0.087811 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.111885 | val 0.087788 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.111074 | val 0.087778 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.110647 | val 0.087786 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.109676 | val 0.087787 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.111973 | val 0.087790 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.110010 | val 0.087879 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.110287 | val 0.087971 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.110850 | val 0.088088 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.108577 | val 0.088191 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.110206 | val 0.088243 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.110526 | val 0.088313 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.111028 | val 0.088289 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.110982 | val 0.088221 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.111093 | val 0.088254 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.110284 | val 0.088259 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.110675 | val 0.088253 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.110055 | val 0.088238 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.110990 | val 0.088216 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.109291 | val 0.088160 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.14506 | val 0.12170 | noisy 0.12369 | Δ +1.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.13443 | val 0.11550 | noisy 0.13574 | Δ +14.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.13287 | val 0.12250 | noisy 0.13143 | Δ +6.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.13291 | val 0.11845 | noisy 0.13141 | Δ +9.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.13154 | val 0.11242 | noisy 0.13362 | Δ +15.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.12947 | val 0.12286 | noisy 0.12732 | Δ +3.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.13174 | val 0.11143 | noisy 0.13904 | Δ +19.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.12855 | val 0.10502 | noisy 0.13825 | Δ +24.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.13009 | val 0.10747 | noisy 0.11509 | Δ +6.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.12634 | val 0.11285 | noisy 0.14309 | Δ +21.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.12831 | val 0.10699 | noisy 0.13752 | Δ +22.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.12697 | val 0.11015 | noisy 0.14054 | Δ +21.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.12544 | val 0.11971 | noisy 0.13027 | Δ +8.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.12943 | val 0.10690 | noisy 0.13729 | Δ +22.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.12894 | val 0.11925 | noisy 0.13009 | Δ +8.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.12706 | val 0.11285 | noisy 0.12538 | Δ +10.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.12952 | val 0.10696 | noisy 0.13504 | Δ +20.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.13237 | val 0.10908 | noisy 0.12555 | Δ +13.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.13035 | val 0.11881 | noisy 0.12690 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.11986 | val 0.10903 | noisy 0.13028 | Δ +16.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.12678 | val 0.11686 | noisy 0.12940 | Δ +9.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.13008 | val 0.12162 | noisy 0.12470 | Δ +2.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.13079 | val 0.10216 | noisy 0.12738 | Δ +19.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.13238 | val 0.11417 | noisy 0.13608 | Δ +16.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.12883 | val 0.11467 | noisy 0.13718 | Δ +16.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.13273 | val 0.11228 | noisy 0.12981 | Δ +13.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.12308 | val 0.11508 | noisy 0.12449 | Δ +7.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 027 | train 0.13193 | val 0.11575 | noisy 0.13759 | Δ +15.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 028 | train 0.12170 | val 0.10941 | noisy 0.14099 | Δ +22.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 029 | train 0.12693 | val 0.11583 | noisy 0.13665 | Δ +15.2% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.125942 | val 0.106343 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.119391 | val 0.101287 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.115203 | val 0.097584 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.116052 | val 0.094973 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.115201 | val 0.093114 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.112827 | val 0.091832 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.113873 | val 0.090755 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.113416 | val 0.089851 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.111548 | val 0.089199 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.111919 | val 0.088689 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.112634 | val 0.088402 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.112710 | val 0.088214 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.111540 | val 0.088015 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.109192 | val 0.087933 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.111064 | val 0.087867 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.112888 | val 0.087791 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.111000 | val 0.087784 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.109386 | val 0.087824 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.110898 | val 0.087863 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.113678 | val 0.087891 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.109129 | val 0.087987 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.112375 | val 0.088029 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.108399 | val 0.088133 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.109678 | val 0.088258 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.112492 | val 0.088226 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.109369 | val 0.088296 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.112034 | val 0.088330 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.110603 | val 0.088303 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.109389 | val 0.088280 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.112541 | val 0.088203 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.111224 | val 0.088237 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.109769 | val 0.088248 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.109199 | val 0.088248 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.14174 | val 0.12595 | noisy 0.13366 | Δ +5.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.13182 | val 0.12371 | noisy 0.12224 | Δ -1.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.12771 | val 0.11234 | noisy 0.13973 | Δ +19.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.13165 | val 0.13059 | noisy 0.13693 | Δ +4.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.13286 | val 0.11264 | noisy 0.13247 | Δ +15.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.13378 | val 0.12401 | noisy 0.13673 | Δ +9.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.13518 | val 0.11454 | noisy 0.13980 | Δ +18.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.12929 | val 0.11528 | noisy 0.14186 | Δ +18.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.12577 | val 0.10931 | noisy 0.12344 | Δ +11.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.12667 | val 0.10356 | noisy 0.13912 | Δ +25.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.12972 | val 0.11144 | noisy 0.13516 | Δ +17.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.12379 | val 0.11588 | noisy 0.14704 | Δ +21.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.12269 | val 0.11025 | noisy 0.12686 | Δ +13.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.12415 | val 0.10800 | noisy 0.12956 | Δ +16.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.12515 | val 0.12017 | noisy 0.13184 | Δ +8.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.13105 | val 0.11624 | noisy 0.12523 | Δ +7.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.12157 | val 0.10968 | noisy 0.14280 | Δ +23.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.12576 | val 0.11341 | noisy 0.12808 | Δ +11.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.13262 | val 0.10729 | noisy 0.13608 | Δ +21.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.12485 | val 0.11519 | noisy 0.13641 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.12915 | val 0.10127 | noisy 0.13570 | Δ +25.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.12266 | val 0.11011 | noisy 0.13584 | Δ +18.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.12450 | val 0.11034 | noisy 0.12646 | Δ +12.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.12599 | val 0.11841 | noisy 0.13887 | Δ +14.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.13029 | val 0.10801 | noisy 0.14372 | Δ +24.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.12788 | val 0.11388 | noisy 0.13080 | Δ +12.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.12875 | val 0.12039 | noisy 0.12972 | Δ +7.2% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.124601 | val 0.106366 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.117774 | val 0.101249 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.118797 | val 0.097329 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.115296 | val 0.094703 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.112518 | val 0.092857 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.112111 | val 0.091592 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.112129 | val 0.090521 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.111252 | val 0.089584 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.112722 | val 0.088851 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.111060 | val 0.088338 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.111261 | val 0.088078 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.108073 | val 0.087984 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.111801 | val 0.087905 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.111484 | val 0.087886 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.110019 | val 0.087902 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.111467 | val 0.087862 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.112000 | val 0.087818 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.112451 | val 0.087773 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.109765 | val 0.087790 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.110919 | val 0.087801 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.109356 | val 0.087812 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.111143 | val 0.087736 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.14088 | val 0.13379 | noisy 0.13589 | Δ +1.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.13115 | val 0.12524 | noisy 0.13508 | Δ +7.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.13854 | val 0.12602 | noisy 0.13557 | Δ +7.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.12965 | val 0.12329 | noisy 0.14247 | Δ +13.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.13372 | val 0.11883 | noisy 0.14625 | Δ +18.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.13036 | val 0.10997 | noisy 0.13349 | Δ +17.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.13071 | val 0.10620 | noisy 0.13907 | Δ +23.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.12365 | val 0.10492 | noisy 0.13653 | Δ +23.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.13806 | val 0.11697 | noisy 0.12623 | Δ +7.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.13210 | val 0.11685 | noisy 0.14039 | Δ +16.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.12748 | val 0.11533 | noisy 0.12820 | Δ +10.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.12530 | val 0.11374 | noisy 0.12307 | Δ +7.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.13119 | val 0.11276 | noisy 0.13551 | Δ +16.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.12896 | val 0.11499 | noisy 0.13383 | Δ +14.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.12469 | val 0.12092 | noisy 0.12637 | Δ +4.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.12519 | val 0.10623 | noisy 0.13686 | Δ +22.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.12079 | val 0.11541 | noisy 0.14206 | Δ +18.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.12445 | val 0.10717 | noisy 0.14550 | Δ +26.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.12699 | val 0.10697 | noisy 0.13704 | Δ +21.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.12393 | val 0.10723 | noisy 0.13326 | Δ +19.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.12606 | val 0.11351 | noisy 0.13340 | Δ +14.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.13556 | val 0.11566 | noisy 0.12928 | Δ +10.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.12974 | val 0.11963 | noisy 0.12358 | Δ +3.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.13186 | val 0.11743 | noisy 0.13090 | Δ +10.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.12475 | val 0.11139 | noisy 0.12936 | Δ +13.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.12756 | val 0.11471 | noisy 0.12199 | Δ +6.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.12434 | val 0.12106 | noisy 0.13350 | Δ +9.3% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.122907 | val 0.106764 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.121615 | val 0.101648 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.117495 | val 0.097792 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.113891 | val 0.095169 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.113877 | val 0.093516 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.111984 | val 0.092206 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.112627 | val 0.091091 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.111205 | val 0.090175 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.112305 | val 0.089426 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.111192 | val 0.088816 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.110797 | val 0.088407 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.109917 | val 0.088123 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.111025 | val 0.087978 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.110918 | val 0.087937 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.110425 | val 0.087857 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.111607 | val 0.087781 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.112085 | val 0.087717 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.110657 | val 0.087770 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.111723 | val 0.087749 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.111245 | val 0.087830 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.110694 | val 0.087871 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.111603 | val 0.087943 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.13750 | val 0.12921 | noisy 0.13516 | Δ +4.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.13302 | val 0.11574 | noisy 0.13239 | Δ +12.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.13355 | val 0.11398 | noisy 0.12974 | Δ +12.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.13235 | val 0.11964 | noisy 0.13567 | Δ +11.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.13010 | val 0.11803 | noisy 0.14462 | Δ +18.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.12826 | val 0.11180 | noisy 0.12602 | Δ +11.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.12644 | val 0.11324 | noisy 0.13715 | Δ +17.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.11873 | val 0.11804 | noisy 0.14100 | Δ +16.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.12810 | val 0.10242 | noisy 0.12623 | Δ +18.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.12485 | val 0.11222 | noisy 0.13178 | Δ +14.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.12286 | val 0.10434 | noisy 0.13049 | Δ +20.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.12382 | val 0.11968 | noisy 0.13510 | Δ +11.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.12892 | val 0.11935 | noisy 0.13420 | Δ +11.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.12488 | val 0.11247 | noisy 0.12652 | Δ +11.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.13415 | val 0.11952 | noisy 0.13178 | Δ +9.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.13126 | val 0.11420 | noisy 0.12128 | Δ +5.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.13418 | val 0.11321 | noisy 0.11715 | Δ +3.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.12646 | val 0.11696 | noisy 0.13556 | Δ +13.7% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.125044 | val 0.106921 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.120488 | val 0.101748 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.117119 | val 0.097922 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.114829 | val 0.095457 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.111555 | val 0.093807 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.115009 | val 0.092489 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.111152 | val 0.091460 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.113345 | val 0.090461 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.111062 | val 0.089643 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.110765 | val 0.089025 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.111507 | val 0.088596 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.112137 | val 0.088350 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.111485 | val 0.088326 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.110555 | val 0.088372 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.112685 | val 0.088338 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.108171 | val 0.088339 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.108908 | val 0.088218 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.112021 | val 0.088077 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.109760 | val 0.087963 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.112378 | val 0.087811 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.110827 | val 0.087750 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.110713 | val 0.087757 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.111078 | val 0.087738 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.112870 | val 0.087718 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.110212 | val 0.087805 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.109838 | val 0.087893 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.14661 | val 0.13949 | noisy 0.13183 | Δ -5.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.14405 | val 0.13198 | noisy 0.13270 | Δ +0.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.13094 | val 0.12073 | noisy 0.13582 | Δ +11.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.12752 | val 0.10787 | noisy 0.13406 | Δ +19.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.13133 | val 0.11054 | noisy 0.13393 | Δ +17.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.13648 | val 0.12013 | noisy 0.14930 | Δ +19.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.13011 | val 0.11928 | noisy 0.13303 | Δ +10.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.12530 | val 0.11318 | noisy 0.11673 | Δ +3.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.13277 | val 0.12138 | noisy 0.12975 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.12884 | val 0.12089 | noisy 0.14330 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.12494 | val 0.11086 | noisy 0.13069 | Δ +15.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.13617 | val 0.11520 | noisy 0.13391 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.13465 | val 0.12092 | noisy 0.13021 | Δ +7.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.13043 | val 0.10564 | noisy 0.12761 | Δ +17.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.13339 | val 0.11743 | noisy 0.14363 | Δ +18.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.12680 | val 0.11639 | noisy 0.14121 | Δ +17.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.12454 | val 0.11613 | noisy 0.14577 | Δ +20.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.13225 | val 0.10951 | noisy 0.12972 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.13026 | val 0.11452 | noisy 0.12696 | Δ +9.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.13441 | val 0.10849 | noisy 0.13022 | Δ +16.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.12910 | val 0.11293 | noisy 0.12892 | Δ +12.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.12344 | val 0.11793 | noisy 0.13173 | Δ +10.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.12466 | val 0.11269 | noisy 0.14325 | Δ +21.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.13010 | val 0.11227 | noisy 0.13307 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.12758 | val 0.10738 | noisy 0.12490 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.12681 | val 0.11742 | noisy 0.13133 | Δ +10.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.13213 | val 0.10306 | noisy 0.14802 | Δ +30.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 027 | train 0.12720 | val 0.10960 | noisy 0.13156 | Δ +16.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 028 | train 0.12473 | val 0.10768 | noisy 0.11865 | Δ +9.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 029 | train 0.12509 | val 0.10074 | noisy 0.11860 | Δ +15.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 030 | train 0.13057 | val 0.11380 | noisy 0.14086 | Δ +19.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 031 | train 0.12607 | val 0.11200 | noisy 0.13501 | Δ +17.0% | LR 0.01000\n",
      "\n",
      "Completed 10 runs.\n"
     ]
    }
   ],
   "source": [
    "RUNS = []\n",
    "\n",
    "for L in LAYER_OPTIONS:\n",
    "    for inst in INSTANCE_IDS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Instance {inst} | Layers {L}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        s1 = train_stage1(\n",
    "            X_train, X_val,\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=120, batch_size=32,\n",
    "            lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6,\n",
    "            seed=inst  # <-- ZMIANA\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        s3 = train_stage3(\n",
    "            X_train, X_val,\n",
    "            dec_dagger_params=s1[\"phi\"],\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=60, batch_size=16,\n",
    "            lr_init=0.010, noise_level=EVAL_SIGMA, seed=inst  # <-- już było\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        RUNS.append({\n",
    "            \"instance_id\": inst,\n",
    "            \"n_layers\": L,\n",
    "            \"stage1\": {\n",
    "                \"phi\": s1[\"phi\"],\n",
    "                \"best_val\": s1[\"best_val\"],\n",
    "                \"hist_train\": s1[\"hist_train\"],\n",
    "                \"hist_val\": s1[\"hist_val\"],\n",
    "                \"hist_lr\": s1[\"hist_lr\"],\n",
    "                \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                \"epochs\": s1.get(\"epochs\"),\n",
    "                \"train_seconds\": float(t1 - t0),\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"psi\": s3[\"psi\"],\n",
    "                \"best_val\": s3[\"best_val\"],\n",
    "                \"hist_train\": s3[\"hist_train\"],\n",
    "                \"hist_val\": s3[\"hist_val\"],\n",
    "                \"hist_lr\": s3[\"hist_lr\"],\n",
    "                \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                \"epochs\": s3.get(\"epochs\"),\n",
    "                \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": float(t2 - t1),\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a30b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle → ./runs_halfqae/q6_l2t4/L1\\6q_2l_4t_1ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l2t4/L1\\6q_2l_4t_1ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l2t4/L1\\6q_2l_4t_1ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l2t4/L1\\6q_2l_4t_1ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l2t4/L1\\6q_2l_4t_1ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l2t4/L3\\6q_2l_4t_3ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l2t4/L3\\6q_2l_4t_3ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l2t4/L3\\6q_2l_4t_3ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l2t4/L3\\6q_2l_4t_3ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l2t4/L3\\6q_2l_4t_3ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v4.csv\n",
      "\n",
      "All runs saved and recorded in CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artur\\AppData\\Local\\Temp\\ipykernel_17788\\274111771.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_new = pd.concat([df_old, row_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 7 — Save artifacts (JSON) and append a paper-ready CSV per run\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams logged (keep in sync with training cells) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.010\n",
    "S3_MAX_EPOCHS    = 60\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v4\"  # bump if you change columns\n",
    "\n",
    "# --- ensure dirs ---\n",
    "def ensure_dir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "ensure_dir(OUT_BASE)\n",
    "subroot = ensure_dir(f\"{OUT_BASE}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "\n",
    "# --- CSV path (versioned) ---\n",
    "CSV_PATH = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "\n",
    "# --- header for the full, paper-friendly table ---\n",
    "CSV_HEADER = [\n",
    "    # id / naming\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    # architecture\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    # noise & window\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    # stage-1 hyperparams + outcomes\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    # stage-3 hyperparams + outcomes\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    # params (JSON)\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    # totals\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    needs_header = True\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                first_line = f.readline().rstrip(\"\\n\")\n",
    "            needs_header = (first_line != \",\".join(header))\n",
    "        except Exception:\n",
    "            needs_header = True\n",
    "    if needs_header:\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(header)\n",
    "\n",
    "ensure_csv(CSV_PATH, CSV_HEADER)\n",
    "\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def save_one_run(run):\n",
    "    inst = int(run[\"instance_id\"])\n",
    "    L    = int(run[\"n_layers\"])\n",
    "    seed = int(run.get(\"seed\", inst))\n",
    "\n",
    "    # standardized filename: 6q_2l_4t_{L}ls_{inst:02d}.json\n",
    "    fname = f\"6q_2l_4t_{L}ls_{inst:02d}.json\"\n",
    "    out_dir = ensure_dir(f\"{subroot}/L{L}\")\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    # pull stage results (robust to missing keys)\n",
    "    s1 = run[\"stage1\"]\n",
    "    s3 = run[\"stage3\"]\n",
    "\n",
    "    # Stage-1 metrics\n",
    "    s1_hist_val = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val = float(s1.get(\"best_val\", np.nan))\n",
    "    s1_final_val = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = int(s1.get(\"best_epoch\", _safe_argmin(s1_hist_val)))\n",
    "    s1_epochs = int(s1.get(\"epochs\", len(s1_hist_val)))\n",
    "    s1_seconds = float(s1.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # Stage-3 metrics\n",
    "    s3_hist_val = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "    s3_best_val = float(s3.get(\"best_val\", np.nan))\n",
    "    s3_final_val = _safe_last(s3_hist_val)\n",
    "    s3_best_epoch = int(s3.get(\"best_epoch\", _safe_argmin(s3_hist_val)))\n",
    "    s3_epochs = int(s3.get(\"epochs\", len(s3_hist_val)))\n",
    "    s3_seconds = float(s3.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # --- compute metrics with FALLBACKS if curves are missing ---\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = (float(np.nanmax(s3_hist_delta)) if (len(s3_hist_delta) and np.isfinite(np.nanmax(s3_hist_delta)))\n",
    "                      else np.nan)\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    # bundle JSON (parameters + training curves)\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"scale_low\":  float(scale_low),\n",
    "            \"scale_high\": float(scale_high),\n",
    "            \"window_size\": int(n_qubits),\n",
    "            \"window_stride\": 1,\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(EVAL_SIGMA),\n",
    "            \"sigma_eval\":  float(EVAL_SIGMA),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(n_qubits),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(n_latent),\n",
    "            \"n_trash\":  int(n_trash),\n",
    "            \"latent_wires\": list(range(n_latent)),\n",
    "            \"trash_wires\":  list(range(n_latent, n_qubits)),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val, \"lr_curve\": s3.get(\"hist_lr\", []),\n",
    "                \"noisy_curve\": s3_hist_noisy, \"delta_curve\": s3_hist_delta,\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # assemble CSV row\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        OUT_BASE,\n",
    "        inst, seed,\n",
    "        int(n_qubits), int(n_latent), int(n_trash), int(L),\n",
    "        f\"{EVAL_SIGMA:.3f}\", f\"{EVAL_SIGMA:.3f}\", 1,\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS),\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert row into CSV\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(CSV_PATH).exists():\n",
    "        df_old = pd.read_csv(CSV_PATH)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(CSV_PATH, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Upserted CSV row  → {CSV_PATH}\")\n",
    "\n",
    "# ---- save all runs from Cell 7 ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved and recorded in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f983a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae/all_training_instances_v4.csv\n",
      "Saved per-layer summary → ./runs_halfqae/summary_by_layers_v4.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6q_2l_4t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084583</td>\n",
       "      <td>29</td>\n",
       "      <td>40</td>\n",
       "      <td>362.266902</td>\n",
       "      <td>0.096847</td>\n",
       "      <td>27.030260</td>\n",
       "      <td>13.802455</td>\n",
       "      <td>[0.012143380097392725, 0.1432805901447474, -0....</td>\n",
       "      <td>[-0.010728030959690198, -0.17224978741066804, ...</td>\n",
       "      <td>541.401620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6q_2l_4t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083105</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>278.622731</td>\n",
       "      <td>0.096195</td>\n",
       "      <td>22.993786</td>\n",
       "      <td>18.745425</td>\n",
       "      <td>[-0.027729893275904288, 0.1413423614815289, 0....</td>\n",
       "      <td>[-0.01601141263363774, -0.14928682486781644, 0...</td>\n",
       "      <td>391.960252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6q_2l_4t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078174</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>510.525815</td>\n",
       "      <td>0.096605</td>\n",
       "      <td>28.700940</td>\n",
       "      <td>15.111493</td>\n",
       "      <td>[-0.027198268338826725, 0.1406446500150668, 0....</td>\n",
       "      <td>[0.010174084194762996, -0.16757991338784559, -...</td>\n",
       "      <td>698.275214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6q_2l_4t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082526</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>281.098372</td>\n",
       "      <td>0.094265</td>\n",
       "      <td>23.256423</td>\n",
       "      <td>11.574324</td>\n",
       "      <td>[-0.01848022070332372, 0.15764677747705141, -0...</td>\n",
       "      <td>[-0.005345839425610632, -0.1653743239044449, 0...</td>\n",
       "      <td>377.721080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6q_2l_4t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085707</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "      <td>684.063408</td>\n",
       "      <td>0.096764</td>\n",
       "      <td>26.565883</td>\n",
       "      <td>7.455419</td>\n",
       "      <td>[-0.021543647824686495, 0.15826450696895547, 0...</td>\n",
       "      <td>[0.007316379967300952, -0.18097876780716118, -...</td>\n",
       "      <td>905.950694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6q_2l_4t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115834</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>2182.276234</td>\n",
       "      <td>0.132059</td>\n",
       "      <td>24.035380</td>\n",
       "      <td>15.233271</td>\n",
       "      <td>[0.03295819077228067, 0.0042772556949675885, 0...</td>\n",
       "      <td>[0.006441900801966677, -0.1852132512456289, -0...</td>\n",
       "      <td>3550.284151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6q_2l_4t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120390</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>4611.456481</td>\n",
       "      <td>0.134302</td>\n",
       "      <td>25.558860</td>\n",
       "      <td>7.189583</td>\n",
       "      <td>[0.0004130027166683182, 0.001145083629383589, ...</td>\n",
       "      <td>[0.0280897289290998, -0.18256169348419035, -0....</td>\n",
       "      <td>5527.893094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6q_2l_4t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121061</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>1530.949874</td>\n",
       "      <td>0.133877</td>\n",
       "      <td>26.344039</td>\n",
       "      <td>9.320371</td>\n",
       "      <td>[-0.023114319636427284, -0.0004993691055237776...</td>\n",
       "      <td>[-0.06392018141979411, -0.15978136836311524, 0...</td>\n",
       "      <td>2324.733697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6q_2l_4t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116956</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>1126.049858</td>\n",
       "      <td>0.131768</td>\n",
       "      <td>20.041357</td>\n",
       "      <td>13.726740</td>\n",
       "      <td>[-0.007515602607147276, 0.0029983618269571466,...</td>\n",
       "      <td>[0.01198363316257304, -0.14936091072768942, -0...</td>\n",
       "      <td>1899.289759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6q_2l_4t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112001</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>2920.068596</td>\n",
       "      <td>0.133321</td>\n",
       "      <td>30.376725</td>\n",
       "      <td>17.042503</td>\n",
       "      <td>[0.0034391780777214983, 0.003006072805543395, ...</td>\n",
       "      <td>[0.042338921964118155, -0.18302453313481545, -...</td>\n",
       "      <td>3564.648917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag  dataset_folder  instance_id  rng_seed  \\\n",
       "0  6q_2l_4t_1ls_01.json  inst1_L1  ./runs_halfqae            1         1   \n",
       "1  6q_2l_4t_1ls_02.json  inst2_L1  ./runs_halfqae            2         2   \n",
       "2  6q_2l_4t_1ls_03.json  inst3_L1  ./runs_halfqae            3         3   \n",
       "3  6q_2l_4t_1ls_04.json  inst4_L1  ./runs_halfqae            4         4   \n",
       "4  6q_2l_4t_1ls_05.json  inst5_L1  ./runs_halfqae            5         5   \n",
       "5  6q_2l_4t_3ls_01.json  inst1_L3  ./runs_halfqae            1         1   \n",
       "6  6q_2l_4t_3ls_02.json  inst2_L3  ./runs_halfqae            2         2   \n",
       "7  6q_2l_4t_3ls_03.json  inst3_L3  ./runs_halfqae            3         3   \n",
       "8  6q_2l_4t_3ls_04.json  inst4_L3  ./runs_halfqae            4         4   \n",
       "9  6q_2l_4t_3ls_05.json  inst5_L3  ./runs_halfqae            5         5   \n",
       "\n",
       "   n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  s3_final_val_mse  \\\n",
       "0         6         2        4         1          0.1  ...          0.084583   \n",
       "1         6         2        4         1          0.1  ...          0.083105   \n",
       "2         6         2        4         1          0.1  ...          0.078174   \n",
       "3         6         2        4         1          0.1  ...          0.082526   \n",
       "4         6         2        4         1          0.1  ...          0.085707   \n",
       "5         6         2        4         3          0.1  ...          0.115834   \n",
       "6         6         2        4         3          0.1  ...          0.120390   \n",
       "7         6         2        4         3          0.1  ...          0.121061   \n",
       "8         6         2        4         3          0.1  ...          0.116956   \n",
       "9         6         2        4         3          0.1  ...          0.112001   \n",
       "\n",
       "   s3_best_epoch  s3_epochs  s3_train_seconds  s3_noisy_baseline_mse  \\\n",
       "0             29         40        362.266902               0.096847   \n",
       "1             16         27        278.622731               0.096195   \n",
       "2             26         37        510.525815               0.096605   \n",
       "3             20         31        281.098372               0.094265   \n",
       "4             41         52        684.063408               0.096764   \n",
       "5             19         30       2182.276234               0.132059   \n",
       "6             16         27       4611.456481               0.134302   \n",
       "7             16         27       1530.949874               0.133877   \n",
       "8              7         18       1126.049858               0.131768   \n",
       "9             21         32       2920.068596               0.133321   \n",
       "\n",
       "   s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0          27.030260           13.802455   \n",
       "1          22.993786           18.745425   \n",
       "2          28.700940           15.111493   \n",
       "3          23.256423           11.574324   \n",
       "4          26.565883            7.455419   \n",
       "5          24.035380           15.233271   \n",
       "6          25.558860            7.189583   \n",
       "7          26.344039            9.320371   \n",
       "8          20.041357           13.726740   \n",
       "9          30.376725           17.042503   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [0.012143380097392725, 0.1432805901447474, -0....   \n",
       "1  [-0.027729893275904288, 0.1413423614815289, 0....   \n",
       "2  [-0.027198268338826725, 0.1406446500150668, 0....   \n",
       "3  [-0.01848022070332372, 0.15764677747705141, -0...   \n",
       "4  [-0.021543647824686495, 0.15826450696895547, 0...   \n",
       "5  [0.03295819077228067, 0.0042772556949675885, 0...   \n",
       "6  [0.0004130027166683182, 0.001145083629383589, ...   \n",
       "7  [-0.023114319636427284, -0.0004993691055237776...   \n",
       "8  [-0.007515602607147276, 0.0029983618269571466,...   \n",
       "9  [0.0034391780777214983, 0.003006072805543395, ...   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [-0.010728030959690198, -0.17224978741066804, ...           541.401620  \n",
       "1  [-0.01601141263363774, -0.14928682486781644, 0...           391.960252  \n",
       "2  [0.010174084194762996, -0.16757991338784559, -...           698.275214  \n",
       "3  [-0.005345839425610632, -0.1653743239044449, 0...           377.721080  \n",
       "4  [0.007316379967300952, -0.18097876780716118, -...           905.950694  \n",
       "5  [0.006441900801966677, -0.1852132512456289, -0...          3550.284151  \n",
       "6  [0.0280897289290998, -0.18256169348419035, -0....          5527.893094  \n",
       "7  [-0.06392018141979411, -0.15978136836311524, 0...          2324.733697  \n",
       "8  [0.01198363316257304, -0.14936091072768942, -0...          1899.289759  \n",
       "9  [0.042338921964118155, -0.18302453313481545, -...          3564.648917  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.096135 ± 0.000961</td>\n",
       "      <td>0.090944 ± 0.000855</td>\n",
       "      <td>0.082819 ± 0.002577</td>\n",
       "      <td>25.709458 ± 2.227968</td>\n",
       "      <td>13.337823 ± 3.750108</td>\n",
       "      <td>0.108836 ± 0.000685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.133066 ± 0.000995</td>\n",
       "      <td>0.120879 ± 0.001593</td>\n",
       "      <td>0.117248 ± 0.003287</td>\n",
       "      <td>25.271272 ± 3.351973</td>\n",
       "      <td>12.502494 ± 3.685663</td>\n",
       "      <td>0.108627 ± 0.000668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.096135 ± 0.000961     0.090944 ± 0.000855   \n",
       "3            5           0.133066 ± 0.000995     0.120879 ± 0.001593   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.082819 ± 0.002577      25.709458 ± 2.227968   \n",
       "3             0.117248 ± 0.003287      25.271272 ± 3.351973   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1              13.337823 ± 3.750108    0.108836 ± 0.000685  \n",
       "3              12.502494 ± 3.685663    0.108627 ± 0.000668  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results table\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(CSV_PATH).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Run Cell 8 first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- NEW: drop duplicate runs; keep the newest copy (with baseline/delta)\n",
    "if \"filename\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "# Typical numeric casts (safe)\n",
    "for col in [\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\n",
    "    \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "    \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\"\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "\n",
    "clean_path = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "Path(OUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved training-only table → {clean_path}\")\n",
    "\n",
    "# A compact per-layer summary (mean±std); guards against all-NaN\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "metrics = [\n",
    "    (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "    (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "    (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "    (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "    (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "    (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "]\n",
    "\n",
    "grp = df.groupby(\"n_layers\", dropna=False)\n",
    "summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "for col, label in metrics:\n",
    "    if col in df.columns and np.isfinite(df[col]).any():\n",
    "        summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "summary_path = f\"{OUT_BASE}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "summary.to_csv(summary_path, index=True)\n",
    "print(f\"Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca9b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
