{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4262a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# We'll run 5 instances (same across all notebooks) and two depths: 1 and 3 layers.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # used in filenames as ..._ls_01.json, ..._ls_02.json, ...\n",
    "LAYER_OPTIONS  = [1, 3]            # train 1-layer first, then 3-layers\n",
    "EVAL_SIGMA     = 0.10              # fixed noise everywhere (train & eval)\n",
    "\n",
    "# where to save artifacts (JSON bundles, instance records, CSV summary)\n",
    "OUT_BASE = \"./runs_halfqae\"        # change if you like; subfolders will be created automatically\n",
    "CSV_PATH = f\"{OUT_BASE}/results_instances.csv\"  # will be appended-to if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818bab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== IMPORTY ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys, json, math, random, time, hashlib\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd62d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== USTAWIENIA OGÓLNE ====\n",
    "np.random.seed(42)\n",
    "\n",
    "# symulator (szybki)\n",
    "BACKEND = \"lightning.qubit\"\n",
    "\n",
    "# Half-QAE\n",
    "n_qubits = 4\n",
    "n_latent = 2\n",
    "n_trash  = n_qubits - n_latent  # 2\n",
    "\n",
    "# dane Mackey-Glass \n",
    "beta=0.25\n",
    "gamma=0.1\n",
    "n=10\n",
    "tau=15\n",
    "dt=1.0\n",
    "T=300\n",
    "\n",
    "# skalowanie\n",
    "margin = 0.2\n",
    "scale_low, scale_high = 0.0+margin, 1.0-margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e005c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== POMOCNICZE ====\n",
    "def scale_values(x, new_min=0.0, new_max=1.0):\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    return new_min + (x - x_min) * (new_max - new_min) / max(1e-12, (x_max - x_min))\n",
    "\n",
    "def mackey_glass(beta=0.2, gamma=0.1, n=10, tau=17, dt=0.1, T=1000):\n",
    "    N = int(T/dt)\n",
    "    delay_steps = int(tau/dt)\n",
    "    x = np.zeros(N+delay_steps)\n",
    "    x[0:delay_steps] = 1.2\n",
    "    for t in range(delay_steps, N+delay_steps-1):\n",
    "        x_tau = x[t-delay_steps]\n",
    "        dxdt = beta * x_tau / (1 + x_tau**n) - gamma * x[t]\n",
    "        x[t+1] = x[t] + dxdt * dt\n",
    "    return x[delay_steps:]\n",
    "\n",
    "def ts_add_noise(X, noise=0.1, low=0.0, high=1.0):\n",
    "    Z = X + np.random.normal(0.0, noise, size=X.shape)\n",
    "    return np.clip(Z, low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7fac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== GENEROWANIE DANYCH ====\n",
    "y_raw = mackey_glass(beta=beta, gamma=gamma, n=n, tau=tau, dt=dt, T=T)\n",
    "y_raw = scale_values(y_raw, new_min=scale_low, new_max=scale_high)\n",
    "y = y_raw[2::3]                               # subsampling\n",
    "X_idx = np.arange(len(y))\n",
    "\n",
    "# okna (sliding window)\n",
    "window_size = n_qubits\n",
    "stride = 1\n",
    "X_windows = np.stack([y[i:i+window_size] for i in range(0, len(y)-window_size+1, stride)])\n",
    "\n",
    "# podziały: train/val/test = 0.6 / 0.2 / 0.2\n",
    "X_temp, X_test = train_test_split(X_windows, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_temp,   test_size=0.25, random_state=42)\n",
    "\n",
    "# czyste zbiory (na Stage 1 i Stage 2 generujemy szum dynamicznie)\n",
    "X_train_clean = X_train.copy()\n",
    "X_val_clean   = X_val.copy()\n",
    "X_test_clean  = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7651bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ANSATZE  ====\n",
    "def encoder_ansatz(params, x, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    # angle encoding\n",
    "    for w, val in zip(W, x[:len(W)]):\n",
    "        qml.RY(val*np.pi, wires=w)\n",
    "    # warstwy rotacji\n",
    "    n_block = len(W)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in range(n_layers):\n",
    "        for j, w in enumerate(W):\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RX(params[idx],     wires=w)\n",
    "            qml.RY(params[idx + 1], wires=w)\n",
    "            qml.RZ(params[idx + 2], wires=w)\n",
    "        for a, b in zip(W, W[1:]):\n",
    "            qml.CNOT(wires=[a, b])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n",
    "\n",
    "def decoder_ansatz(params, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    n_block = len(W)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in range(n_layers):\n",
    "        for a, b in zip(W, W[1:]):\n",
    "            qml.CNOT(wires=[a, b])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n",
    "        for j, w in enumerate(W):\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RZ(params[idx + 2], wires=w)\n",
    "            qml.RY(params[idx + 1], wires=w)\n",
    "            qml.RX(params[idx],     wires=w)\n",
    "\n",
    "def adjoint_decoder_ansatz(params, x, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    n_block = len(W)\n",
    "    for w, val in zip(W, x[:n_block]):\n",
    "        qml.RY(val*np.pi, wires=w)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in reversed(range(n_layers)):\n",
    "        for j in reversed(range(n_block)):\n",
    "            w = W[j]\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RX(-params[idx],     wires=w)\n",
    "            qml.RY(-params[idx + 1], wires=w)\n",
    "            qml.RZ(-params[idx + 2], wires=w)\n",
    "        for j in reversed(range(n_block - 1)):\n",
    "            qml.CNOT(wires=[W[j+1], W[j]])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c130eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STAGE 1: swap-test na adjoint_decoder (uczenie „trash->czysto”) ====\n",
    "# inicjalizacje\n",
    "#n_layers = 2\n",
    "#param_shape = n_layers * n_qubits * 3\n",
    "\n",
    "trash2_start = n_qubits               # 4\n",
    "ancilla      = n_qubits + n_trash     # 6\n",
    "num_total_s1 = n_qubits + n_trash + 1 # 7\n",
    "\n",
    "dev_s1 = qml.device(BACKEND, wires=num_total_s1, shots=None)\n",
    "\n",
    "@qml.qnode(dev_s1, interface=\"autograd\")\n",
    "def swap_test_on_adj_decoder(x, params):\n",
    "    main   = list(range(n_qubits))                                # [0,1,2,3]\n",
    "    trash1 = list(range(n_latent, n_latent + n_trash))            # [2,3]\n",
    "    trash2 = list(range(trash2_start, trash2_start + n_trash))    # [4,5]\n",
    "\n",
    "    adjoint_decoder_ansatz(params, x)\n",
    "\n",
    "    for i in trash2:\n",
    "        qml.Hadamard(wires=i)\n",
    "    qml.Hadamard(wires=ancilla)\n",
    "    for i in range(n_trash):\n",
    "        qml.CSWAP(wires=[ancilla, trash1[i], trash2[i]])\n",
    "    qml.Hadamard(wires=ancilla)\n",
    "\n",
    "    return qml.probs(wires=ancilla)\n",
    "\n",
    "def s1_cost(params, X_batch):\n",
    "    acc = pnp.array(0.0)\n",
    "    for x in X_batch:\n",
    "        p0 = swap_test_on_adj_decoder(x, params)[0]\n",
    "        acc = acc + (1.0 - p0)\n",
    "    return acc / len(X_batch)\n",
    "\n",
    "def train_adjoint_decoder(params_init, X_train, n_epochs=80, batch_size=8, lr=0.01, X_val_clean=None, seed=0):  # <-- ZMIANA\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    params = pnp.array(params_init, requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr)\n",
    "    hist = []\n",
    "    hist_val = []\n",
    "    lr_hist = []\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    t0 = time.time()\n",
    "    for ep in range(n_epochs):\n",
    "        idx = rng.permutation(len(X_train))  # <-- ZMIANA\n",
    "        s=0.0; nb=0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            Xb = X_train[idx[i:i+batch_size]]\n",
    "            params, c = opt.step_and_cost(lambda p: s1_cost(p, Xb), params)\n",
    "            s += float(c); nb += 1\n",
    "        train_cost = s/nb\n",
    "        hist.append(train_cost)\n",
    "        lr_hist.append(opt.stepsize)\n",
    "        # --- walidacja ---\n",
    "        if X_val_clean is not None:\n",
    "            val_cost = float(s1_cost(params, X_val_clean))\n",
    "            hist_val.append(val_cost)\n",
    "        else:\n",
    "            val_cost = train_cost\n",
    "            hist_val.append(train_cost)\n",
    "        print(f\"[Stage1] L={len(params_init)//(n_qubits*3)} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {opt.stepsize:.5f}\")\n",
    "        if train_cost < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = train_cost, pnp.array(params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= 10:\n",
    "            break\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    return dict(\n",
    "        phi=best_params if best_params is not None else params,\n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=len(hist),\n",
    "        hist_train=list(map(float, hist)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb1478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STAGE 2: trenujemy ENCODER latent↔latent (noisy vs clean_ref) ====\n",
    "dev_s2 = qml.device(BACKEND, wires=2*n_qubits + 1, shots=None)\n",
    "\n",
    "@qml.qnode(dev_s2, interface=\"autograd\")\n",
    "def swap_test_encoder_latent(x_noisy, x_clean, enc_params, dec_dagger_params):\n",
    "    encoder = list(range(n_qubits))                  # blok encodera\n",
    "    ref     = list(range(n_qubits, 2*n_qubits))      # blok referencyjny\n",
    "    anc     = 2*n_qubits\n",
    "\n",
    "    encoder_ansatz(enc_params, x_noisy, wires=encoder)\n",
    "    adjoint_decoder_ansatz(dec_dagger_params, x_clean, wires=ref)\n",
    "\n",
    "    qml.Hadamard(wires=anc)\n",
    "    for i in range(n_latent):\n",
    "        qml.CSWAP(wires=[anc, encoder[i], ref[i]])   # SWAP tylko latentów\n",
    "    qml.Hadamard(wires=anc)\n",
    "    return qml.probs(wires=anc)\n",
    "\n",
    "def s2_cost(enc_params, X_noisy_b, X_clean_b, dec_dagger_params):\n",
    "    acc = pnp.array(0.0)\n",
    "    for xn, xc in zip(X_noisy_b, X_clean_b):\n",
    "        p0 = swap_test_encoder_latent(xn, xc, enc_params, dec_dagger_params)[0]\n",
    "        acc = acc + (1.0 - p0)\n",
    "    return acc / len(X_noisy_b)\n",
    "\n",
    "# skala szumu czerpana z realnego skalowania danych\n",
    "info = {\"scale_low\": scale_low, \"scale_high\": scale_high}\n",
    "\n",
    "def s2_cost_dataset(enc_params, X_clean_set, dec_params, noise_level):\n",
    "    \"\"\"Metryka testowa S2: generujemy szum o danym poziomie i liczymy koszt.\"\"\"\n",
    "    low, high = info[\"scale_low\"], info[\"scale_high\"]\n",
    "    sigma = noise_level * (high - low)\n",
    "    X_noisy_set = ts_add_noise(X_clean_set, noise=sigma, low=low, high=high)\n",
    "    return float(s2_cost(enc_params, X_noisy_set, X_clean_set, dec_params))\n",
    "\n",
    "def train_encoder_with_sidekick_dyn_noise(enc_params_init, X_clean,\n",
    "                                         dec_dagger_params, noise_level,\n",
    "                                         n_epochs=80, batch_size=8, lr=0.01, seed=0,\n",
    "                                         X_val_clean=None):\n",
    "    params = pnp.array(enc_params_init, requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr)\n",
    "    hist = []\n",
    "    hist_val = []\n",
    "    lr_hist = []\n",
    "    hist_noisy = []\n",
    "    hist_delta = []\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    t0 = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    low, high = info[\"scale_low\"], info[\"scale_high\"]\n",
    "    sigma = noise_level * (high - low)\n",
    "    for ep in range(n_epochs):\n",
    "        idx = rng.permutation(len(X_clean))\n",
    "        Xc = X_clean[idx]\n",
    "        s = 0.0; nb = 0\n",
    "        for i in range(0, len(Xc), batch_size):\n",
    "            Xcb = Xc[i:i+batch_size]\n",
    "            Xnb = ts_add_noise(Xcb, noise=sigma, low=low, high=high)\n",
    "            params, c = opt.step_and_cost(\n",
    "                lambda p: s2_cost(p, Xnb, Xcb, dec_dagger_params), params\n",
    "            )\n",
    "            s += float(c); nb += 1\n",
    "        train_cost = s/nb\n",
    "        hist.append(train_cost)\n",
    "        lr_hist.append(opt.stepsize)\n",
    "        # --- walidacja ---\n",
    "        if X_val_clean is not None:\n",
    "            val_cost = float(s2_cost_dataset(params, X_val_clean, dec_dagger_params, noise_level))\n",
    "            # --- baseline/noise-only na walidacji ---\n",
    "            X_noisy_val = ts_add_noise(X_val_clean, noise=sigma, low=low, high=high)\n",
    "            mse_noisy = float(s2_cost(params*0, X_noisy_val, X_val_clean, dec_dagger_params))  # params*0 = brak uczenia\n",
    "            hist_noisy.append(mse_noisy)\n",
    "            if mse_noisy > 1e-12:\n",
    "                delta = 100.0 * (1.0 - val_cost / mse_noisy)\n",
    "            else:\n",
    "                delta = 0.0\n",
    "            hist_delta.append(delta)\n",
    "            hist_val.append(val_cost)\n",
    "        else:\n",
    "            val_cost = train_cost\n",
    "            hist_val.append(train_cost)\n",
    "            hist_noisy.append(np.nan)\n",
    "            hist_delta.append(np.nan)\n",
    "        print(f\"[Stage3] L={len(enc_params_init)//(n_qubits*3)} ep {ep:03d} | train {train_cost:.5f} | \"\n",
    "              f\"val {val_cost:.5f} | noisy {mse_noisy:.5f} | Δ {delta:+.1f}% | LR {opt.stepsize:.5f}\")\n",
    "        if train_cost < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = train_cost, pnp.array(params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= 10:\n",
    "            break\n",
    "    \n",
    "    train_seconds = float(time.time() - t0)\n",
    "    return dict(\n",
    "        psi=best_params if best_params is not None else params,\n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=len(hist),\n",
    "        hist_train=list(map(float, hist)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdbf3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(X_train, X_val, n_layers=2, instance_id=1, n_epochs=120, batch_size=32, lr_init=0.01, patience=10, lr_patience=8, min_delta=1e-6, seed=0):  # <-- ZMIANA\n",
    "    param_shape = n_layers * n_qubits * 3\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    params_init = rng.uniform(-0.01, 0.01, param_shape)  # <-- ZMIANA\n",
    "    result = train_adjoint_decoder(\n",
    "        params_init, X_train,\n",
    "        n_epochs=n_epochs, batch_size=batch_size, lr=lr_init,\n",
    "        X_val_clean=X_val,\n",
    "        seed=seed  # <-- ZMIANA\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def train_stage3(X_train, X_val, dec_dagger_params, n_layers=2, instance_id=1, n_epochs=120, batch_size=32, lr_init=0.01, noise_level=0.10, seed=0):\n",
    "    param_shape = n_layers * n_qubits * 3\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    enc_params_init = rng.uniform(-0.01, 0.01, param_shape)  # <-- ZMIANA\n",
    "    result = train_encoder_with_sidekick_dyn_noise(\n",
    "        enc_params_init, X_train,\n",
    "        dec_dagger_params, noise_level,\n",
    "        n_epochs=n_epochs, batch_size=batch_size, lr=lr_init, seed=seed,\n",
    "        X_val_clean=X_val\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "067c377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.087824 | val 0.073845 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.086957 | val 0.072158 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.085381 | val 0.070621 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.083324 | val 0.069362 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.083298 | val 0.068340 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.082691 | val 0.067508 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.081251 | val 0.066918 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.081301 | val 0.066531 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.080645 | val 0.066284 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.080227 | val 0.066190 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.079468 | val 0.066147 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.081018 | val 0.066209 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.081150 | val 0.066288 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.080301 | val 0.066399 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.082320 | val 0.066482 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.080065 | val 0.066610 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.080610 | val 0.066667 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.080800 | val 0.066710 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.081501 | val 0.066730 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.081375 | val 0.066709 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.081527 | val 0.066713 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.11352 | val 0.08881 | noisy 0.08735 | Δ -1.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11376 | val 0.08947 | noisy 0.08494 | Δ -5.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.11298 | val 0.09128 | noisy 0.08430 | Δ -8.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10750 | val 0.07936 | noisy 0.08135 | Δ +2.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.10298 | val 0.07142 | noisy 0.07931 | Δ +9.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10575 | val 0.08547 | noisy 0.09169 | Δ +6.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10158 | val 0.07984 | noisy 0.09369 | Δ +14.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10359 | val 0.07484 | noisy 0.08684 | Δ +13.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10014 | val 0.07113 | noisy 0.08342 | Δ +14.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10350 | val 0.07764 | noisy 0.09263 | Δ +16.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10223 | val 0.07042 | noisy 0.08362 | Δ +15.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.09400 | val 0.07224 | noisy 0.08393 | Δ +13.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10297 | val 0.08239 | noisy 0.09506 | Δ +13.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.09875 | val 0.09264 | noisy 0.09752 | Δ +5.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.10706 | val 0.08368 | noisy 0.09360 | Δ +10.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.09706 | val 0.07670 | noisy 0.07806 | Δ +1.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.09821 | val 0.07705 | noisy 0.09314 | Δ +17.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.10585 | val 0.07629 | noisy 0.08475 | Δ +10.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.09940 | val 0.08079 | noisy 0.08855 | Δ +8.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.10037 | val 0.07497 | noisy 0.08276 | Δ +9.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.09258 | val 0.07733 | noisy 0.08631 | Δ +10.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.10287 | val 0.08225 | noisy 0.08728 | Δ +5.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.09820 | val 0.07633 | noisy 0.08534 | Δ +10.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.10327 | val 0.07574 | noisy 0.07958 | Δ +4.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 024 | train 0.10092 | val 0.07214 | noisy 0.08500 | Δ +15.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 025 | train 0.10834 | val 0.07529 | noisy 0.08418 | Δ +10.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 026 | train 0.10136 | val 0.07473 | noisy 0.08332 | Δ +10.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 027 | train 0.09843 | val 0.07381 | noisy 0.08384 | Δ +12.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 028 | train 0.09596 | val 0.08327 | noisy 0.07711 | Δ -8.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 029 | train 0.09394 | val 0.07761 | noisy 0.07586 | Δ -2.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 030 | train 0.09825 | val 0.07637 | noisy 0.08442 | Δ +9.5% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.089051 | val 0.074454 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.087951 | val 0.072727 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.086238 | val 0.071236 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.083549 | val 0.069879 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.081836 | val 0.068738 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.082533 | val 0.067834 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.081059 | val 0.067117 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.081505 | val 0.066599 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.078512 | val 0.066278 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.082412 | val 0.066100 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.080430 | val 0.066044 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.078364 | val 0.066052 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.080956 | val 0.066116 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.080780 | val 0.066222 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.081775 | val 0.066351 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.080726 | val 0.066462 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.080157 | val 0.066525 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.080373 | val 0.066593 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.081588 | val 0.066642 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.082443 | val 0.066679 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.081159 | val 0.066645 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.080853 | val 0.066663 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.10934 | val 0.08877 | noisy 0.09267 | Δ +4.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11107 | val 0.07756 | noisy 0.07959 | Δ +2.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10413 | val 0.08415 | noisy 0.08906 | Δ +5.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10696 | val 0.07698 | noisy 0.09338 | Δ +17.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.10481 | val 0.07943 | noisy 0.08945 | Δ +11.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10190 | val 0.07899 | noisy 0.08590 | Δ +8.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10858 | val 0.07610 | noisy 0.08917 | Δ +14.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10245 | val 0.07759 | noisy 0.08297 | Δ +6.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.09606 | val 0.07920 | noisy 0.08464 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10553 | val 0.07828 | noisy 0.08285 | Δ +5.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10469 | val 0.08241 | noisy 0.07870 | Δ -4.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.09522 | val 0.07617 | noisy 0.08638 | Δ +11.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10916 | val 0.07760 | noisy 0.09296 | Δ +16.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10783 | val 0.07922 | noisy 0.09641 | Δ +17.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.09985 | val 0.08038 | noisy 0.08662 | Δ +7.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.10247 | val 0.08748 | noisy 0.07934 | Δ -10.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.10448 | val 0.08699 | noisy 0.08759 | Δ +0.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.09796 | val 0.07682 | noisy 0.09305 | Δ +17.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.09921 | val 0.07670 | noisy 0.08921 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.09605 | val 0.07774 | noisy 0.09060 | Δ +14.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.09318 | val 0.08037 | noisy 0.09489 | Δ +15.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.09730 | val 0.07577 | noisy 0.09269 | Δ +18.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.10160 | val 0.07022 | noisy 0.08478 | Δ +17.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.10380 | val 0.07240 | noisy 0.08332 | Δ +13.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 024 | train 0.09775 | val 0.07598 | noisy 0.09192 | Δ +17.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 025 | train 0.10620 | val 0.07943 | noisy 0.08721 | Δ +8.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 026 | train 0.09648 | val 0.07396 | noisy 0.08959 | Δ +17.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 027 | train 0.09743 | val 0.07193 | noisy 0.08844 | Δ +18.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 028 | train 0.10163 | val 0.08307 | noisy 0.08872 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 029 | train 0.10311 | val 0.08337 | noisy 0.08083 | Δ -3.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 030 | train 0.10614 | val 0.07592 | noisy 0.08694 | Δ +12.7% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.089320 | val 0.074594 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.088690 | val 0.072752 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.086180 | val 0.071149 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.086778 | val 0.069850 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.083971 | val 0.068841 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.083010 | val 0.067961 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.083065 | val 0.067283 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.082047 | val 0.066780 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.081458 | val 0.066428 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.082222 | val 0.066208 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.079513 | val 0.066125 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.081001 | val 0.066119 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.079161 | val 0.066199 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.081175 | val 0.066290 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.080861 | val 0.066362 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.080938 | val 0.066448 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.080659 | val 0.066523 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.079598 | val 0.066585 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.080265 | val 0.066623 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.079149 | val 0.066623 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.081644 | val 0.066630 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.081455 | val 0.066638 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.080655 | val 0.066598 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.078936 | val 0.066562 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.081356 | val 0.066518 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.081162 | val 0.066485 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.081861 | val 0.066483 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.081138 | val 0.066467 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.080903 | val 0.066462 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.081151 | val 0.066479 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.080292 | val 0.066519 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.080300 | val 0.066531 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.081258 | val 0.066515 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.079678 | val 0.066546 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.11506 | val 0.08780 | noisy 0.08405 | Δ -4.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11309 | val 0.08353 | noisy 0.08883 | Δ +6.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.11250 | val 0.07522 | noisy 0.08873 | Δ +15.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10546 | val 0.08388 | noisy 0.09385 | Δ +10.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.09859 | val 0.07183 | noisy 0.08205 | Δ +12.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10563 | val 0.07706 | noisy 0.08218 | Δ +6.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10451 | val 0.08994 | noisy 0.07541 | Δ -19.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.09659 | val 0.07138 | noisy 0.09393 | Δ +24.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10344 | val 0.08550 | noisy 0.08771 | Δ +2.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10472 | val 0.07277 | noisy 0.08724 | Δ +16.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.09514 | val 0.07912 | noisy 0.09118 | Δ +13.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10244 | val 0.07531 | noisy 0.08752 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10507 | val 0.07391 | noisy 0.07919 | Δ +6.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10282 | val 0.08719 | noisy 0.08050 | Δ -8.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.09654 | val 0.08680 | noisy 0.07502 | Δ -15.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.10139 | val 0.07946 | noisy 0.08442 | Δ +5.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.09857 | val 0.08353 | noisy 0.09323 | Δ +10.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.09523 | val 0.07391 | noisy 0.09395 | Δ +21.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.09977 | val 0.07464 | noisy 0.09163 | Δ +18.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.09839 | val 0.07291 | noisy 0.08006 | Δ +8.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.09956 | val 0.07978 | noisy 0.08221 | Δ +3.0% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.089193 | val 0.073858 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.085024 | val 0.071996 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.083042 | val 0.070547 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.084498 | val 0.069393 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.085317 | val 0.068402 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.082198 | val 0.067638 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.083465 | val 0.067075 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.081368 | val 0.066687 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.083159 | val 0.066446 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.079920 | val 0.066357 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.082222 | val 0.066336 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.079772 | val 0.066389 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.080730 | val 0.066426 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.079526 | val 0.066494 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.081735 | val 0.066541 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.080165 | val 0.066577 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.078528 | val 0.066592 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.081566 | val 0.066644 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.080608 | val 0.066664 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.080914 | val 0.066653 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.079646 | val 0.066610 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.079543 | val 0.066579 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.081835 | val 0.066572 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.081188 | val 0.066544 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.078818 | val 0.066539 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.079273 | val 0.066534 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.080553 | val 0.066495 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.11131 | val 0.08172 | noisy 0.08642 | Δ +5.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.10625 | val 0.08853 | noisy 0.08539 | Δ -3.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10241 | val 0.07804 | noisy 0.09910 | Δ +21.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10051 | val 0.06960 | noisy 0.08375 | Δ +16.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.10825 | val 0.07818 | noisy 0.08469 | Δ +7.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10436 | val 0.08070 | noisy 0.09020 | Δ +10.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10549 | val 0.08020 | noisy 0.09122 | Δ +12.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10312 | val 0.07471 | noisy 0.09263 | Δ +19.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10518 | val 0.08168 | noisy 0.08896 | Δ +8.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.09819 | val 0.07184 | noisy 0.08743 | Δ +17.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10279 | val 0.07906 | noisy 0.08848 | Δ +10.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10101 | val 0.08528 | noisy 0.09112 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10175 | val 0.08204 | noisy 0.08436 | Δ +2.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.09681 | val 0.07387 | noisy 0.08423 | Δ +12.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.10096 | val 0.08489 | noisy 0.08479 | Δ -0.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.09770 | val 0.07675 | noisy 0.08306 | Δ +7.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.10176 | val 0.08100 | noisy 0.09458 | Δ +14.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.10262 | val 0.08143 | noisy 0.08944 | Δ +8.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.09770 | val 0.08187 | noisy 0.08389 | Δ +2.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.10123 | val 0.08250 | noisy 0.08975 | Δ +8.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.10121 | val 0.07799 | noisy 0.08742 | Δ +10.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.10132 | val 0.07406 | noisy 0.08569 | Δ +13.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.10094 | val 0.08162 | noisy 0.08705 | Δ +6.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.09903 | val 0.07801 | noisy 0.08294 | Δ +5.9% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.089376 | val 0.074690 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.087208 | val 0.073289 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.086451 | val 0.071843 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.084805 | val 0.070655 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.084142 | val 0.069600 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.083385 | val 0.068665 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.082700 | val 0.067929 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.082134 | val 0.067328 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.082347 | val 0.066879 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.079757 | val 0.066613 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.079686 | val 0.066401 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.079831 | val 0.066262 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.081166 | val 0.066210 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.079641 | val 0.066219 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.081235 | val 0.066271 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.081082 | val 0.066315 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.079639 | val 0.066347 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.081881 | val 0.066412 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.080086 | val 0.066503 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.079073 | val 0.066602 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.080822 | val 0.066628 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.081295 | val 0.066669 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.081719 | val 0.066699 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.079928 | val 0.066701 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.081165 | val 0.066696 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.081704 | val 0.066666 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.078884 | val 0.066622 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.079954 | val 0.066558 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.079957 | val 0.066514 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.081989 | val 0.066481 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.079786 | val 0.066448 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.080557 | val 0.066436 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.080892 | val 0.066401 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.082574 | val 0.066390 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.081128 | val 0.066391 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.080859 | val 0.066358 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.082033 | val 0.066365 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.10951 | val 0.08540 | noisy 0.09615 | Δ +11.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.10977 | val 0.08379 | noisy 0.08828 | Δ +5.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.10994 | val 0.07668 | noisy 0.08934 | Δ +14.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.10377 | val 0.07896 | noisy 0.08803 | Δ +10.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.09595 | val 0.07046 | noisy 0.09049 | Δ +22.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.10238 | val 0.08087 | noisy 0.08669 | Δ +6.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.10107 | val 0.07521 | noisy 0.09582 | Δ +21.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10300 | val 0.08023 | noisy 0.09679 | Δ +17.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.10145 | val 0.08085 | noisy 0.09294 | Δ +13.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.09682 | val 0.08538 | noisy 0.08099 | Δ -5.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10050 | val 0.07825 | noisy 0.08563 | Δ +8.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.09880 | val 0.07716 | noisy 0.09512 | Δ +18.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10294 | val 0.07496 | noisy 0.08460 | Δ +11.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.09147 | val 0.07522 | noisy 0.09924 | Δ +24.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.10304 | val 0.07913 | noisy 0.08936 | Δ +11.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.09141 | val 0.08099 | noisy 0.09987 | Δ +18.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.10311 | val 0.07363 | noisy 0.09155 | Δ +19.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.10126 | val 0.07970 | noisy 0.07512 | Δ -6.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.09909 | val 0.07644 | noisy 0.08732 | Δ +12.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.09877 | val 0.07479 | noisy 0.07429 | Δ -0.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.09411 | val 0.07530 | noisy 0.09666 | Δ +22.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.09695 | val 0.07617 | noisy 0.08543 | Δ +10.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.09894 | val 0.07447 | noisy 0.08445 | Δ +11.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.10252 | val 0.08600 | noisy 0.09005 | Δ +4.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 024 | train 0.10031 | val 0.08631 | noisy 0.09177 | Δ +5.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 025 | train 0.09755 | val 0.07271 | noisy 0.08023 | Δ +9.4% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 1 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.087170 | val 0.070808 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.084547 | val 0.067760 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.082799 | val 0.065733 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.080759 | val 0.064722 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.080863 | val 0.064364 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.080784 | val 0.064321 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.079689 | val 0.064537 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.079669 | val 0.064894 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.079086 | val 0.065257 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.078734 | val 0.065559 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.077891 | val 0.065600 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.079350 | val 0.065597 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.079349 | val 0.065497 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.078520 | val 0.065516 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.080492 | val 0.065449 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.078305 | val 0.065566 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.078848 | val 0.065603 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.079006 | val 0.065571 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.079686 | val 0.065501 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.079565 | val 0.065464 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.079696 | val 0.065446 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.11587 | val 0.09925 | noisy 0.09262 | Δ -7.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.11191 | val 0.09664 | noisy 0.10251 | Δ +5.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.11271 | val 0.09517 | noisy 0.10357 | Δ +8.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.11058 | val 0.09473 | noisy 0.09637 | Δ +1.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.11290 | val 0.08779 | noisy 0.09801 | Δ +10.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.11144 | val 0.09782 | noisy 0.10729 | Δ +8.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.10749 | val 0.09063 | noisy 0.10228 | Δ +11.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.10515 | val 0.10102 | noisy 0.10131 | Δ +0.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.10926 | val 0.08540 | noisy 0.09495 | Δ +10.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.10567 | val 0.09809 | noisy 0.09600 | Δ -2.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.10696 | val 0.09141 | noisy 0.09252 | Δ +1.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.10199 | val 0.09934 | noisy 0.09743 | Δ -2.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.11046 | val 0.08389 | noisy 0.09904 | Δ +15.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.10037 | val 0.10328 | noisy 0.10094 | Δ -2.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.10675 | val 0.08885 | noisy 0.09152 | Δ +2.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.10728 | val 0.08830 | noisy 0.09790 | Δ +9.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.11138 | val 0.09202 | noisy 0.09556 | Δ +3.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.10045 | val 0.08718 | noisy 0.10294 | Δ +15.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.10798 | val 0.09256 | noisy 0.09298 | Δ +0.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.11042 | val 0.09543 | noisy 0.09650 | Δ +1.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.10663 | val 0.08978 | noisy 0.10609 | Δ +15.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.10974 | val 0.08497 | noisy 0.08696 | Δ +2.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.10685 | val 0.09038 | noisy 0.11187 | Δ +19.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.10694 | val 0.07967 | noisy 0.09697 | Δ +17.8% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.087713 | val 0.070771 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.085133 | val 0.067930 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.082761 | val 0.066139 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.080568 | val 0.065081 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.079346 | val 0.064592 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.080142 | val 0.064446 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.079164 | val 0.064543 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.079717 | val 0.064709 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.076980 | val 0.064991 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.080670 | val 0.065196 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.078922 | val 0.065308 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.076773 | val 0.065265 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.079160 | val 0.065215 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.078980 | val 0.065151 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.079921 | val 0.065158 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.078986 | val 0.065216 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.078452 | val 0.065266 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.078714 | val 0.065419 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.079760 | val 0.065451 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.080518 | val 0.065426 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.079323 | val 0.065389 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.079022 | val 0.065409 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.11569 | val 0.09904 | noisy 0.10116 | Δ +2.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.11399 | val 0.09017 | noisy 0.10764 | Δ +16.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.10962 | val 0.08820 | noisy 0.10525 | Δ +16.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.10603 | val 0.09283 | noisy 0.09584 | Δ +3.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.10606 | val 0.09731 | noisy 0.09740 | Δ +0.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.10904 | val 0.09412 | noisy 0.09943 | Δ +5.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.10944 | val 0.08766 | noisy 0.10191 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.10794 | val 0.09481 | noisy 0.10251 | Δ +7.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.10246 | val 0.09351 | noisy 0.09544 | Δ +2.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.11184 | val 0.08980 | noisy 0.09198 | Δ +2.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.10467 | val 0.09428 | noisy 0.09986 | Δ +5.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.10589 | val 0.09370 | noisy 0.09978 | Δ +6.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.10985 | val 0.08286 | noisy 0.10007 | Δ +17.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.10919 | val 0.09622 | noisy 0.09921 | Δ +3.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.10557 | val 0.09390 | noisy 0.09474 | Δ +0.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.10512 | val 0.09327 | noisy 0.09720 | Δ +4.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.10857 | val 0.08639 | noisy 0.09679 | Δ +10.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.10546 | val 0.08055 | noisy 0.09308 | Δ +13.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.10424 | val 0.09158 | noisy 0.09794 | Δ +6.5% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.088172 | val 0.071609 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.086280 | val 0.068311 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.083399 | val 0.066143 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.083813 | val 0.065031 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.080966 | val 0.064614 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.080680 | val 0.064412 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.081090 | val 0.064463 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.080075 | val 0.064671 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.079726 | val 0.064915 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.080463 | val 0.065049 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.077896 | val 0.065231 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.079203 | val 0.065361 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.077480 | val 0.065576 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.079511 | val 0.065751 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.079143 | val 0.065700 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.079215 | val 0.065625 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.079049 | val 0.065567 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.077953 | val 0.065387 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.078552 | val 0.065244 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.077486 | val 0.065118 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.079827 | val 0.065081 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.079550 | val 0.065095 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.078853 | val 0.065048 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.11462 | val 0.08889 | noisy 0.09878 | Δ +10.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.11422 | val 0.09826 | noisy 0.10035 | Δ +2.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.11670 | val 0.10125 | noisy 0.10320 | Δ +1.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.11225 | val 0.09285 | noisy 0.08829 | Δ -5.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.10641 | val 0.09322 | noisy 0.09479 | Δ +1.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.10767 | val 0.08914 | noisy 0.10315 | Δ +13.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.10778 | val 0.09227 | noisy 0.10231 | Δ +9.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.10568 | val 0.09557 | noisy 0.10635 | Δ +10.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.10925 | val 0.09195 | noisy 0.09312 | Δ +1.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.11291 | val 0.09308 | noisy 0.09611 | Δ +3.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.10823 | val 0.08486 | noisy 0.10364 | Δ +18.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.10449 | val 0.08945 | noisy 0.10299 | Δ +13.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.10827 | val 0.08573 | noisy 0.09723 | Δ +11.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.10155 | val 0.09565 | noisy 0.10301 | Δ +7.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.10703 | val 0.09837 | noisy 0.10512 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.10723 | val 0.09363 | noisy 0.09194 | Δ -1.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.10539 | val 0.09559 | noisy 0.11187 | Δ +14.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.10568 | val 0.08519 | noisy 0.10282 | Δ +17.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.10510 | val 0.08679 | noisy 0.10011 | Δ +13.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.10528 | val 0.09429 | noisy 0.10898 | Δ +13.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.10208 | val 0.08657 | noisy 0.10904 | Δ +20.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.11441 | val 0.08345 | noisy 0.10420 | Δ +19.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.10886 | val 0.08633 | noisy 0.09998 | Δ +13.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.10757 | val 0.09382 | noisy 0.09649 | Δ +2.8% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.088360 | val 0.070738 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.082756 | val 0.067476 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.080065 | val 0.065680 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.081344 | val 0.064742 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.082727 | val 0.064229 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.080158 | val 0.064144 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.081518 | val 0.064282 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.079656 | val 0.064537 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.081608 | val 0.064784 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.078381 | val 0.065149 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.080476 | val 0.065442 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.078193 | val 0.065800 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.079019 | val 0.065944 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.077901 | val 0.066031 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.079986 | val 0.065959 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.078545 | val 0.065845 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.076962 | val 0.065597 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.079720 | val 0.065450 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.078858 | val 0.065310 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.079161 | val 0.065148 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.077928 | val 0.065083 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.077845 | val 0.065055 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.079984 | val 0.065068 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.079378 | val 0.065065 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.077171 | val 0.065108 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.077547 | val 0.065068 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.078756 | val 0.064962 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.12025 | val 0.09644 | noisy 0.10048 | Δ +4.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.10877 | val 0.09695 | noisy 0.09776 | Δ +0.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.11103 | val 0.08807 | noisy 0.10393 | Δ +15.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.10347 | val 0.09032 | noisy 0.10515 | Δ +14.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.11517 | val 0.09931 | noisy 0.10524 | Δ +5.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.10514 | val 0.08589 | noisy 0.10096 | Δ +14.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.10069 | val 0.08625 | noisy 0.10449 | Δ +17.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.10263 | val 0.08059 | noisy 0.10329 | Δ +22.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.11008 | val 0.08986 | noisy 0.10372 | Δ +13.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.10243 | val 0.09284 | noisy 0.08648 | Δ -7.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.10941 | val 0.08567 | noisy 0.10920 | Δ +21.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.10321 | val 0.08955 | noisy 0.10585 | Δ +15.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.11034 | val 0.09209 | noisy 0.09089 | Δ -1.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.10304 | val 0.09732 | noisy 0.09378 | Δ -3.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.10619 | val 0.08832 | noisy 0.10234 | Δ +13.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.10302 | val 0.09480 | noisy 0.09511 | Δ +0.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.10614 | val 0.08679 | noisy 0.09976 | Δ +13.0% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.088445 | val 0.071556 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.084433 | val 0.068965 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.083288 | val 0.066850 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.081047 | val 0.065598 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.080636 | val 0.064870 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.080544 | val 0.064416 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.080036 | val 0.064290 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.080238 | val 0.064325 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.080549 | val 0.064417 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.077903 | val 0.064644 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.077898 | val 0.064824 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.078036 | val 0.065005 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.079419 | val 0.065185 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.077918 | val 0.065398 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.079468 | val 0.065555 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.079402 | val 0.065617 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.077930 | val 0.065594 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.080095 | val 0.065528 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.078346 | val 0.065586 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.077456 | val 0.065645 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.079003 | val 0.065524 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.079440 | val 0.065457 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.079820 | val 0.065404 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.078130 | val 0.065330 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.079346 | val 0.065299 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.079828 | val 0.065191 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.077205 | val 0.065082 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.078257 | val 0.064962 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.078181 | val 0.064913 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.080133 | val 0.064877 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.078080 | val 0.064856 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.078838 | val 0.064884 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.079118 | val 0.064805 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.080748 | val 0.064798 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.079314 | val 0.064849 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.079171 | val 0.064789 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.080153 | val 0.064880 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.11680 | val 0.08606 | noisy 0.09629 | Δ +10.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.11507 | val 0.09147 | noisy 0.09999 | Δ +8.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.11040 | val 0.09881 | noisy 0.10057 | Δ +1.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.11028 | val 0.09212 | noisy 0.09878 | Δ +6.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.10552 | val 0.09106 | noisy 0.10637 | Δ +14.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.10679 | val 0.09024 | noisy 0.09624 | Δ +6.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.10675 | val 0.08928 | noisy 0.10640 | Δ +16.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.11179 | val 0.09731 | noisy 0.09895 | Δ +1.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.10617 | val 0.08866 | noisy 0.09688 | Δ +8.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.10501 | val 0.09110 | noisy 0.09910 | Δ +8.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.10413 | val 0.09135 | noisy 0.09973 | Δ +8.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.10629 | val 0.09792 | noisy 0.09777 | Δ -0.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.11150 | val 0.08140 | noisy 0.10502 | Δ +22.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.10720 | val 0.08508 | noisy 0.09354 | Δ +9.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.11034 | val 0.08465 | noisy 0.10070 | Δ +15.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.09545 | val 0.08710 | noisy 0.10737 | Δ +18.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.10429 | val 0.08385 | noisy 0.09551 | Δ +12.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.10765 | val 0.08905 | noisy 0.10758 | Δ +17.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.10702 | val 0.08164 | noisy 0.10334 | Δ +21.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.10823 | val 0.09298 | noisy 0.10977 | Δ +15.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.09929 | val 0.08529 | noisy 0.09554 | Δ +10.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.10646 | val 0.09131 | noisy 0.10488 | Δ +12.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.10007 | val 0.08358 | noisy 0.09859 | Δ +15.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.10633 | val 0.09775 | noisy 0.10157 | Δ +3.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.10074 | val 0.08620 | noisy 0.10702 | Δ +19.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.10694 | val 0.08134 | noisy 0.09870 | Δ +17.6% | LR 0.01000\n",
      "\n",
      "Completed 10 runs.\n"
     ]
    }
   ],
   "source": [
    "RUNS = []\n",
    "\n",
    "for L in LAYER_OPTIONS:\n",
    "    for inst in INSTANCE_IDS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Instance {inst} | Layers {L}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        s1 = train_stage1(\n",
    "            X_train, X_val,\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=120, batch_size=32,\n",
    "            lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6,\n",
    "            seed=inst  # <-- ZMIANA\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        s3 = train_stage3(\n",
    "            X_train, X_val,\n",
    "            dec_dagger_params=s1[\"phi\"],\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=60, batch_size=16,\n",
    "            lr_init=0.010, noise_level=EVAL_SIGMA, seed=inst  # <-- już było\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        RUNS.append({\n",
    "            \"instance_id\": inst,\n",
    "            \"n_layers\": L,\n",
    "            \"stage1\": {\n",
    "                \"phi\": s1[\"phi\"],\n",
    "                \"best_val\": s1[\"best_val\"],\n",
    "                \"hist_train\": s1[\"hist_train\"],\n",
    "                \"hist_val\": s1[\"hist_val\"],\n",
    "                \"hist_lr\": s1[\"hist_lr\"],\n",
    "                \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                \"epochs\": s1.get(\"epochs\"),\n",
    "                \"train_seconds\": float(t1 - t0),\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"psi\": s3[\"psi\"],\n",
    "                \"best_val\": s3[\"best_val\"],\n",
    "                \"hist_train\": s3[\"hist_train\"],\n",
    "                \"hist_val\": s3[\"hist_val\"],\n",
    "                \"hist_lr\": s3[\"hist_lr\"],\n",
    "                \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                \"epochs\": s3.get(\"epochs\"),\n",
    "                \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": float(t2 - t1),\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60c58223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1\\4q_2l_2t_1ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1\\4q_2l_2t_1ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1\\4q_2l_2t_1ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1\\4q_2l_2t_1ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1\\4q_2l_2t_1ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3\\4q_2l_2t_3ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3\\4q_2l_2t_3ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3\\4q_2l_2t_3ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3\\4q_2l_2t_3ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3\\4q_2l_2t_3ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "\n",
      "All runs saved and recorded in CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artur\\AppData\\Local\\Temp\\ipykernel_5556\\2169294299.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_new = pd.concat([df_old, row_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 7 — Save artifacts (JSON) and append a paper-ready CSV per run\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams logged (keep in sync with training cells) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.010\n",
    "S3_MAX_EPOCHS    = 60\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v2\"  # bump if you change columns\n",
    "\n",
    "# --- ensure dirs ---\n",
    "def ensure_dir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "ensure_dir(OUT_BASE)\n",
    "subroot = ensure_dir(f\"{OUT_BASE}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "\n",
    "# --- CSV path (versioned) ---\n",
    "CSV_PATH = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "\n",
    "# --- header for the full, paper-friendly table ---\n",
    "CSV_HEADER = [\n",
    "    # id / naming\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    # architecture\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    # noise & window\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    # stage-1 hyperparams + outcomes\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    # stage-3 hyperparams + outcomes\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    # params (JSON)\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    # totals\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    needs_header = True\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                first_line = f.readline().rstrip(\"\\n\")\n",
    "            needs_header = (first_line != \",\".join(header))\n",
    "        except Exception:\n",
    "            needs_header = True\n",
    "    if needs_header:\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(header)\n",
    "\n",
    "ensure_csv(CSV_PATH, CSV_HEADER)\n",
    "\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def save_one_run(run):\n",
    "    inst = int(run[\"instance_id\"])\n",
    "    L    = int(run[\"n_layers\"])\n",
    "    seed = int(run.get(\"seed\", inst))\n",
    "\n",
    "    # standardized filename: 4q_2l_2t_{L}ls_{inst:02d}.json\n",
    "    fname = f\"4q_2l_2t_{L}ls_{inst:02d}.json\"\n",
    "    out_dir = ensure_dir(f\"{subroot}/L{L}\")\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    # pull stage results (robust to missing keys)\n",
    "    s1 = run[\"stage1\"]\n",
    "    s3 = run[\"stage3\"]\n",
    "\n",
    "    # Stage-1 metrics\n",
    "    s1_hist_val = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val = float(s1.get(\"best_val\", np.nan))\n",
    "    s1_final_val = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = int(s1.get(\"best_epoch\", _safe_argmin(s1_hist_val)))\n",
    "    s1_epochs = int(s1.get(\"epochs\", len(s1_hist_val)))\n",
    "    s1_seconds = float(s1.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # Stage-3 metrics\n",
    "    s3_hist_val = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "    s3_best_val = float(s3.get(\"best_val\", np.nan))\n",
    "    s3_final_val = _safe_last(s3_hist_val)\n",
    "    s3_best_epoch = int(s3.get(\"best_epoch\", _safe_argmin(s3_hist_val)))\n",
    "    s3_epochs = int(s3.get(\"epochs\", len(s3_hist_val)))\n",
    "    s3_seconds = float(s3.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # --- compute metrics with FALLBACKS if curves are missing ---\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = (float(np.nanmax(s3_hist_delta)) if (len(s3_hist_delta) and np.isfinite(np.nanmax(s3_hist_delta)))\n",
    "                      else np.nan)\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    # bundle JSON (parameters + training curves)\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"scale_low\":  float(scale_low),\n",
    "            \"scale_high\": float(scale_high),\n",
    "            \"window_size\": int(n_qubits),\n",
    "            \"window_stride\": 1,\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(EVAL_SIGMA),\n",
    "            \"sigma_eval\":  float(EVAL_SIGMA),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(n_qubits),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(n_latent),\n",
    "            \"n_trash\":  int(n_trash),\n",
    "            \"latent_wires\": list(range(n_latent)),\n",
    "            \"trash_wires\":  list(range(n_latent, n_qubits)),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val, \"lr_curve\": s3.get(\"hist_lr\", []),\n",
    "                \"noisy_curve\": s3_hist_noisy, \"delta_curve\": s3_hist_delta,\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # assemble CSV row\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        OUT_BASE,\n",
    "        inst, seed,\n",
    "        int(n_qubits), int(n_latent), int(n_trash), int(L),\n",
    "        f\"{EVAL_SIGMA:.3f}\", f\"{EVAL_SIGMA:.3f}\", 1,\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS),\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert row into CSV\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(CSV_PATH).exists():\n",
    "        df_old = pd.read_csv(CSV_PATH)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(CSV_PATH, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Upserted CSV row  → {CSV_PATH}\")\n",
    "\n",
    "# ---- save all runs from Cell 7 ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved and recorded in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e3561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved per-layer summary → ./runs_halfqae/summary_by_layers_v2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4q_2l_2t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076368</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>197.352818</td>\n",
       "      <td>0.085766</td>\n",
       "      <td>17.277527</td>\n",
       "      <td>9.536423</td>\n",
       "      <td>[0.07750708820456045, 0.17410562440618502, -0....</td>\n",
       "      <td>[-0.02454464751522512, -0.2211014395389548, 0....</td>\n",
       "      <td>223.439499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4q_2l_2t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075915</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>95.651453</td>\n",
       "      <td>0.087738</td>\n",
       "      <td>18.670788</td>\n",
       "      <td>12.677006</td>\n",
       "      <td>[-0.14218156978650803, 0.1706661290116303, 0.0...</td>\n",
       "      <td>[0.05303408779286446, -0.24337812064684775, -0...</td>\n",
       "      <td>164.156461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4q_2l_2t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079775</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>65.547793</td>\n",
       "      <td>0.085850</td>\n",
       "      <td>24.011768</td>\n",
       "      <td>2.960107</td>\n",
       "      <td>[-0.07330867470920703, 0.1914427128787363, 0.0...</td>\n",
       "      <td>[0.09786877742753125, -0.2386412418804441, -0....</td>\n",
       "      <td>135.002950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4q_2l_2t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078009</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>92.336017</td>\n",
       "      <td>0.087774</td>\n",
       "      <td>21.251096</td>\n",
       "      <td>5.946393</td>\n",
       "      <td>[0.02150945549707438, 0.2011181557257183, -0.0...</td>\n",
       "      <td>[0.011222380810189999, -0.2444294464069941, -0...</td>\n",
       "      <td>196.026160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4q_2l_2t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072709</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>79.274294</td>\n",
       "      <td>0.089085</td>\n",
       "      <td>24.201384</td>\n",
       "      <td>9.370492</td>\n",
       "      <td>[-0.008000868481867264, 0.19142555654718177, 0...</td>\n",
       "      <td>[-0.023278744303053985, -0.2436734971976682, 0...</td>\n",
       "      <td>154.628318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4q_2l_2t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079674</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>1129.249917</td>\n",
       "      <td>0.098505</td>\n",
       "      <td>19.206765</td>\n",
       "      <td>17.835994</td>\n",
       "      <td>[-0.02238813112128236, -0.08819760612572718, -...</td>\n",
       "      <td>[0.17330682123680805, -0.21623626205887883, -0...</td>\n",
       "      <td>1400.693673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4q_2l_2t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091578</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>610.852622</td>\n",
       "      <td>0.098801</td>\n",
       "      <td>17.204934</td>\n",
       "      <td>6.495378</td>\n",
       "      <td>[-0.024101648035003767, -0.07206687616643762, ...</td>\n",
       "      <td>[-0.004220184951955679, -0.18321915123133306, ...</td>\n",
       "      <td>853.553256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4q_2l_2t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093821</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>688.826294</td>\n",
       "      <td>0.100995</td>\n",
       "      <td>20.602683</td>\n",
       "      <td>2.767267</td>\n",
       "      <td>[-0.021642670195468363, -0.06786874745338507, ...</td>\n",
       "      <td>[0.13408317625782007, -0.21733531547817916, -0...</td>\n",
       "      <td>931.949331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4q_2l_2t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086787</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>379.077247</td>\n",
       "      <td>0.100496</td>\n",
       "      <td>21.972869</td>\n",
       "      <td>13.001689</td>\n",
       "      <td>[0.004143455473566463, -0.05730605818481233, 0...</td>\n",
       "      <td>[-0.0395953786902304, -0.20296374902229103, 0....</td>\n",
       "      <td>675.825717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4q_2l_2t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081337</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>528.442718</td>\n",
       "      <td>0.101007</td>\n",
       "      <td>22.493377</td>\n",
       "      <td>17.589461</td>\n",
       "      <td>[-0.0530839156231797, -0.07252436798882102, -0...</td>\n",
       "      <td>[0.07043412481566812, -0.2282931148236656, -0....</td>\n",
       "      <td>900.327000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag  dataset_folder  instance_id  rng_seed  \\\n",
       "0  4q_2l_2t_1ls_01.json  inst1_L1  ./runs_halfqae            1         1   \n",
       "1  4q_2l_2t_1ls_02.json  inst2_L1  ./runs_halfqae            2         2   \n",
       "2  4q_2l_2t_1ls_03.json  inst3_L1  ./runs_halfqae            3         3   \n",
       "3  4q_2l_2t_1ls_04.json  inst4_L1  ./runs_halfqae            4         4   \n",
       "4  4q_2l_2t_1ls_05.json  inst5_L1  ./runs_halfqae            5         5   \n",
       "5  4q_2l_2t_3ls_01.json  inst1_L3  ./runs_halfqae            1         1   \n",
       "6  4q_2l_2t_3ls_02.json  inst2_L3  ./runs_halfqae            2         2   \n",
       "7  4q_2l_2t_3ls_03.json  inst3_L3  ./runs_halfqae            3         3   \n",
       "8  4q_2l_2t_3ls_04.json  inst4_L3  ./runs_halfqae            4         4   \n",
       "9  4q_2l_2t_3ls_05.json  inst5_L3  ./runs_halfqae            5         5   \n",
       "\n",
       "   n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  s3_final_val_mse  \\\n",
       "0         4         2        2         1          0.1  ...          0.076368   \n",
       "1         4         2        2         1          0.1  ...          0.075915   \n",
       "2         4         2        2         1          0.1  ...          0.079775   \n",
       "3         4         2        2         1          0.1  ...          0.078009   \n",
       "4         4         2        2         1          0.1  ...          0.072709   \n",
       "5         4         2        2         3          0.1  ...          0.079674   \n",
       "6         4         2        2         3          0.1  ...          0.091578   \n",
       "7         4         2        2         3          0.1  ...          0.093821   \n",
       "8         4         2        2         3          0.1  ...          0.086787   \n",
       "9         4         2        2         3          0.1  ...          0.081337   \n",
       "\n",
       "   s3_best_epoch  s3_epochs  s3_train_seconds  s3_noisy_baseline_mse  \\\n",
       "0             20         31        197.352818               0.085766   \n",
       "1             20         31         95.651453               0.087738   \n",
       "2             10         21         65.547793               0.085850   \n",
       "3             13         24         92.336017               0.087774   \n",
       "4             15         26         79.274294               0.089085   \n",
       "5             13         24       1129.249917               0.098505   \n",
       "6              8         19        610.852622               0.098801   \n",
       "7             13         24        688.826294               0.100995   \n",
       "8              6         17        379.077247               0.100496   \n",
       "9             15         26        528.442718               0.101007   \n",
       "\n",
       "   s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0          17.277527            9.536423   \n",
       "1          18.670788           12.677006   \n",
       "2          24.011768            2.960107   \n",
       "3          21.251096            5.946393   \n",
       "4          24.201384            9.370492   \n",
       "5          19.206765           17.835994   \n",
       "6          17.204934            6.495378   \n",
       "7          20.602683            2.767267   \n",
       "8          21.972869           13.001689   \n",
       "9          22.493377           17.589461   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [0.07750708820456045, 0.17410562440618502, -0....   \n",
       "1  [-0.14218156978650803, 0.1706661290116303, 0.0...   \n",
       "2  [-0.07330867470920703, 0.1914427128787363, 0.0...   \n",
       "3  [0.02150945549707438, 0.2011181557257183, -0.0...   \n",
       "4  [-0.008000868481867264, 0.19142555654718177, 0...   \n",
       "5  [-0.02238813112128236, -0.08819760612572718, -...   \n",
       "6  [-0.024101648035003767, -0.07206687616643762, ...   \n",
       "7  [-0.021642670195468363, -0.06786874745338507, ...   \n",
       "8  [0.004143455473566463, -0.05730605818481233, 0...   \n",
       "9  [-0.0530839156231797, -0.07252436798882102, -0...   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [-0.02454464751522512, -0.2211014395389548, 0....           223.439499  \n",
       "1  [0.05303408779286446, -0.24337812064684775, -0...           164.156461  \n",
       "2  [0.09786877742753125, -0.2386412418804441, -0....           135.002950  \n",
       "3  [0.011222380810189999, -0.2444294464069941, -0...           196.026160  \n",
       "4  [-0.023278744303053985, -0.2436734971976682, 0...           154.628318  \n",
       "5  [0.17330682123680805, -0.21623626205887883, -0...          1400.693673  \n",
       "6  [-0.004220184951955679, -0.18321915123133306, ...           853.553256  \n",
       "7  [0.13408317625782007, -0.21733531547817916, -0...           931.949331  \n",
       "8  [-0.0395953786902304, -0.20296374902229103, 0....           675.825717  \n",
       "9  [0.07043412481566812, -0.2282931148236656, -0....           900.327000  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.087243 ± 0.001268</td>\n",
       "      <td>0.093823 ± 0.001920</td>\n",
       "      <td>0.076555 ± 0.002354</td>\n",
       "      <td>21.082513 ± 2.779592</td>\n",
       "      <td>8.098084 ± 3.337153</td>\n",
       "      <td>0.078836 ± 0.000382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.099961 ± 0.001088</td>\n",
       "      <td>0.100102 ± 0.002438</td>\n",
       "      <td>0.086639 ± 0.005525</td>\n",
       "      <td>20.296125 ± 1.922266</td>\n",
       "      <td>11.537958 ± 6.013004</td>\n",
       "      <td>0.077262 ± 0.000394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.087243 ± 0.001268     0.093823 ± 0.001920   \n",
       "3            5           0.099961 ± 0.001088     0.100102 ± 0.002438   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.076555 ± 0.002354      21.082513 ± 2.779592   \n",
       "3             0.086639 ± 0.005525      20.296125 ± 1.922266   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1               8.098084 ± 3.337153    0.078836 ± 0.000382  \n",
       "3              11.537958 ± 6.013004    0.077262 ± 0.000394  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results table\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(CSV_PATH).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Run Cell 8 first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- NEW: drop duplicate runs; keep the newest copy (with baseline/delta)\n",
    "if \"filename\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "# Typical numeric casts (safe)\n",
    "for col in [\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\n",
    "    \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "    \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\"\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "\n",
    "clean_path = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "Path(OUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved training-only table → {clean_path}\")\n",
    "\n",
    "# A compact per-layer summary (mean±std); guards against all-NaN\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "metrics = [\n",
    "    (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "    (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "    (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "    (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "    (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "    (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "]\n",
    "\n",
    "grp = df.groupby(\"n_layers\", dropna=False)\n",
    "summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "for col, label in metrics:\n",
    "    if col in df.columns and np.isfinite(df[col]).any():\n",
    "        summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "summary_path = f\"{OUT_BASE}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "summary.to_csv(summary_path, index=True)\n",
    "print(f\"Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3555806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
