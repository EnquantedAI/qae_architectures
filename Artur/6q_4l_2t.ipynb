{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0bd5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# We'll run 5 instances (same across all notebooks) and two depths: 1 and 3 layers.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # used in filenames as ..._ls_01.json, ..._ls_02.json, ...\n",
    "LAYER_OPTIONS  = [1, 3]            # train 1-layer first, then 3-layers\n",
    "EVAL_SIGMA     = 0.10              # fixed noise everywhere (train & eval)\n",
    "\n",
    "# where to save artifacts (JSON bundles, instance records, CSV summary)\n",
    "OUT_BASE = \"./runs_halfqae\"        # change if you like; subfolders will be created automatically\n",
    "CSV_PATH = f\"{OUT_BASE}/results_instances.csv\"  # will be appended-to if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcff7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== IMPORTY ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys, json, math, random, time, hashlib\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a96dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== USTAWIENIA OGÓLNE ====\n",
    "np.random.seed(42)\n",
    "\n",
    "# symulator (szybki)\n",
    "BACKEND = \"lightning.qubit\"\n",
    "\n",
    "# Half-QAE\n",
    "n_qubits = 6\n",
    "n_latent = 4\n",
    "n_trash  = n_qubits - n_latent  # 2\n",
    "\n",
    "# dane Mackey-Glass \n",
    "beta=0.25\n",
    "gamma=0.1\n",
    "n=10\n",
    "tau=15\n",
    "dt=1.0\n",
    "T=300\n",
    "\n",
    "# skalowanie\n",
    "margin = 0.2\n",
    "scale_low, scale_high = 0.0+margin, 1.0-margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848a9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== POMOCNICZE ====\n",
    "def scale_values(x, new_min=0.0, new_max=1.0):\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    return new_min + (x - x_min) * (new_max - new_min) / max(1e-12, (x_max - x_min))\n",
    "\n",
    "def mackey_glass(beta=0.2, gamma=0.1, n=10, tau=17, dt=0.1, T=1000):\n",
    "    N = int(T/dt)\n",
    "    delay_steps = int(tau/dt)\n",
    "    x = np.zeros(N+delay_steps)\n",
    "    x[0:delay_steps] = 1.2\n",
    "    for t in range(delay_steps, N+delay_steps-1):\n",
    "        x_tau = x[t-delay_steps]\n",
    "        dxdt = beta * x_tau / (1 + x_tau**n) - gamma * x[t]\n",
    "        x[t+1] = x[t] + dxdt * dt\n",
    "    return x[delay_steps:]\n",
    "\n",
    "def ts_add_noise(X, noise=0.1, low=0.0, high=1.0):\n",
    "    Z = X + np.random.normal(0.0, noise, size=X.shape)\n",
    "    return np.clip(Z, low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ce1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== GENEROWANIE DANYCH ====\n",
    "y_raw = mackey_glass(beta=beta, gamma=gamma, n=n, tau=tau, dt=dt, T=T)\n",
    "y_raw = scale_values(y_raw, new_min=scale_low, new_max=scale_high)\n",
    "y = y_raw[2::3]                               # subsampling\n",
    "X_idx = np.arange(len(y))\n",
    "\n",
    "# okna (sliding window)\n",
    "window_size = n_qubits\n",
    "stride = 1\n",
    "X_windows = np.stack([y[i:i+window_size] for i in range(0, len(y)-window_size+1, stride)])\n",
    "\n",
    "# podziały: train/val/test = 0.6 / 0.2 / 0.2\n",
    "X_temp, X_test = train_test_split(X_windows, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_temp,   test_size=0.25, random_state=42)\n",
    "\n",
    "# czyste zbiory (na Stage 1 i Stage 2 generujemy szum dynamicznie)\n",
    "X_train_clean = X_train.copy()\n",
    "X_val_clean   = X_val.copy()\n",
    "X_test_clean  = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f15e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ANSATZE  ====\n",
    "def encoder_ansatz(params, x, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    # angle encoding\n",
    "    for w, val in zip(W, x[:len(W)]):\n",
    "        qml.RY(val*np.pi, wires=w)\n",
    "    # warstwy rotacji\n",
    "    n_block = len(W)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in range(n_layers):\n",
    "        for j, w in enumerate(W):\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RX(params[idx],     wires=w)\n",
    "            qml.RY(params[idx + 1], wires=w)\n",
    "            qml.RZ(params[idx + 2], wires=w)\n",
    "        for a, b in zip(W, W[1:]):\n",
    "            qml.CNOT(wires=[a, b])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n",
    "\n",
    "def decoder_ansatz(params, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    n_block = len(W)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in range(n_layers):\n",
    "        for a, b in zip(W, W[1:]):\n",
    "            qml.CNOT(wires=[a, b])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n",
    "        for j, w in enumerate(W):\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RZ(params[idx + 2], wires=w)\n",
    "            qml.RY(params[idx + 1], wires=w)\n",
    "            qml.RX(params[idx],     wires=w)\n",
    "\n",
    "def adjoint_decoder_ansatz(params, x, wires=None):\n",
    "    W = list(range(n_qubits)) if wires is None else list(wires)\n",
    "    n_block = len(W)\n",
    "    for w, val in zip(W, x[:n_block]):\n",
    "        qml.RY(val*np.pi, wires=w)\n",
    "    n_layers = len(params) // (n_block * 3)\n",
    "    for layer in reversed(range(n_layers)):\n",
    "        for j in reversed(range(n_block)):\n",
    "            w = W[j]\n",
    "            idx = layer * n_block * 3 + j * 3\n",
    "            qml.RX(-params[idx],     wires=w)\n",
    "            qml.RY(-params[idx + 1], wires=w)\n",
    "            qml.RZ(-params[idx + 2], wires=w)\n",
    "        for j in reversed(range(n_block - 1)):\n",
    "            qml.CNOT(wires=[W[j+1], W[j]])\n",
    "        qml.CNOT(wires=[W[-1], W[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016c03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STAGE 1: swap-test na adjoint_decoder (uczenie „trash->czysto”) ====\n",
    "# inicjalizacje\n",
    "#n_layers = 2\n",
    "#param_shape = n_layers * n_qubits * 3\n",
    "\n",
    "trash2_start = n_qubits               # 4\n",
    "ancilla      = n_qubits + n_trash     # 6\n",
    "num_total_s1 = n_qubits + n_trash + 1 # 7\n",
    "\n",
    "dev_s1 = qml.device(BACKEND, wires=num_total_s1, shots=None)\n",
    "\n",
    "@qml.qnode(dev_s1, interface=\"autograd\")\n",
    "def swap_test_on_adj_decoder(x, params):\n",
    "    main   = list(range(n_qubits))                                # [0,1,2,3]\n",
    "    trash1 = list(range(n_latent, n_latent + n_trash))            # [2,3]\n",
    "    trash2 = list(range(trash2_start, trash2_start + n_trash))    # [4,5]\n",
    "\n",
    "    adjoint_decoder_ansatz(params, x)\n",
    "\n",
    "    for i in trash2:\n",
    "        qml.Hadamard(wires=i)\n",
    "    qml.Hadamard(wires=ancilla)\n",
    "    for i in range(n_trash):\n",
    "        qml.CSWAP(wires=[ancilla, trash1[i], trash2[i]])\n",
    "    qml.Hadamard(wires=ancilla)\n",
    "\n",
    "    return qml.probs(wires=ancilla)\n",
    "\n",
    "def s1_cost(params, X_batch):\n",
    "    acc = pnp.array(0.0)\n",
    "    for x in X_batch:\n",
    "        p0 = swap_test_on_adj_decoder(x, params)[0]\n",
    "        acc = acc + (1.0 - p0)\n",
    "    return acc / len(X_batch)\n",
    "\n",
    "def train_adjoint_decoder(params_init, X_train, n_epochs=80, batch_size=8, lr=0.01, X_val_clean=None, seed=0):  # <-- ZMIANA\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    params = pnp.array(params_init, requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr)\n",
    "    hist = []\n",
    "    hist_val = []\n",
    "    lr_hist = []\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    t0 = time.time()\n",
    "    for ep in range(n_epochs):\n",
    "        idx = rng.permutation(len(X_train))  # <-- ZMIANA\n",
    "        s=0.0; nb=0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            Xb = X_train[idx[i:i+batch_size]]\n",
    "            params, c = opt.step_and_cost(lambda p: s1_cost(p, Xb), params)\n",
    "            s += float(c); nb += 1\n",
    "        train_cost = s/nb\n",
    "        hist.append(train_cost)\n",
    "        lr_hist.append(opt.stepsize)\n",
    "        # --- walidacja ---\n",
    "        if X_val_clean is not None:\n",
    "            val_cost = float(s1_cost(params, X_val_clean))\n",
    "            hist_val.append(val_cost)\n",
    "        else:\n",
    "            val_cost = train_cost\n",
    "            hist_val.append(train_cost)\n",
    "        print(f\"[Stage1] L={len(params_init)//(n_qubits*3)} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {opt.stepsize:.5f}\")\n",
    "        if train_cost < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = train_cost, pnp.array(params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= 10:\n",
    "            break\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    return dict(\n",
    "        phi=best_params if best_params is not None else params,\n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=len(hist),\n",
    "        hist_train=list(map(float, hist)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f996eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STAGE 2: trenujemy ENCODER latent↔latent (noisy vs clean_ref) ====\n",
    "dev_s2 = qml.device(BACKEND, wires=2*n_qubits + 1, shots=None)\n",
    "\n",
    "@qml.qnode(dev_s2, interface=\"autograd\")\n",
    "def swap_test_encoder_latent(x_noisy, x_clean, enc_params, dec_dagger_params):\n",
    "    encoder = list(range(n_qubits))                  # blok encodera\n",
    "    ref     = list(range(n_qubits, 2*n_qubits))      # blok referencyjny\n",
    "    anc     = 2*n_qubits\n",
    "\n",
    "    encoder_ansatz(enc_params, x_noisy, wires=encoder)\n",
    "    adjoint_decoder_ansatz(dec_dagger_params, x_clean, wires=ref)\n",
    "\n",
    "    qml.Hadamard(wires=anc)\n",
    "    for i in range(n_latent):\n",
    "        qml.CSWAP(wires=[anc, encoder[i], ref[i]])   # SWAP tylko latentów\n",
    "    qml.Hadamard(wires=anc)\n",
    "    return qml.probs(wires=anc)\n",
    "\n",
    "def s2_cost(enc_params, X_noisy_b, X_clean_b, dec_dagger_params):\n",
    "    acc = pnp.array(0.0)\n",
    "    for xn, xc in zip(X_noisy_b, X_clean_b):\n",
    "        p0 = swap_test_encoder_latent(xn, xc, enc_params, dec_dagger_params)[0]\n",
    "        acc = acc + (1.0 - p0)\n",
    "    return acc / len(X_noisy_b)\n",
    "\n",
    "# skala szumu czerpana z realnego skalowania danych\n",
    "info = {\"scale_low\": scale_low, \"scale_high\": scale_high}\n",
    "\n",
    "def s2_cost_dataset(enc_params, X_clean_set, dec_params, noise_level):\n",
    "    \"\"\"Metryka testowa S2: generujemy szum o danym poziomie i liczymy koszt.\"\"\"\n",
    "    low, high = info[\"scale_low\"], info[\"scale_high\"]\n",
    "    sigma = noise_level * (high - low)\n",
    "    X_noisy_set = ts_add_noise(X_clean_set, noise=sigma, low=low, high=high)\n",
    "    return float(s2_cost(enc_params, X_noisy_set, X_clean_set, dec_params))\n",
    "\n",
    "def train_encoder_with_sidekick_dyn_noise(enc_params_init, X_clean,\n",
    "                                         dec_dagger_params, noise_level,\n",
    "                                         n_epochs=80, batch_size=8, lr=0.01, seed=0,\n",
    "                                         X_val_clean=None):\n",
    "    params = pnp.array(enc_params_init, requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr)\n",
    "    hist = []\n",
    "    hist_val = []\n",
    "    lr_hist = []\n",
    "    hist_noisy = []\n",
    "    hist_delta = []\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    t0 = time.time()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    low, high = info[\"scale_low\"], info[\"scale_high\"]\n",
    "    sigma = noise_level * (high - low)\n",
    "    for ep in range(n_epochs):\n",
    "        idx = rng.permutation(len(X_clean))\n",
    "        Xc = X_clean[idx]\n",
    "        s = 0.0; nb = 0\n",
    "        for i in range(0, len(Xc), batch_size):\n",
    "            Xcb = Xc[i:i+batch_size]\n",
    "            Xnb = ts_add_noise(Xcb, noise=sigma, low=low, high=high)\n",
    "            params, c = opt.step_and_cost(\n",
    "                lambda p: s2_cost(p, Xnb, Xcb, dec_dagger_params), params\n",
    "            )\n",
    "            s += float(c); nb += 1\n",
    "        train_cost = s/nb\n",
    "        hist.append(train_cost)\n",
    "        lr_hist.append(opt.stepsize)\n",
    "        # --- walidacja ---\n",
    "        if X_val_clean is not None:\n",
    "            val_cost = float(s2_cost_dataset(params, X_val_clean, dec_dagger_params, noise_level))\n",
    "            # --- baseline/noise-only na walidacji ---\n",
    "            X_noisy_val = ts_add_noise(X_val_clean, noise=sigma, low=low, high=high)\n",
    "            mse_noisy = float(s2_cost(params*0, X_noisy_val, X_val_clean, dec_dagger_params))  # params*0 = brak uczenia\n",
    "            hist_noisy.append(mse_noisy)\n",
    "            if mse_noisy > 1e-12:\n",
    "                delta = 100.0 * (1.0 - val_cost / mse_noisy)\n",
    "            else:\n",
    "                delta = 0.0\n",
    "            hist_delta.append(delta)\n",
    "            hist_val.append(val_cost)\n",
    "        else:\n",
    "            val_cost = train_cost\n",
    "            hist_val.append(train_cost)\n",
    "            hist_noisy.append(np.nan)\n",
    "            hist_delta.append(np.nan)\n",
    "        print(f\"[Stage3] L={len(enc_params_init)//(n_qubits*3)} ep {ep:03d} | train {train_cost:.5f} | \"\n",
    "              f\"val {val_cost:.5f} | noisy {mse_noisy:.5f} | Δ {delta:+.1f}% | LR {opt.stepsize:.5f}\")\n",
    "        if train_cost < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = train_cost, pnp.array(params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= 10:\n",
    "            break\n",
    "    \n",
    "    train_seconds = float(time.time() - t0)\n",
    "    return dict(\n",
    "        psi=best_params if best_params is not None else params,\n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=len(hist),\n",
    "        hist_train=list(map(float, hist)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f15c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(X_train, X_val, n_layers=2, instance_id=1, n_epochs=120, batch_size=32, lr_init=0.01, patience=10, lr_patience=8, min_delta=1e-6, seed=0):  # <-- ZMIANA\n",
    "    param_shape = n_layers * n_qubits * 3\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    params_init = rng.uniform(-0.01, 0.01, param_shape)  # <-- ZMIANA\n",
    "    result = train_adjoint_decoder(\n",
    "        params_init, X_train,\n",
    "        n_epochs=n_epochs, batch_size=batch_size, lr=lr_init,\n",
    "        X_val_clean=X_val,\n",
    "        seed=seed  # <-- ZMIANA\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def train_stage3(X_train, X_val, dec_dagger_params, n_layers=2, instance_id=1, n_epochs=120, batch_size=32, lr_init=0.01, noise_level=0.10, seed=0):\n",
    "    param_shape = n_layers * n_qubits * 3\n",
    "    rng = np.random.default_rng(seed)  # <-- ZMIANA\n",
    "    enc_params_init = rng.uniform(-0.01, 0.01, param_shape)  # <-- ZMIANA\n",
    "    result = train_encoder_with_sidekick_dyn_noise(\n",
    "        enc_params_init, X_train,\n",
    "        dec_dagger_params, noise_level,\n",
    "        n_epochs=n_epochs, batch_size=batch_size, lr=lr_init, seed=seed,\n",
    "        X_val_clean=X_val\n",
    "    )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7a6d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.090445 | val 0.076295 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.086394 | val 0.073817 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.084327 | val 0.071489 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.083915 | val 0.069399 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.081279 | val 0.067604 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.082829 | val 0.066001 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.081036 | val 0.064664 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.080696 | val 0.063567 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.079055 | val 0.062713 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.078301 | val 0.062004 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.077745 | val 0.061481 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.078117 | val 0.061125 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.077526 | val 0.060879 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.077915 | val 0.060715 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.078495 | val 0.060575 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.078390 | val 0.060501 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.077481 | val 0.060486 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.078243 | val 0.060504 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.079282 | val 0.060527 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.076843 | val 0.060570 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.077451 | val 0.060597 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.079270 | val 0.060652 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.078461 | val 0.060691 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.078181 | val 0.060735 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.077245 | val 0.060756 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.079254 | val 0.060777 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.078054 | val 0.060833 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.077590 | val 0.060867 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.077687 | val 0.060909 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.077450 | val 0.060952 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.12497 | val 0.11683 | noisy 0.10961 | Δ -6.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.12306 | val 0.10987 | noisy 0.09985 | Δ -10.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.11411 | val 0.09660 | noisy 0.11859 | Δ +18.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.12036 | val 0.10497 | noisy 0.10201 | Δ -2.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.11626 | val 0.10956 | noisy 0.10837 | Δ -1.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.11890 | val 0.11423 | noisy 0.12404 | Δ +7.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.11407 | val 0.09907 | noisy 0.11525 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.12028 | val 0.10048 | noisy 0.11178 | Δ +10.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.12020 | val 0.10190 | noisy 0.10489 | Δ +2.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.11655 | val 0.10307 | noisy 0.11306 | Δ +8.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10931 | val 0.09145 | noisy 0.10185 | Δ +10.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.11657 | val 0.10411 | noisy 0.10889 | Δ +4.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.11881 | val 0.10605 | noisy 0.11610 | Δ +8.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.11440 | val 0.10248 | noisy 0.10551 | Δ +2.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.11913 | val 0.10605 | noisy 0.10308 | Δ -2.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.11723 | val 0.09885 | noisy 0.11456 | Δ +13.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.11240 | val 0.10331 | noisy 0.10092 | Δ -2.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.11497 | val 0.10730 | noisy 0.10514 | Δ -2.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.11808 | val 0.10411 | noisy 0.10742 | Δ +3.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.10906 | val 0.10150 | noisy 0.10596 | Δ +4.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.11918 | val 0.09869 | noisy 0.11144 | Δ +11.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.11260 | val 0.10295 | noisy 0.09979 | Δ -3.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 022 | train 0.11701 | val 0.08861 | noisy 0.11957 | Δ +25.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 023 | train 0.11360 | val 0.10563 | noisy 0.10158 | Δ -4.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 024 | train 0.11930 | val 0.10035 | noisy 0.10719 | Δ +6.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 025 | train 0.11601 | val 0.09880 | noisy 0.10499 | Δ +5.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 026 | train 0.11445 | val 0.10339 | noisy 0.11944 | Δ +13.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 027 | train 0.11662 | val 0.09877 | noisy 0.10819 | Δ +8.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 028 | train 0.11513 | val 0.10312 | noisy 0.09688 | Δ -6.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 029 | train 0.11708 | val 0.10577 | noisy 0.11416 | Δ +7.4% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.089421 | val 0.076077 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.086650 | val 0.073381 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.083524 | val 0.071011 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.083920 | val 0.068896 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.083064 | val 0.067083 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.080832 | val 0.065568 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.081005 | val 0.064288 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.080160 | val 0.063238 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.077884 | val 0.062442 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.079203 | val 0.061807 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.079083 | val 0.061368 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.079195 | val 0.061037 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.079048 | val 0.060803 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.076828 | val 0.060658 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.078216 | val 0.060557 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.079924 | val 0.060510 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.078330 | val 0.060496 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.077686 | val 0.060509 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.078011 | val 0.060533 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.080861 | val 0.060568 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.076720 | val 0.060630 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.078991 | val 0.060692 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.077337 | val 0.060788 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.077472 | val 0.060888 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.078407 | val 0.060929 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.078170 | val 0.061003 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.079386 | val 0.061061 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.078391 | val 0.061098 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.076958 | val 0.061118 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.079070 | val 0.061098 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.078106 | val 0.061124 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.12307 | val 0.09541 | noisy 0.11736 | Δ +18.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11446 | val 0.10667 | noisy 0.10375 | Δ -2.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.11201 | val 0.10150 | noisy 0.11198 | Δ +9.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.12451 | val 0.10975 | noisy 0.10967 | Δ -0.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.11724 | val 0.10572 | noisy 0.10735 | Δ +1.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.11701 | val 0.09636 | noisy 0.10842 | Δ +11.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.11425 | val 0.09914 | noisy 0.11742 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.11421 | val 0.09242 | noisy 0.11166 | Δ +17.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.11471 | val 0.11337 | noisy 0.11834 | Δ +4.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.11395 | val 0.09741 | noisy 0.11152 | Δ +12.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.10425 | val 0.10465 | noisy 0.11064 | Δ +5.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.11607 | val 0.09339 | noisy 0.11362 | Δ +17.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.11220 | val 0.10384 | noisy 0.11221 | Δ +7.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.11276 | val 0.09226 | noisy 0.12003 | Δ +23.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.11440 | val 0.09946 | noisy 0.11623 | Δ +14.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.11320 | val 0.10437 | noisy 0.10973 | Δ +4.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.11604 | val 0.09942 | noisy 0.11553 | Δ +13.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.10709 | val 0.10054 | noisy 0.11822 | Δ +15.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.10763 | val 0.09878 | noisy 0.11919 | Δ +17.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.11277 | val 0.10865 | noisy 0.11213 | Δ +3.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.12243 | val 0.10345 | noisy 0.10644 | Δ +2.8% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.089832 | val 0.076873 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.085427 | val 0.074101 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.086432 | val 0.071563 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.084406 | val 0.069354 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.081431 | val 0.067404 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.080501 | val 0.065797 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.079563 | val 0.064429 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.078967 | val 0.063303 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.079166 | val 0.062465 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.078859 | val 0.061837 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.078690 | val 0.061392 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.076416 | val 0.061076 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.079396 | val 0.060820 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.077996 | val 0.060660 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.077156 | val 0.060562 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.078863 | val 0.060496 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.079388 | val 0.060454 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.079730 | val 0.060449 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.077638 | val 0.060460 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.078107 | val 0.060477 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.077391 | val 0.060511 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.078135 | val 0.060515 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.12592 | val 0.11537 | noisy 0.11201 | Δ -3.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11756 | val 0.10850 | noisy 0.10426 | Δ -4.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.12352 | val 0.10484 | noisy 0.10991 | Δ +4.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.11065 | val 0.10448 | noisy 0.10607 | Δ +1.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.11336 | val 0.09650 | noisy 0.10631 | Δ +9.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.11420 | val 0.09452 | noisy 0.10940 | Δ +13.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.11812 | val 0.09744 | noisy 0.10264 | Δ +5.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.11494 | val 0.09908 | noisy 0.10544 | Δ +6.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.11520 | val 0.09743 | noisy 0.09886 | Δ +1.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.11335 | val 0.11003 | noisy 0.12190 | Δ +9.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.11287 | val 0.10291 | noisy 0.10567 | Δ +2.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.11713 | val 0.09219 | noisy 0.11637 | Δ +20.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.11438 | val 0.09562 | noisy 0.10938 | Δ +12.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.11476 | val 0.09821 | noisy 0.11414 | Δ +14.0% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.086665 | val 0.075683 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.086602 | val 0.073045 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.083762 | val 0.070631 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.082016 | val 0.068526 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.081981 | val 0.066893 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.079639 | val 0.065484 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.080634 | val 0.064258 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.078800 | val 0.063271 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.079369 | val 0.062493 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.077891 | val 0.061875 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.078263 | val 0.061422 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.077587 | val 0.061078 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.077944 | val 0.060841 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.077972 | val 0.060689 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.078245 | val 0.060585 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.079122 | val 0.060520 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.078721 | val 0.060479 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.078114 | val 0.060492 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.078736 | val 0.060495 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.078002 | val 0.060533 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.077443 | val 0.060574 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.078257 | val 0.060648 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.077357 | val 0.060707 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.078783 | val 0.060763 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.077081 | val 0.060836 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.078498 | val 0.060885 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.077279 | val 0.060953 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.079259 | val 0.060987 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.077951 | val 0.061057 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.078953 | val 0.061072 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.076522 | val 0.061102 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.077744 | val 0.061085 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.080202 | val 0.061081 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.079116 | val 0.061099 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.079063 | val 0.061089 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.078913 | val 0.061065 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.077862 | val 0.061050 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.078361 | val 0.061020 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.079825 | val 0.061000 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.078476 | val 0.060994 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.078473 | val 0.061028 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.12523 | val 0.10914 | noisy 0.11606 | Δ +6.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.11356 | val 0.10426 | noisy 0.09607 | Δ -8.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.11961 | val 0.10465 | noisy 0.10518 | Δ +0.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.11949 | val 0.09449 | noisy 0.11674 | Δ +19.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.11879 | val 0.10985 | noisy 0.11854 | Δ +7.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.11321 | val 0.10385 | noisy 0.11667 | Δ +11.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.11380 | val 0.10508 | noisy 0.10317 | Δ -1.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.10762 | val 0.09412 | noisy 0.11382 | Δ +17.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.11432 | val 0.10624 | noisy 0.10670 | Δ +0.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.11127 | val 0.09967 | noisy 0.11307 | Δ +11.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.11937 | val 0.09639 | noisy 0.11233 | Δ +14.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.10610 | val 0.09868 | noisy 0.12225 | Δ +19.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.10801 | val 0.10292 | noisy 0.11494 | Δ +10.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.10851 | val 0.09894 | noisy 0.10270 | Δ +3.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.11638 | val 0.10326 | noisy 0.11678 | Δ +11.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.11618 | val 0.09848 | noisy 0.10660 | Δ +7.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.11772 | val 0.10434 | noisy 0.11710 | Δ +10.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.11823 | val 0.11084 | noisy 0.10879 | Δ -1.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.11735 | val 0.10540 | noisy 0.11197 | Δ +5.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.11657 | val 0.09605 | noisy 0.10926 | Δ +12.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 020 | train 0.11116 | val 0.09346 | noisy 0.11626 | Δ +19.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 021 | train 0.12309 | val 0.09960 | noisy 0.12339 | Δ +19.3% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.088930 | val 0.076014 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.086236 | val 0.073371 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.084023 | val 0.071008 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.082891 | val 0.068983 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.080455 | val 0.067240 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.081921 | val 0.065732 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.079196 | val 0.064570 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.079534 | val 0.063543 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.078661 | val 0.062744 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.078328 | val 0.062110 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.078911 | val 0.061607 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.079209 | val 0.061232 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.078387 | val 0.061005 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.078168 | val 0.060847 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.079779 | val 0.060714 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.076423 | val 0.060653 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.076953 | val 0.060594 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.078651 | val 0.060568 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.077326 | val 0.060568 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.079529 | val 0.060572 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.079180 | val 0.060602 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.077934 | val 0.060641 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.078560 | val 0.060677 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.079544 | val 0.060708 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.078095 | val 0.060786 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.077614 | val 0.060860 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.12169 | val 0.10761 | noisy 0.10616 | Δ -1.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 001 | train 0.12287 | val 0.11092 | noisy 0.10096 | Δ -9.9% | LR 0.01000\n",
      "[Stage3] L=1 ep 002 | train 0.11450 | val 0.09445 | noisy 0.11088 | Δ +14.8% | LR 0.01000\n",
      "[Stage3] L=1 ep 003 | train 0.11907 | val 0.10737 | noisy 0.10508 | Δ -2.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 004 | train 0.11376 | val 0.10271 | noisy 0.10960 | Δ +6.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 005 | train 0.11525 | val 0.08975 | noisy 0.11894 | Δ +24.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 006 | train 0.11683 | val 0.09485 | noisy 0.12195 | Δ +22.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 007 | train 0.12006 | val 0.09383 | noisy 0.10600 | Δ +11.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 008 | train 0.11191 | val 0.10076 | noisy 0.10797 | Δ +6.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 009 | train 0.10898 | val 0.10088 | noisy 0.12051 | Δ +16.3% | LR 0.01000\n",
      "[Stage3] L=1 ep 010 | train 0.11413 | val 0.09305 | noisy 0.12477 | Δ +25.4% | LR 0.01000\n",
      "[Stage3] L=1 ep 011 | train 0.11503 | val 0.09850 | noisy 0.11465 | Δ +14.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 012 | train 0.11480 | val 0.10127 | noisy 0.12002 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=1 ep 013 | train 0.11388 | val 0.09021 | noisy 0.11666 | Δ +22.7% | LR 0.01000\n",
      "[Stage3] L=1 ep 014 | train 0.12088 | val 0.09819 | noisy 0.11186 | Δ +12.2% | LR 0.01000\n",
      "[Stage3] L=1 ep 015 | train 0.11580 | val 0.10225 | noisy 0.10758 | Δ +5.0% | LR 0.01000\n",
      "[Stage3] L=1 ep 016 | train 0.11049 | val 0.10523 | noisy 0.10969 | Δ +4.1% | LR 0.01000\n",
      "[Stage3] L=1 ep 017 | train 0.11852 | val 0.09502 | noisy 0.11524 | Δ +17.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 018 | train 0.11096 | val 0.10717 | noisy 0.10775 | Δ +0.5% | LR 0.01000\n",
      "[Stage3] L=1 ep 019 | train 0.11552 | val 0.10562 | noisy 0.10603 | Δ +0.4% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 1 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.121924 | val 0.104562 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.114993 | val 0.100063 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.111642 | val 0.096283 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.111142 | val 0.093463 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.108286 | val 0.091507 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.110920 | val 0.089962 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.110449 | val 0.088748 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.109858 | val 0.087726 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.107997 | val 0.086886 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.106899 | val 0.086118 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.105646 | val 0.085572 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.106611 | val 0.085247 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.105415 | val 0.085120 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.106083 | val 0.085099 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.106656 | val 0.085020 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.107211 | val 0.084932 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.105627 | val 0.084882 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.107066 | val 0.084814 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.107158 | val 0.084690 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.104468 | val 0.084664 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.105948 | val 0.084586 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.107432 | val 0.084571 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.106616 | val 0.084577 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.106213 | val 0.084602 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.105335 | val 0.084610 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.107453 | val 0.084628 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.105577 | val 0.084721 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.105835 | val 0.084813 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.106213 | val 0.084937 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.104166 | val 0.085043 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.105631 | val 0.085101 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.105858 | val 0.085175 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.106326 | val 0.085145 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.106321 | val 0.085060 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.106322 | val 0.085087 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.105508 | val 0.085068 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.105779 | val 0.085037 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.105226 | val 0.085001 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.106040 | val 0.084956 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.104411 | val 0.084863 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.19134 | val 0.16776 | noisy 0.17292 | Δ +3.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.18087 | val 0.15303 | noisy 0.17440 | Δ +12.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.17749 | val 0.15384 | noisy 0.18390 | Δ +16.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.18175 | val 0.15875 | noisy 0.17902 | Δ +11.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.17496 | val 0.14604 | noisy 0.17094 | Δ +14.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.17339 | val 0.15176 | noisy 0.17499 | Δ +13.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.17844 | val 0.14928 | noisy 0.17810 | Δ +16.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.17780 | val 0.15201 | noisy 0.18134 | Δ +16.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.18141 | val 0.14406 | noisy 0.17931 | Δ +19.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.17335 | val 0.14135 | noisy 0.18055 | Δ +21.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.17077 | val 0.13875 | noisy 0.17753 | Δ +21.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.16569 | val 0.14247 | noisy 0.17168 | Δ +17.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.17680 | val 0.14617 | noisy 0.17908 | Δ +18.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.17190 | val 0.14377 | noisy 0.17705 | Δ +18.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.17656 | val 0.15152 | noisy 0.18101 | Δ +16.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.17244 | val 0.15877 | noisy 0.18381 | Δ +13.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.17472 | val 0.15082 | noisy 0.17659 | Δ +14.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.17237 | val 0.13706 | noisy 0.17204 | Δ +20.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.16911 | val 0.14934 | noisy 0.18213 | Δ +18.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.16841 | val 0.13800 | noisy 0.17849 | Δ +22.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.16549 | val 0.14172 | noisy 0.16364 | Δ +13.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.17428 | val 0.14527 | noisy 0.18082 | Δ +19.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.17092 | val 0.15233 | noisy 0.17110 | Δ +11.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.17344 | val 0.15075 | noisy 0.16310 | Δ +7.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.16950 | val 0.15276 | noisy 0.16991 | Δ +10.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.17000 | val 0.14424 | noisy 0.18359 | Δ +21.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.16534 | val 0.14628 | noisy 0.17594 | Δ +16.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 027 | train 0.17018 | val 0.15228 | noisy 0.19096 | Δ +20.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 028 | train 0.17262 | val 0.13802 | noisy 0.17027 | Δ +18.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 029 | train 0.17096 | val 0.14577 | noisy 0.16654 | Δ +12.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 030 | train 0.16880 | val 0.14455 | noisy 0.17814 | Δ +18.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 031 | train 0.16690 | val 0.15224 | noisy 0.18315 | Δ +16.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 032 | train 0.16810 | val 0.13419 | noisy 0.18022 | Δ +25.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 033 | train 0.17099 | val 0.15593 | noisy 0.17458 | Δ +10.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 034 | train 0.16698 | val 0.14891 | noisy 0.17126 | Δ +13.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 035 | train 0.16810 | val 0.14190 | noisy 0.18388 | Δ +22.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 036 | train 0.17641 | val 0.14141 | noisy 0.18287 | Δ +22.7% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.120561 | val 0.103556 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.114606 | val 0.098729 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.110697 | val 0.095127 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.111652 | val 0.092495 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.110749 | val 0.090612 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.108588 | val 0.089284 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.109470 | val 0.088134 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.109056 | val 0.087173 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.107295 | val 0.086477 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.107567 | val 0.085915 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.108287 | val 0.085575 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.108241 | val 0.085326 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.107190 | val 0.085074 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.104986 | val 0.084943 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.106715 | val 0.084831 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.108381 | val 0.084725 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.106591 | val 0.084673 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.105115 | val 0.084672 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.106552 | val 0.084671 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.109065 | val 0.084671 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.104861 | val 0.084749 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.107836 | val 0.084784 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.104087 | val 0.084890 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.105168 | val 0.085032 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.107938 | val 0.085018 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.104808 | val 0.085108 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.107246 | val 0.085165 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.105957 | val 0.085152 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.104879 | val 0.085138 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.107693 | val 0.085064 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.106385 | val 0.085097 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.104960 | val 0.085096 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.104429 | val 0.085078 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.19054 | val 0.17459 | noisy 0.16880 | Δ -3.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.17970 | val 0.16294 | noisy 0.17571 | Δ +7.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.17129 | val 0.16133 | noisy 0.17720 | Δ +9.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.17966 | val 0.15475 | noisy 0.18296 | Δ +15.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.17870 | val 0.15202 | noisy 0.18938 | Δ +19.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.17290 | val 0.14488 | noisy 0.18056 | Δ +19.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.18154 | val 0.14348 | noisy 0.16950 | Δ +15.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.17441 | val 0.14110 | noisy 0.17701 | Δ +20.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.16916 | val 0.14606 | noisy 0.17724 | Δ +17.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.17343 | val 0.15070 | noisy 0.17217 | Δ +12.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.16180 | val 0.15537 | noisy 0.17731 | Δ +12.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.16876 | val 0.15218 | noisy 0.18134 | Δ +16.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.17123 | val 0.14346 | noisy 0.18248 | Δ +21.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.16763 | val 0.15506 | noisy 0.17407 | Δ +10.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.17075 | val 0.14785 | noisy 0.16363 | Δ +9.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.16562 | val 0.15231 | noisy 0.17976 | Δ +15.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.16355 | val 0.15610 | noisy 0.18552 | Δ +15.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.16785 | val 0.13536 | noisy 0.16555 | Δ +18.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.17384 | val 0.15552 | noisy 0.17902 | Δ +13.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.16657 | val 0.15384 | noisy 0.17958 | Δ +14.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.16667 | val 0.15902 | noisy 0.18068 | Δ +12.0% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.119384 | val 0.103625 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.113223 | val 0.098741 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.114199 | val 0.094903 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.110924 | val 0.092260 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.108293 | val 0.090357 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.107916 | val 0.089044 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.107887 | val 0.087927 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.106999 | val 0.086944 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.108440 | val 0.086162 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.106746 | val 0.085595 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.106923 | val 0.085268 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.103943 | val 0.085103 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.107476 | val 0.084956 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.107155 | val 0.084885 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.105771 | val 0.084849 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.107136 | val 0.084758 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.107511 | val 0.084666 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.108015 | val 0.084574 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.105465 | val 0.084552 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.106558 | val 0.084538 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.104987 | val 0.084546 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.106779 | val 0.084465 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.19215 | val 0.17897 | noisy 0.17328 | Δ -3.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.18420 | val 0.16093 | noisy 0.15917 | Δ -1.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.17787 | val 0.16179 | noisy 0.17106 | Δ +5.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.18162 | val 0.15019 | noisy 0.17419 | Δ +13.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.17323 | val 0.15514 | noisy 0.18185 | Δ +14.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.17623 | val 0.15885 | noisy 0.17672 | Δ +10.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.17304 | val 0.14548 | noisy 0.19327 | Δ +24.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.16997 | val 0.14590 | noisy 0.17942 | Δ +18.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.17848 | val 0.14720 | noisy 0.17812 | Δ +17.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.17550 | val 0.14318 | noisy 0.17502 | Δ +18.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.16836 | val 0.14061 | noisy 0.17493 | Δ +19.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.16582 | val 0.14756 | noisy 0.17513 | Δ +15.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.16616 | val 0.15003 | noisy 0.16645 | Δ +9.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.17594 | val 0.14060 | noisy 0.18422 | Δ +23.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.16635 | val 0.15257 | noisy 0.17491 | Δ +12.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.16815 | val 0.14416 | noisy 0.17611 | Δ +18.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.16568 | val 0.14759 | noisy 0.17484 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.16774 | val 0.14763 | noisy 0.17172 | Δ +14.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.16343 | val 0.15214 | noisy 0.18007 | Δ +15.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.16974 | val 0.14156 | noisy 0.17122 | Δ +17.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.17222 | val 0.14703 | noisy 0.17162 | Δ +14.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.17495 | val 0.15325 | noisy 0.17721 | Δ +13.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.17310 | val 0.15296 | noisy 0.17630 | Δ +13.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.17653 | val 0.14847 | noisy 0.17796 | Δ +16.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.17376 | val 0.14735 | noisy 0.18063 | Δ +18.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.17173 | val 0.14473 | noisy 0.17626 | Δ +17.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.16547 | val 0.14771 | noisy 0.17038 | Δ +13.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 027 | train 0.17543 | val 0.14735 | noisy 0.18471 | Δ +20.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 028 | train 0.17462 | val 0.15331 | noisy 0.18162 | Δ +15.6% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.117748 | val 0.103922 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.116667 | val 0.099033 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.112862 | val 0.095270 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.109538 | val 0.092634 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.109475 | val 0.090971 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.107744 | val 0.089620 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.108291 | val 0.088462 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.106891 | val 0.087537 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.107902 | val 0.086744 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.106935 | val 0.086087 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.106578 | val 0.085623 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.105675 | val 0.085275 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.106743 | val 0.085075 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.106620 | val 0.084976 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.106120 | val 0.084842 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.107132 | val 0.084709 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.107713 | val 0.084595 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.106228 | val 0.084609 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.107432 | val 0.084548 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.106837 | val 0.084606 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.106409 | val 0.084630 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.107028 | val 0.084706 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.18188 | val 0.17370 | noisy 0.17571 | Δ +1.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.18226 | val 0.16426 | noisy 0.17508 | Δ +6.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.17710 | val 0.14921 | noisy 0.17789 | Δ +16.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.17346 | val 0.15194 | noisy 0.17527 | Δ +13.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.17572 | val 0.16150 | noisy 0.17361 | Δ +7.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.17310 | val 0.15702 | noisy 0.18238 | Δ +13.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.17303 | val 0.14709 | noisy 0.17427 | Δ +15.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.16898 | val 0.15459 | noisy 0.17460 | Δ +11.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.17247 | val 0.15345 | noisy 0.17817 | Δ +13.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.16685 | val 0.15227 | noisy 0.17679 | Δ +13.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.17219 | val 0.13567 | noisy 0.16847 | Δ +19.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.16500 | val 0.14582 | noisy 0.17386 | Δ +16.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.16985 | val 0.14016 | noisy 0.17626 | Δ +20.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.16282 | val 0.15132 | noisy 0.17235 | Δ +12.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.16894 | val 0.13865 | noisy 0.16823 | Δ +17.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.16989 | val 0.14254 | noisy 0.18250 | Δ +21.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.17819 | val 0.14961 | noisy 0.17424 | Δ +14.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 017 | train 0.17371 | val 0.15773 | noisy 0.17253 | Δ +8.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 018 | train 0.17109 | val 0.15148 | noisy 0.16886 | Δ +10.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 019 | train 0.17095 | val 0.15207 | noisy 0.18273 | Δ +16.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 020 | train 0.17065 | val 0.14688 | noisy 0.19348 | Δ +24.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 021 | train 0.17557 | val 0.15100 | noisy 0.17310 | Δ +12.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 022 | train 0.16148 | val 0.14798 | noisy 0.17062 | Δ +13.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 023 | train 0.17593 | val 0.15255 | noisy 0.18037 | Δ +15.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 024 | train 0.17304 | val 0.15595 | noisy 0.18924 | Δ +17.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 025 | train 0.16378 | val 0.14721 | noisy 0.18175 | Δ +19.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 026 | train 0.16481 | val 0.14644 | noisy 0.16527 | Δ +11.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 027 | train 0.16744 | val 0.13373 | noisy 0.16971 | Δ +21.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 028 | train 0.17639 | val 0.15270 | noisy 0.19090 | Δ +20.0% | LR 0.01000\n",
      "[Stage3] L=3 ep 029 | train 0.17507 | val 0.15420 | noisy 0.16596 | Δ +7.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 030 | train 0.17064 | val 0.14842 | noisy 0.17531 | Δ +15.3% | LR 0.01000\n",
      "[Stage3] L=3 ep 031 | train 0.17672 | val 0.15444 | noisy 0.17528 | Δ +11.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 032 | train 0.18987 | val 0.14154 | noisy 0.16327 | Δ +13.3% | LR 0.01000\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.119792 | val 0.104153 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.115707 | val 0.099227 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.112654 | val 0.095511 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.110558 | val 0.093042 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.107430 | val 0.091326 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.110574 | val 0.089990 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.106996 | val 0.088956 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.109139 | val 0.087927 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.106928 | val 0.087075 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.106673 | val 0.086399 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.107359 | val 0.085889 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.107845 | val 0.085552 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.107181 | val 0.085447 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.106256 | val 0.085426 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.108203 | val 0.085320 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.104015 | val 0.085257 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.104740 | val 0.085076 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.107608 | val 0.084896 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.105520 | val 0.084757 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.107950 | val 0.084601 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.106381 | val 0.084544 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.106225 | val 0.084559 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.106677 | val 0.084549 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.108372 | val 0.084541 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.105743 | val 0.084651 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.105440 | val 0.084756 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.19406 | val 0.17079 | noisy 0.17281 | Δ +1.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 001 | train 0.18483 | val 0.15324 | noisy 0.17250 | Δ +11.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 002 | train 0.18039 | val 0.16406 | noisy 0.17541 | Δ +6.5% | LR 0.01000\n",
      "[Stage3] L=3 ep 003 | train 0.17249 | val 0.15835 | noisy 0.17815 | Δ +11.1% | LR 0.01000\n",
      "[Stage3] L=3 ep 004 | train 0.18087 | val 0.15328 | noisy 0.16639 | Δ +7.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 005 | train 0.17682 | val 0.15238 | noisy 0.19469 | Δ +21.7% | LR 0.01000\n",
      "[Stage3] L=3 ep 006 | train 0.16628 | val 0.15066 | noisy 0.17607 | Δ +14.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 007 | train 0.17932 | val 0.14722 | noisy 0.17567 | Δ +16.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 008 | train 0.17153 | val 0.15820 | noisy 0.18222 | Δ +13.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 009 | train 0.16725 | val 0.14281 | noisy 0.17673 | Δ +19.2% | LR 0.01000\n",
      "[Stage3] L=3 ep 010 | train 0.17559 | val 0.15091 | noisy 0.17080 | Δ +11.6% | LR 0.01000\n",
      "[Stage3] L=3 ep 011 | train 0.18057 | val 0.14615 | noisy 0.17700 | Δ +17.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 012 | train 0.17308 | val 0.14182 | noisy 0.16767 | Δ +15.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 013 | train 0.16990 | val 0.14933 | noisy 0.19005 | Δ +21.4% | LR 0.01000\n",
      "[Stage3] L=3 ep 014 | train 0.17520 | val 0.14457 | noisy 0.18245 | Δ +20.8% | LR 0.01000\n",
      "[Stage3] L=3 ep 015 | train 0.16993 | val 0.14406 | noisy 0.16534 | Δ +12.9% | LR 0.01000\n",
      "[Stage3] L=3 ep 016 | train 0.17116 | val 0.14426 | noisy 0.17829 | Δ +19.1% | LR 0.01000\n",
      "\n",
      "Completed 10 runs.\n"
     ]
    }
   ],
   "source": [
    "RUNS = []\n",
    "\n",
    "for L in LAYER_OPTIONS:\n",
    "    for inst in INSTANCE_IDS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Instance {inst} | Layers {L}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        s1 = train_stage1(\n",
    "            X_train, X_val,\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=120, batch_size=32,\n",
    "            lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6,\n",
    "            seed=inst  # <-- ZMIANA\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        s3 = train_stage3(\n",
    "            X_train, X_val,\n",
    "            dec_dagger_params=s1[\"phi\"],\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=60, batch_size=16,\n",
    "            lr_init=0.010, noise_level=EVAL_SIGMA, seed=inst  # <-- już było\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        RUNS.append({\n",
    "            \"instance_id\": inst,\n",
    "            \"n_layers\": L,\n",
    "            \"stage1\": {\n",
    "                \"phi\": s1[\"phi\"],\n",
    "                \"best_val\": s1[\"best_val\"],\n",
    "                \"hist_train\": s1[\"hist_train\"],\n",
    "                \"hist_val\": s1[\"hist_val\"],\n",
    "                \"hist_lr\": s1[\"hist_lr\"],\n",
    "                \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                \"epochs\": s1.get(\"epochs\"),\n",
    "                \"train_seconds\": float(t1 - t0),\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"psi\": s3[\"psi\"],\n",
    "                \"best_val\": s3[\"best_val\"],\n",
    "                \"hist_train\": s3[\"hist_train\"],\n",
    "                \"hist_val\": s3[\"hist_val\"],\n",
    "                \"hist_lr\": s3[\"hist_lr\"],\n",
    "                \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                \"epochs\": s3.get(\"epochs\"),\n",
    "                \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": float(t2 - t1),\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "136e261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle → ./runs_halfqae/q6_l4t2/L1\\6q_4l_2t_1ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l4t2/L1\\6q_4l_2t_1ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l4t2/L1\\6q_4l_2t_1ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l4t2/L1\\6q_4l_2t_1ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l4t2/L1\\6q_4l_2t_1ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l4t2/L3\\6q_4l_2t_3ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l4t2/L3\\6q_4l_2t_3ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l4t2/L3\\6q_4l_2t_3ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l4t2/L3\\6q_4l_2t_3ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae/q6_l4t2/L3\\6q_4l_2t_3ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v5.csv\n",
      "\n",
      "All runs saved and recorded in CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artur\\AppData\\Local\\Temp\\ipykernel_20880\\473950109.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_new = pd.concat([df_old, row_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 7 — Save artifacts (JSON) and append a paper-ready CSV per run\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams logged (keep in sync with training cells) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.010\n",
    "S3_MAX_EPOCHS    = 60\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v5\"  # bump if you change columns\n",
    "\n",
    "# --- ensure dirs ---\n",
    "def ensure_dir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "ensure_dir(OUT_BASE)\n",
    "subroot = ensure_dir(f\"{OUT_BASE}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "\n",
    "# --- CSV path (versioned) ---\n",
    "CSV_PATH = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "\n",
    "# --- header for the full, paper-friendly table ---\n",
    "CSV_HEADER = [\n",
    "    # id / naming\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    # architecture\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    # noise & window\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    # stage-1 hyperparams + outcomes\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    # stage-3 hyperparams + outcomes\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    # params (JSON)\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    # totals\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    needs_header = True\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                first_line = f.readline().rstrip(\"\\n\")\n",
    "            needs_header = (first_line != \",\".join(header))\n",
    "        except Exception:\n",
    "            needs_header = True\n",
    "    if needs_header:\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(header)\n",
    "\n",
    "ensure_csv(CSV_PATH, CSV_HEADER)\n",
    "\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def save_one_run(run):\n",
    "    inst = int(run[\"instance_id\"])\n",
    "    L    = int(run[\"n_layers\"])\n",
    "    seed = int(run.get(\"seed\", inst))\n",
    "\n",
    "    # standardized filename: 6q_4l_2t_{L}ls_{inst:02d}.json\n",
    "    fname = f\"6q_4l_2t_{L}ls_{inst:02d}.json\"\n",
    "    out_dir = ensure_dir(f\"{subroot}/L{L}\")\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    # pull stage results (robust to missing keys)\n",
    "    s1 = run[\"stage1\"]\n",
    "    s3 = run[\"stage3\"]\n",
    "\n",
    "    # Stage-1 metrics\n",
    "    s1_hist_val = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val = float(s1.get(\"best_val\", np.nan))\n",
    "    s1_final_val = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = int(s1.get(\"best_epoch\", _safe_argmin(s1_hist_val)))\n",
    "    s1_epochs = int(s1.get(\"epochs\", len(s1_hist_val)))\n",
    "    s1_seconds = float(s1.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # Stage-3 metrics\n",
    "    s3_hist_val = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "    s3_best_val = float(s3.get(\"best_val\", np.nan))\n",
    "    s3_final_val = _safe_last(s3_hist_val)\n",
    "    s3_best_epoch = int(s3.get(\"best_epoch\", _safe_argmin(s3_hist_val)))\n",
    "    s3_epochs = int(s3.get(\"epochs\", len(s3_hist_val)))\n",
    "    s3_seconds = float(s3.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # --- compute metrics with FALLBACKS if curves are missing ---\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = (float(np.nanmax(s3_hist_delta)) if (len(s3_hist_delta) and np.isfinite(np.nanmax(s3_hist_delta)))\n",
    "                      else np.nan)\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    # bundle JSON (parameters + training curves)\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"scale_low\":  float(scale_low),\n",
    "            \"scale_high\": float(scale_high),\n",
    "            \"window_size\": int(n_qubits),\n",
    "            \"window_stride\": 1,\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(EVAL_SIGMA),\n",
    "            \"sigma_eval\":  float(EVAL_SIGMA),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(n_qubits),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(n_latent),\n",
    "            \"n_trash\":  int(n_trash),\n",
    "            \"latent_wires\": list(range(n_latent)),\n",
    "            \"trash_wires\":  list(range(n_latent, n_qubits)),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val, \"lr_curve\": s3.get(\"hist_lr\", []),\n",
    "                \"noisy_curve\": s3_hist_noisy, \"delta_curve\": s3_hist_delta,\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # assemble CSV row\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        OUT_BASE,\n",
    "        inst, seed,\n",
    "        int(n_qubits), int(n_latent), int(n_trash), int(L),\n",
    "        f\"{EVAL_SIGMA:.3f}\", f\"{EVAL_SIGMA:.3f}\", 1,\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS),\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert row into CSV\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(CSV_PATH).exists():\n",
    "        df_old = pd.read_csv(CSV_PATH)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(CSV_PATH, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Upserted CSV row  → {CSV_PATH}\")\n",
    "\n",
    "# ---- save all runs from Cell 7 ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved and recorded in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af6b78d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae/all_training_instances_v5.csv\n",
      "Saved per-layer summary → ./runs_halfqae/summary_by_layers_v5.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6q_4l_2t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105768</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>300.646887</td>\n",
       "      <td>0.108670</td>\n",
       "      <td>25.890446</td>\n",
       "      <td>7.353200</td>\n",
       "      <td>[0.021424241310524225, 0.1398092740086357, -0....</td>\n",
       "      <td>[-0.0003743123678062272, -0.14194482742414802,...</td>\n",
       "      <td>416.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6q_4l_2t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103451</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>248.712803</td>\n",
       "      <td>0.112926</td>\n",
       "      <td>23.131543</td>\n",
       "      <td>2.810995</td>\n",
       "      <td>[-0.022829690193182144, 0.13526411899478302, 0...</td>\n",
       "      <td>[0.10685200865270353, -0.17793218958964277, -0...</td>\n",
       "      <td>454.161308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6q_4l_2t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098206</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>197.337891</td>\n",
       "      <td>0.108740</td>\n",
       "      <td>20.780890</td>\n",
       "      <td>13.963739</td>\n",
       "      <td>[-0.040932184844466535, 0.14186585930108347, 0...</td>\n",
       "      <td>[0.048611319402249296, -0.09779375447214318, -...</td>\n",
       "      <td>323.936069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6q_4l_2t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099596</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>275.660331</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>19.613702</td>\n",
       "      <td>19.283901</td>\n",
       "      <td>[-0.0040379877981413315, 0.12978607713940463, ...</td>\n",
       "      <td>[0.0033350771967958675, -0.17088919947235454, ...</td>\n",
       "      <td>588.942944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6q_4l_2t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105622</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>321.542510</td>\n",
       "      <td>0.112115</td>\n",
       "      <td>25.421534</td>\n",
       "      <td>0.382790</td>\n",
       "      <td>[-0.010449802938099495, 0.1525133803247939, 0....</td>\n",
       "      <td>[0.01198760989827228, -0.16140176396271685, -6...</td>\n",
       "      <td>508.024537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6q_4l_2t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141410</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>3483.830889</td>\n",
       "      <td>0.176889</td>\n",
       "      <td>25.541202</td>\n",
       "      <td>22.673592</td>\n",
       "      <td>[0.011084335532771574, 0.013957120213802606, 0...</td>\n",
       "      <td>[-0.0010949442226705344, -0.18071538989962782,...</td>\n",
       "      <td>4625.637101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6q_4l_2t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159023</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>1114.367456</td>\n",
       "      <td>0.177118</td>\n",
       "      <td>21.383193</td>\n",
       "      <td>11.985292</td>\n",
       "      <td>[0.001647692376375872, 0.008682182184787075, -...</td>\n",
       "      <td>[-0.016715956407928695, -0.17940926768393817, ...</td>\n",
       "      <td>2126.169818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6q_4l_2t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153305</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>2511.626946</td>\n",
       "      <td>0.176150</td>\n",
       "      <td>24.730452</td>\n",
       "      <td>15.588397</td>\n",
       "      <td>[-0.01577574414304634, 0.005417223866251867, 0...</td>\n",
       "      <td>[-0.02253091129366146, -0.1892546285923441, 0....</td>\n",
       "      <td>3255.859216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6q_4l_2t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141540</td>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>2910.507771</td>\n",
       "      <td>0.175699</td>\n",
       "      <td>24.083963</td>\n",
       "      <td>13.311890</td>\n",
       "      <td>[-0.007273131246887093, 0.005858683105969604, ...</td>\n",
       "      <td>[-0.024179526283464498, -0.18441982758795705, ...</td>\n",
       "      <td>3605.973515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6q_4l_2t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>./runs_halfqae</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144263</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1138.776224</td>\n",
       "      <td>0.176601</td>\n",
       "      <td>21.733374</td>\n",
       "      <td>19.084915</td>\n",
       "      <td>[-0.0010564796963250026, 0.004949702446021306,...</td>\n",
       "      <td>[-0.038174205406885965, -0.17135642739894544, ...</td>\n",
       "      <td>2175.077263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag  dataset_folder  instance_id  rng_seed  \\\n",
       "0  6q_4l_2t_1ls_01.json  inst1_L1  ./runs_halfqae            1         1   \n",
       "1  6q_4l_2t_1ls_02.json  inst2_L1  ./runs_halfqae            2         2   \n",
       "2  6q_4l_2t_1ls_03.json  inst3_L1  ./runs_halfqae            3         3   \n",
       "3  6q_4l_2t_1ls_04.json  inst4_L1  ./runs_halfqae            4         4   \n",
       "4  6q_4l_2t_1ls_05.json  inst5_L1  ./runs_halfqae            5         5   \n",
       "5  6q_4l_2t_3ls_01.json  inst1_L3  ./runs_halfqae            1         1   \n",
       "6  6q_4l_2t_3ls_02.json  inst2_L3  ./runs_halfqae            2         2   \n",
       "7  6q_4l_2t_3ls_03.json  inst3_L3  ./runs_halfqae            3         3   \n",
       "8  6q_4l_2t_3ls_04.json  inst4_L3  ./runs_halfqae            4         4   \n",
       "9  6q_4l_2t_3ls_05.json  inst5_L3  ./runs_halfqae            5         5   \n",
       "\n",
       "   n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  s3_final_val_mse  \\\n",
       "0         6         4        2         1          0.1  ...          0.105768   \n",
       "1         6         4        2         1          0.1  ...          0.103451   \n",
       "2         6         4        2         1          0.1  ...          0.098206   \n",
       "3         6         4        2         1          0.1  ...          0.099596   \n",
       "4         6         4        2         1          0.1  ...          0.105622   \n",
       "5         6         4        2         3          0.1  ...          0.141410   \n",
       "6         6         4        2         3          0.1  ...          0.159023   \n",
       "7         6         4        2         3          0.1  ...          0.153305   \n",
       "8         6         4        2         3          0.1  ...          0.141540   \n",
       "9         6         4        2         3          0.1  ...          0.144263   \n",
       "\n",
       "   s3_best_epoch  s3_epochs  s3_train_seconds  s3_noisy_baseline_mse  \\\n",
       "0             19         30        300.646887               0.108670   \n",
       "1             10         21        248.712803               0.112926   \n",
       "2              3         14        197.337891               0.108740   \n",
       "3             11         22        275.660331               0.112200   \n",
       "4              9         20        321.542510               0.112115   \n",
       "5             26         37       3483.830889               0.176889   \n",
       "6             10         21       1114.367456               0.177118   \n",
       "7             18         29       2511.626946               0.176150   \n",
       "8             22         33       2910.507771               0.175699   \n",
       "9              6         17       1138.776224               0.176601   \n",
       "\n",
       "   s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0          25.890446            7.353200   \n",
       "1          23.131543            2.810995   \n",
       "2          20.780890           13.963739   \n",
       "3          19.613702           19.283901   \n",
       "4          25.421534            0.382790   \n",
       "5          25.541202           22.673592   \n",
       "6          21.383193           11.985292   \n",
       "7          24.730452           15.588397   \n",
       "8          24.083963           13.311890   \n",
       "9          21.733374           19.084915   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [0.021424241310524225, 0.1398092740086357, -0....   \n",
       "1  [-0.022829690193182144, 0.13526411899478302, 0...   \n",
       "2  [-0.040932184844466535, 0.14186585930108347, 0...   \n",
       "3  [-0.0040379877981413315, 0.12978607713940463, ...   \n",
       "4  [-0.010449802938099495, 0.1525133803247939, 0....   \n",
       "5  [0.011084335532771574, 0.013957120213802606, 0...   \n",
       "6  [0.001647692376375872, 0.008682182184787075, -...   \n",
       "7  [-0.01577574414304634, 0.005417223866251867, 0...   \n",
       "8  [-0.007273131246887093, 0.005858683105969604, ...   \n",
       "9  [-0.0010564796963250026, 0.004949702446021306,...   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [-0.0003743123678062272, -0.14194482742414802,...           416.581082  \n",
       "1  [0.10685200865270353, -0.17793218958964277, -0...           454.161308  \n",
       "2  [0.048611319402249296, -0.09779375447214318, -...           323.936069  \n",
       "3  [0.0033350771967958675, -0.17088919947235454, ...           588.942944  \n",
       "4  [0.01198760989827228, -0.16140176396271685, -6...           508.024537  \n",
       "5  [-0.0010949442226705344, -0.18071538989962782,...          4625.637101  \n",
       "6  [-0.016715956407928695, -0.17940926768393817, ...          2126.169818  \n",
       "7  [-0.02253091129366146, -0.1892546285923441, 0....          3255.859216  \n",
       "8  [-0.024179526283464498, -0.18441982758795705, ...          3605.973515  \n",
       "9  [-0.038174205406885965, -0.17135642739894544, ...          2175.077263  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.110930 ± 0.001839</td>\n",
       "      <td>0.107809 ± 0.002307</td>\n",
       "      <td>0.102529 ± 0.003105</td>\n",
       "      <td>22.967623 ± 2.474754</td>\n",
       "      <td>8.758925 ± 7.005409</td>\n",
       "      <td>0.076585 ± 0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.176492 ± 0.000511</td>\n",
       "      <td>0.163667 ± 0.001896</td>\n",
       "      <td>0.147908 ± 0.007053</td>\n",
       "      <td>23.494437 ± 1.650649</td>\n",
       "      <td>16.528817 ± 3.903038</td>\n",
       "      <td>0.104377 ± 0.000653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.110930 ± 0.001839     0.107809 ± 0.002307   \n",
       "3            5           0.176492 ± 0.000511     0.163667 ± 0.001896   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.102529 ± 0.003105      22.967623 ± 2.474754   \n",
       "3             0.147908 ± 0.007053      23.494437 ± 1.650649   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1               8.758925 ± 7.005409    0.076585 ± 0.000170  \n",
       "3              16.528817 ± 3.903038    0.104377 ± 0.000653  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results table\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(CSV_PATH).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Run Cell 8 first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- NEW: drop duplicate runs; keep the newest copy (with baseline/delta)\n",
    "if \"filename\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "# Typical numeric casts (safe)\n",
    "for col in [\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\n",
    "    \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "    \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\"\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "\n",
    "clean_path = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "Path(OUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved training-only table → {clean_path}\")\n",
    "\n",
    "# A compact per-layer summary (mean±std); guards against all-NaN\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "metrics = [\n",
    "    (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "    (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "    (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "    (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "    (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "    (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "]\n",
    "\n",
    "grp = df.groupby(\"n_layers\", dropna=False)\n",
    "summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "for col, label in metrics:\n",
    "    if col in df.columns and np.isfinite(df[col]).any():\n",
    "        summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "summary_path = f\"{OUT_BASE}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "summary.to_csv(summary_path, index=True)\n",
    "print(f\"Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922f78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
