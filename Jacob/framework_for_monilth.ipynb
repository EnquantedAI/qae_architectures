{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MONOLITH QAE BENCH — Quickstart\n",
    "\n",
    "What you need to provide:\n",
    "-------------------------------------\n",
    "1) CIRCUIT TEMPLATE (Cell 3)\n",
    "   - Implement your full QAE as two callables inside the class:\n",
    "       self._encoder_block(params_per_layer)\n",
    "       self._decoder_block(params_per_layer)\n",
    "     Each should apply RX/RY/RZ + entanglers (or your custom brick).\n",
    "   - Choose how to use your single parameter array:\n",
    "       decoder_mode = \"adjoint_encoder\"  -> decoder = adjoint(encoder)\n",
    "       decoder_mode = \"mirror_same\"      -> decoder uses the SAME angles, forward\n",
    "       decoder_mode = \"separate_split\"   -> split array into [encoder | decoder]\n",
    "     (Set this in Cell 4 via JSON or keep default; also auto-inferred from length.)\n",
    "\n",
    "2) MODEL PATHS (Cell 7)\n",
    "   - Put your .json files into MODEL_PATHS = {\"Tag\": \"/path/to/model.json\", ...}\n",
    "   - Standard filename pattern is supported (auto meta):\n",
    "       4q_2l_2t_3ls_01.json  -> nq=4, latent=2, trash=2, layers=3, instance=01\n",
    "     Otherwise we look for \"architecture\" in your JSON.\n",
    "\n",
    "3) Run cells 1 → 7.\n",
    "\n",
    "Standardized evaluation:\n",
    "------------------------\n",
    "• Deterministic Mackey–Glass datasets (MG_A, MG_B), window sizes {4,6}.\n",
    "• Fixed noisy variants at σ=0.10 (change EVAL_SIGMA if you must).\n",
    "• Metrics: ΔMSE% per-window mean + 95% CI, sign test, success rate.\n",
    "• Visuals: recon examples, robustness (kept but σ list defaults to 0.10),\n",
    "  full-series, panelized views, “error lens”, and heatmaps (if many models).\n",
    "\n",
    "Notes:\n",
    "------\n",
    "• If your JSON has a single flat array, we infer n_layers from len(params)/(n_qubits*3).\n",
    "  If the length equals 2*n_layers*n_qubits*3 we switch to \"separate_split\".\n",
    "• Latent/trash: set n_latent and trash_wires (defaults to [latent..nq-1]).\n",
    "• Deterministic seeds → identical fixed noisy sets across runs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674c6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, math, hashlib, warnings\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4)\n",
    "\n",
    "# ---------- scaling & angles ----------\n",
    "def values_to_v01(v, low, high):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    return (v - low) / max(1e-12, (high - low))\n",
    "\n",
    "def v01_to_values(v01, low, high):\n",
    "    v01 = np.asarray(v01, dtype=float)\n",
    "    return low + v01 * (high - low)\n",
    "\n",
    "def readoutZ_to_values(z, low, high):\n",
    "    # RY(pi*v01) → z = cos(pi*v01)  ⇒ v01 = arccos(z)/pi\n",
    "    z = np.clip(np.asarray(z, dtype=float), -0.999999, 0.999999)\n",
    "    v01 = np.arccos(z) / np.pi\n",
    "    return v01_to_values(v01, low, high)\n",
    "\n",
    "# ---------- stats ----------\n",
    "def bootstrap_ci_mean(x, B=3000, alpha=0.05, rng=None):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0: return (np.nan, np.nan, np.nan)\n",
    "    rng = np.random.default_rng(None if rng is None else rng)\n",
    "    n = x.size\n",
    "    xb = np.empty(B, float)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        xb[b] = np.mean(x[idx])\n",
    "    xb.sort()\n",
    "    lo = xb[int((alpha/2)*B)]\n",
    "    hi = xb[int((1 - alpha/2)*B) - 1]\n",
    "    return float(np.mean(x)), float(lo), float(hi)\n",
    "\n",
    "def sign_test_pvalue(diffs):\n",
    "    \"\"\"Two-sided exact sign test on paired diffs (>0 counts as win).\"\"\"\n",
    "    diffs = np.asarray(diffs, dtype=float)\n",
    "    wins  = int(np.sum(diffs > 0))\n",
    "    losses= int(np.sum(diffs < 0))\n",
    "    n     = wins + losses\n",
    "    if n == 0: return 1.0\n",
    "    tail = sum(math.comb(n, k) for k in range(0, min(wins, losses)+1)) / (2**n)\n",
    "    return float(min(1.0, 2*tail))\n",
    "\n",
    "def symmetric_delta_pct(noisy_mse, deno_mse, floor=1e-12):\n",
    "    \"\"\"2*(noisy - deno) / max(noisy + deno, floor) → symmetric, bounded.\"\"\"\n",
    "    n = float(noisy_mse); d = float(deno_mse)\n",
    "    denom = max(n + d, floor)\n",
    "    return 200.0 * (n - d) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20887671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready for window sizes: [4, 6]\n"
     ]
    }
   ],
   "source": [
    "# Global evaluation sigma (kept everywhere)\n",
    "EVAL_SIGMA = 0.10\n",
    "NOISE_GRID = (EVAL_SIGMA,)  # build datasets only for this sigma\n",
    "\n",
    "def mackey_glass(length=1300, tau=17, beta=0.2, gamma=0.1, n=10, x0=1.2, dt=1.0):\n",
    "    x = np.zeros(length + tau + 1, dtype=float)\n",
    "    x[:tau+1] = x0\n",
    "    for t in range(tau, length + tau):\n",
    "        xt = x[t]; xt_tau = x[t - tau]\n",
    "        dx = beta * xt_tau / (1.0 + xt_tau**n) - gamma * xt\n",
    "        x[t+1] = xt + dx * dt\n",
    "    return x[tau+1:]\n",
    "\n",
    "def scale_to_range(y, low=0.2, high=0.8):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    ymin, ymax = float(y.min()), float(y.max())\n",
    "    if ymax - ymin < 1e-12:\n",
    "        return np.full_like(y, (low+high)/2), (low, high)\n",
    "    z = (y - ymin) / (ymax - ymin)\n",
    "    return low + z * (high - low), (low, high)\n",
    "\n",
    "def make_windows(ts, size, step):\n",
    "    ts = np.asarray(ts, dtype=float)\n",
    "    return np.array([ts[i:i+size] for i in range(0, len(ts)-size+1, step)], dtype=float)\n",
    "\n",
    "def _stable_seed(tag: str) -> int:\n",
    "    h = hashlib.sha256(tag.encode(\"utf-8\")).digest()\n",
    "    return int.from_bytes(h[:8], \"little\") & 0x7FFFFFFF\n",
    "\n",
    "def add_gaussian_noise_series(series, sigma, low, high, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    noise = rng.normal(0.0, sigma * (high - low), size=series.shape)\n",
    "    x = series + noise\n",
    "    return np.clip(x, low, high)\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    name: str\n",
    "    clean: np.ndarray\n",
    "    scale_low: float\n",
    "    scale_high: float\n",
    "    windows_clean: np.ndarray\n",
    "    split_idx: int\n",
    "    noisy_series_by_sigma: dict\n",
    "    noisy_windows_by_sigma: dict\n",
    "\n",
    "def build_standard_datasets(window_sizes=(4,6), window_step=1, noise_grid=NOISE_GRID):\n",
    "    mgA = mackey_glass(length=1300, tau=17)\n",
    "    mgB = mackey_glass(length=1300, tau=30)\n",
    "\n",
    "    out = {}\n",
    "    for W in window_sizes:\n",
    "        sA, (loA, hiA) = scale_to_range(mgA, 0.2, 0.8)\n",
    "        sB, (loB, hiB) = scale_to_range(mgB, 0.2, 0.8)\n",
    "        winA = make_windows(sA, W, window_step)\n",
    "        winB = make_windows(sB, W, window_step)\n",
    "        splitA = int(0.75 * len(winA))\n",
    "        splitB = int(0.75 * len(winB))\n",
    "\n",
    "        def noisy_maps(name, scaled, low, high):\n",
    "            noisy_series, noisy_windows = {}, {}\n",
    "            for sigma in noise_grid:\n",
    "                seed = _stable_seed(f\"{name}|W={W}|sigma={sigma:.3f}|v1\")\n",
    "                ns = add_gaussian_noise_series(scaled, sigma, low, high, seed=seed)\n",
    "                noisy_series[round(sigma,3)] = ns\n",
    "                noisy_windows[round(sigma,3)] = make_windows(ns, W, window_step)\n",
    "            return noisy_series, noisy_windows\n",
    "\n",
    "        nsA_s, nsA_w = noisy_maps(\"MG_A\", sA, loA, hiA)\n",
    "        nsB_s, nsB_w = noisy_maps(\"MG_B\", sB, loB, hiB)\n",
    "        out.setdefault(W, {})\n",
    "        out[W][\"MG_A\"] = Dataset(\"MG_A\", sA, loA, hiA, winA, splitA, nsA_s, nsA_w)\n",
    "        out[W][\"MG_B\"] = Dataset(\"MG_B\", sB, loB, hiB, winB, splitB, nsB_s, nsB_w)\n",
    "\n",
    "    print(\"Datasets ready for window sizes:\", list(out.keys()))\n",
    "    return out\n",
    "\n",
    "# Build both sizes so nq=4 and nq=6 models are covered\n",
    "WINDOW_STEP = 1\n",
    "DATASETS_BY_W = build_standard_datasets(window_sizes=(4,6), window_step=WINDOW_STEP, noise_grid=NOISE_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e3ef1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonolithFullQAE:\n",
    "    \"\"\"\n",
    "    Monolithic full-QAE with a SINGLE parameter array.\n",
    "    decoder_mode:\n",
    "      - \"adjoint_encoder\"  -> decoder = adjoint(encoder) using same angles\n",
    "      - \"mirror_same\"      -> decoder uses the same brick forward (not adjoint)\n",
    "      - \"separate_split\"   -> params = [encoder | decoder] (equal halves)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_qubits: int,\n",
    "                 n_layers: int,\n",
    "                 n_latent: int,\n",
    "                 trash_wires: list | None,\n",
    "                 scale_low: float,\n",
    "                 scale_high: float,\n",
    "                 params: np.ndarray,       # flat array from JSON\n",
    "                 decoder_mode: str = \"adjoint_encoder\"):\n",
    "        self.n_qubits = int(n_qubits)\n",
    "        self.n_layers = int(n_layers)\n",
    "        self.n_latent = int(n_latent)\n",
    "        self.latent_wires = list(range(self.n_latent))\n",
    "        self.trash_wires = list(trash_wires) if trash_wires is not None else list(range(self.n_latent, self.n_qubits))\n",
    "        self.scale_low  = float(scale_low)\n",
    "        self.scale_high = float(scale_high)\n",
    "        self.decoder_mode = str(decoder_mode)\n",
    "\n",
    "        fp = np.asarray(params, dtype=float).ravel()\n",
    "        self.full_params = pnp.asarray(fp, requires_grad=False)\n",
    "\n",
    "        per_layer = self.n_qubits * 3  # RX, RY, RZ per qubit\n",
    "        if self.decoder_mode == \"separate_split\":\n",
    "            expected = 2 * self.n_layers * per_layer\n",
    "            if fp.size != expected:\n",
    "                raise ValueError(f\"[separate_split] Expect len(params) == 2*L*Q*3 = {expected}, got {fp.size}\")\n",
    "            half = fp.size // 2\n",
    "            self.enc_flat = pnp.asarray(fp[:half], requires_grad=False)\n",
    "            self.dec_flat = pnp.asarray(fp[half:],  requires_grad=False)\n",
    "        else:\n",
    "            need = self.n_layers * per_layer\n",
    "            if fp.size < need:\n",
    "                raise ValueError(f\"Expect len(params) >= L*Q*3 = {need}, got {fp.size}\")\n",
    "            self.enc_flat = pnp.asarray(fp[:need], requires_grad=False)\n",
    "            self.dec_flat = self.enc_flat  # same angles\n",
    "\n",
    "        self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        self._q_forward = None\n",
    "        self._q_latents = None\n",
    "        self._q_trash   = None\n",
    "\n",
    "    # ======== TODO: If your ansatz differs, edit these three helpers ==========\n",
    "    def _layer_rxryrz_ring(self, params_layer):\n",
    "        \"\"\"Default brick: RX/RY/RZ per qubit + ring CNOT entanglers.\"\"\"\n",
    "        P = np.asarray(params_layer, dtype=float).reshape(self.n_qubits, 3)\n",
    "        for q in range(self.n_qubits):\n",
    "            rx, ry, rz = P[q]\n",
    "            qml.RX(rx, wires=q); qml.RY(ry, wires=q); qml.RZ(rz, wires=q)\n",
    "        for q in range(self.n_qubits - 1):\n",
    "            qml.CNOT(wires=[q, q+1])\n",
    "        qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "\n",
    "    def _encoder_block(self, flat_params):\n",
    "        P = pnp.asarray(flat_params).reshape(self.n_layers, self.n_qubits, 3)\n",
    "        for l in range(self.n_layers):\n",
    "            self._layer_rxryrz_ring(P[l])\n",
    "\n",
    "    def _decoder_block(self, flat_params):\n",
    "        if self.decoder_mode == \"adjoint_encoder\":\n",
    "            qml.adjoint(self._encoder_block)(flat_params)\n",
    "        else:\n",
    "            P = pnp.asarray(flat_params).reshape(self.n_layers, self.n_qubits, 3)\n",
    "            for l in range(self.n_layers):\n",
    "                self._layer_rxryrz_ring(P[l])\n",
    "    # ==========================================================================\n",
    "\n",
    "    def _embed_from_values(self, values_window):\n",
    "        v01 = values_to_v01(values_window, self.scale_low, self.scale_high)\n",
    "        thetas = np.pi * v01\n",
    "        for i, th in enumerate(thetas):\n",
    "            qml.RY(float(th), wires=i)\n",
    "\n",
    "    def map_expZ_to_values(self, z_all):\n",
    "        return readoutZ_to_values(z_all, self.scale_low, self.scale_high)\n",
    "\n",
    "    # QNodes -------------------------------------------------------------------\n",
    "    def _build_forward(self):\n",
    "        @qml.qnode(self.dev, diff_method=None)\n",
    "        def qnode(values_window):\n",
    "            self._embed_from_values(values_window)\n",
    "            self._encoder_block(self.enc_flat)\n",
    "            self._decoder_block(self.dec_flat)\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "        return qnode\n",
    "\n",
    "    def _build_latents_after_encoder(self):\n",
    "        @qml.qnode(self.dev, diff_method=None)\n",
    "        def qnode(values_window):\n",
    "            self._embed_from_values(values_window)\n",
    "            self._encoder_block(self.enc_flat)\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in self.latent_wires]\n",
    "        return qnode\n",
    "\n",
    "    def _build_trash_probs_after_encoder(self):\n",
    "        @qml.qnode(self.dev, diff_method=None)\n",
    "        def qnode(values_window):\n",
    "            self._embed_from_values(values_window)\n",
    "            self._encoder_block(self.enc_flat)\n",
    "            return qml.probs(wires=self.trash_wires)\n",
    "        return qnode\n",
    "\n",
    "    # Public inference ---------------------------------------------------------\n",
    "    def forward_expZ(self, values_window):\n",
    "        if self._q_forward is None: self._q_forward = self._build_forward()\n",
    "        return np.asarray(self._q_forward(values_window), dtype=float)\n",
    "\n",
    "    def latents_after_encoder_expZ(self, values_window):\n",
    "        if self._q_latents is None: self._q_latents = self._build_latents_after_encoder()\n",
    "        return np.asarray(self._q_latents(values_window), dtype=float)\n",
    "\n",
    "    def trash_probs_after_encoder(self, values_window):\n",
    "        if self._q_trash is None: self._q_trash = self._build_trash_probs_after_encoder()\n",
    "        return np.asarray(self._q_trash(values_window), dtype=float)\n",
    "\n",
    "    def predict_values(self, values_window):\n",
    "        return self.map_expZ_to_values(self.forward_expZ(values_window))\n",
    "\n",
    "    def describe(self):\n",
    "        print(f\"Monolith QAE — nq={self.n_qubits}, layers={self.n_layers}, \"\n",
    "              f\"latent={self.n_latent}, trash={len(self.trash_wires)} ({self.trash_wires}), \"\n",
    "              f\"decoder_mode={self.decoder_mode}, scale=[{self.scale_low},{self.scale_high}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5975d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monolith loader ready.\n"
     ]
    }
   ],
   "source": [
    "def _sha256_params(obj):\n",
    "    buf = bytearray()\n",
    "    def walk(x):\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            for y in x: walk(y)\n",
    "        elif isinstance(x, np.ndarray):\n",
    "            buf.extend(np.ascontiguousarray(x, dtype=np.float64).tobytes())\n",
    "        elif isinstance(x, (float, int, np.floating, np.integer)):\n",
    "            buf.extend(np.asarray([x], dtype=np.float64).tobytes())\n",
    "        else:\n",
    "            buf.extend(str(x).encode(\"utf-8\"))\n",
    "    walk(obj)\n",
    "    return hashlib.sha256(buf).hexdigest()\n",
    "\n",
    "# Standard filename:  4q_2l_2t_3ls_01.json\n",
    "_STD_RE = re.compile(r\"(?P<nq>\\d+)q_(?P<nl>\\d+)l_(?P<nt>\\d+)t_(?P<L>\\d+)ls_(?P<inst>\\d+)\\.json$\", re.IGNORECASE)\n",
    "\n",
    "def parse_std_filename(path):\n",
    "    m = _STD_RE.search(os.path.basename(path))\n",
    "    if not m: return None\n",
    "    d = {k:int(v) for k,v in m.groupdict().items()}\n",
    "    return {\"n_qubits\": d[\"nq\"], \"n_latent\": d[\"nl\"], \"n_trash\": d[\"nt\"], \"n_layers\": d[\"L\"], \"instance\": d[\"inst\"]}\n",
    "\n",
    "@dataclass\n",
    "class ModelEntry:\n",
    "    name: str\n",
    "    path: str\n",
    "    n_qubits: int\n",
    "    n_latent: int\n",
    "    trash_wires: list\n",
    "    scale_low: float\n",
    "    scale_high: float\n",
    "    noise_sigma_train: float\n",
    "    meta: dict\n",
    "    params: dict\n",
    "    fingerprint: str\n",
    "    impl: object\n",
    "\n",
    "# --------- JSON parser for \"one-array\" monolithic bundles --------------------\n",
    "def parse_model_json_monolith(path, default_decoder_mode=\"adjoint_encoder\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        J = json.load(f)\n",
    "\n",
    "    # Locate a single parameter array\n",
    "    full_params = None\n",
    "    if isinstance(J.get(\"parameters\"), dict):\n",
    "        for k, v in J[\"parameters\"].items():\n",
    "            if isinstance(v, (list, tuple)) and len(v) > 0 and np.isscalar(v[0]):\n",
    "                full_params = np.asarray(v, dtype=float).ravel()\n",
    "                break\n",
    "    if full_params is None:\n",
    "        for key in [\"full_params\",\"theta\",\"weights\",\"params\",\"phi\",\"psi\"]:\n",
    "            if key in J and isinstance(J[key], (list, tuple)):\n",
    "                full_params = np.asarray(J[key], dtype=float).ravel()\n",
    "                break\n",
    "    if full_params is None:\n",
    "        raise ValueError(\"Could not find a single flat parameter array in JSON.\")\n",
    "\n",
    "    arch = J.get(\"architecture\", {})\n",
    "    ds   = J.get(\"dataset\", {})\n",
    "    run  = J.get(\"run\", {})\n",
    "\n",
    "    # filename fallback\n",
    "    fn_meta = parse_std_filename(path) or {}\n",
    "\n",
    "    n_qubits = int(arch.get(\"n_qubits\", ds.get(\"window_size\", fn_meta.get(\"n_qubits\", 4))))\n",
    "    n_latent = int(arch.get(\"n_latent\", fn_meta.get(\"n_latent\", max(1, n_qubits//2))))\n",
    "    n_trash  = int(arch.get(\"n_trash\",  fn_meta.get(\"n_trash\", n_qubits - n_latent)))\n",
    "    trash_wires = arch.get(\"trash_wires\", list(range(n_latent, n_qubits)))\n",
    "    scale_low  = float(ds.get(\"scale_low\", 0.2))\n",
    "    scale_high = float(ds.get(\"scale_high\", 0.8))\n",
    "\n",
    "    # infer n_layers and decoder_mode from length unless explicitly given\n",
    "    per_layer = n_qubits * 3\n",
    "    n_layers = arch.get(\"n_layers\")\n",
    "    decoder_mode = arch.get(\"decoder_mode\", default_decoder_mode)\n",
    "\n",
    "    if n_layers is None:\n",
    "        L1 = len(full_params) / per_layer\n",
    "        L2 = len(full_params) / (2*per_layer)\n",
    "        if abs(L1 - round(L1)) < 1e-9:\n",
    "            n_layers = int(round(L1))\n",
    "            # keep decoder_mode (default or JSON override)\n",
    "        elif abs(L2 - round(L2)) < 1e-9:\n",
    "            n_layers = int(round(L2))\n",
    "            decoder_mode = \"separate_split\"\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot infer n_layers from len(params)={len(full_params)} and n_qubits={n_qubits}.\")\n",
    "    else:\n",
    "        n_layers = int(n_layers)\n",
    "        if len(full_params) == 2 * n_layers * per_layer:\n",
    "            decoder_mode = \"separate_split\"\n",
    "\n",
    "    info = {\n",
    "        \"n_qubits\": n_qubits, \"n_layers\": n_layers,\n",
    "        \"n_latent\": n_latent, \"trash_wires\": list(trash_wires),\n",
    "        \"scale_low\": scale_low, \"scale_high\": scale_high,\n",
    "        \"n_trash\": n_trash,\n",
    "        \"decoder_mode\": decoder_mode,\n",
    "        \"noise_sigma_train\": float(run.get(\"sigma_train\", EVAL_SIGMA)),\n",
    "        \"timestamp\": J.get(\"timestamp\"),\n",
    "        \"source_schema\": \"monolith_one_array\",\n",
    "        **arch\n",
    "    }\n",
    "    return {\"full_params\": full_params, \"info\": info}\n",
    "\n",
    "def instantiate_model_monolith(parsed):\n",
    "    i = parsed[\"info\"]\n",
    "    impl = MonolithFullQAE(\n",
    "        n_qubits=i[\"n_qubits\"], n_layers=i[\"n_layers\"],\n",
    "        n_latent=i[\"n_latent\"], trash_wires=i[\"trash_wires\"],\n",
    "        scale_low=i[\"scale_low\"], scale_high=i[\"scale_high\"],\n",
    "        params=parsed[\"full_params\"],                      \n",
    "        decoder_mode=i.get(\"decoder_mode\",\"adjoint_encoder\"),\n",
    "    )\n",
    "    return impl\n",
    "\n",
    "def load_models_monolith(model_paths: dict):\n",
    "    registry = {}\n",
    "    for name, path in model_paths.items():\n",
    "        try:\n",
    "            P = parse_model_json_monolith(path)\n",
    "            impl = instantiate_model_monolith(P)\n",
    "            fp = _sha256_params([P[\"full_params\"]])\n",
    "            i = P[\"info\"]\n",
    "            entry = ModelEntry(\n",
    "                name=name, path=path, n_qubits=i[\"n_qubits\"], n_latent=i[\"n_latent\"],\n",
    "                trash_wires=list(i[\"trash_wires\"]), scale_low=i[\"scale_low\"], scale_high=i[\"scale_high\"],\n",
    "                noise_sigma_train=float(i[\"noise_sigma_train\"]), meta=i,\n",
    "                params={\"full_params\": P[\"full_params\"]},\n",
    "                fingerprint=fp, impl=impl\n",
    "            )\n",
    "            registry[name] = entry\n",
    "            print(f\"✓ Loaded {name}  (nq={entry.n_qubits}, L={i['n_layers']}, \"\n",
    "                  f\"latent={entry.n_latent}, trash={len(entry.trash_wires)}, \"\n",
    "                  f\"mode={i.get('decoder_mode')}, σ_train={entry.noise_sigma_train:g})\")\n",
    "            impl.describe()\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {name}: {e}\")\n",
    "    return registry\n",
    "\n",
    "print(\"Monolith loader ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2b9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep σ fixed everywhere and test N windows per dataset\n",
    "N_EVAL_WINDOWS = 20\n",
    "\n",
    "# Sanity: datasets-by-window must exist and contain σ=0.10\n",
    "for w, dsdict in DATASETS_BY_W.items():\n",
    "    for dsname, ds in dsdict.items():\n",
    "        assert round(EVAL_SIGMA,3) in ds.noisy_windows_by_sigma, (\n",
    "            f\"{dsname} (w={w}) missing σ={EVAL_SIGMA:.3f}. Rebuild datasets with NOISE_GRID=(0.10,).\"\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    noisy_mse: np.ndarray\n",
    "    model_mse: np.ndarray\n",
    "    delta_pct: np.ndarray\n",
    "    delta_pct_sym: np.ndarray\n",
    "    recon_values: np.ndarray\n",
    "    noisy_values: np.ndarray\n",
    "    clean_values: np.ndarray\n",
    "    lat_clean: np.ndarray|None\n",
    "    lat_noisy: np.ndarray|None\n",
    "    p00_trash: np.ndarray|None\n",
    "\n",
    "def _sigma_key(s: float) -> float:\n",
    "    return float(np.round(float(s), 3))\n",
    "\n",
    "def _pick_sigma(entry, override):\n",
    "    # \"model\" → use entry.noise_sigma_train; else explicit float\n",
    "    return float(entry.noise_sigma_train) if (override == \"model\") else float(override)\n",
    "\n",
    "def eval_model_on_dataset(entry, ds, n_eval=20, sigma_override=EVAL_SIGMA):\n",
    "    \"\"\"Deterministic eval on the dataset's fixed noisy windows.\"\"\"\n",
    "    s = _pick_sigma(entry, sigma_override) if isinstance(sigma_override, str) else float(sigma_override)\n",
    "    s_key = _sigma_key(s)\n",
    "    if s_key not in ds.noisy_windows_by_sigma:\n",
    "        avail = \", \".join([f\"{k:.3f}\" for k in sorted(ds.noisy_windows_by_sigma)])\n",
    "        raise ValueError(f\"{ds.name}: σ={s_key:.3f} not in [{avail}]\")\n",
    "\n",
    "    cleanW = ds.windows_clean[ds.split_idx:]\n",
    "    noisyW = ds.noisy_windows_by_sigma[s_key][ds.split_idx:]\n",
    "    N = min(int(n_eval), len(cleanW))\n",
    "    cleanW = cleanW[:N]; noisyW = noisyW[:N]\n",
    "\n",
    "    impl = entry.impl\n",
    "    recon, nmse, dmse, d_pct, d_sym = [], [], [], [], []\n",
    "    lat_c, lat_n, p00 = [], [], []\n",
    "\n",
    "    for w_clean, w_noisy in zip(cleanW, noisyW):\n",
    "        z = impl.forward_expZ(w_noisy)\n",
    "        y = impl.map_expZ_to_values(z)\n",
    "        recon.append(y)\n",
    "\n",
    "        mse_n = float(np.mean((w_clean - w_noisy)**2))\n",
    "        mse_d = float(np.mean((w_clean - y      )**2))\n",
    "        nmse.append(mse_n); dmse.append(mse_d)\n",
    "        d_pct.append(0.0 if mse_n < 1e-12 else 100.0 * (mse_n - mse_d) / mse_n)\n",
    "        d_sym.append(200.0 * (mse_n - mse_d) / max(mse_n + mse_d, 1e-12))\n",
    "\n",
    "        # Optional hooks (monolith might not implement; safe in try/except)\n",
    "        try:\n",
    "            lc = impl.latents_after_encoder_expZ(w_clean)\n",
    "            ln = impl.latents_after_encoder_expZ(w_noisy)\n",
    "            lat_c.append(lc); lat_n.append(ln)\n",
    "        except Exception:\n",
    "            lat_c = lat_n = None\n",
    "        try:\n",
    "            P = impl.trash_probs_after_encoder(w_noisy)\n",
    "            p00.append(float(P[0]))\n",
    "        except Exception:\n",
    "            p00 = None\n",
    "\n",
    "    return EvalResult(\n",
    "        noisy_mse=np.asarray(nmse), model_mse=np.asarray(dmse),\n",
    "        delta_pct=np.asarray(d_pct), delta_pct_sym=np.asarray(d_sym),\n",
    "        recon_values=np.asarray(recon), noisy_values=noisyW, clean_values=cleanW,\n",
    "        lat_clean=None if lat_c is None else np.asarray(lat_c),\n",
    "        lat_noisy=None if lat_n is None else np.asarray(lat_n),\n",
    "        p00_trash=None if p00 is None else np.asarray(p00)\n",
    "    )\n",
    "\n",
    "def summarize_eval(er: EvalResult):\n",
    "    mean_imp, lo_imp, hi_imp = bootstrap_ci_mean(er.delta_pct)\n",
    "    mean_sym, lo_sym, hi_sym = bootstrap_ci_mean(er.delta_pct_sym)\n",
    "    p_val = sign_test_pvalue(er.noisy_mse - er.model_mse)\n",
    "    succ  = 100.0 * float(np.mean(er.delta_pct > 0))\n",
    "    return {\n",
    "        \"noisy_MSE_mean\": float(np.mean(er.noisy_mse)),\n",
    "        \"model_MSE_mean\": float(np.mean(er.model_mse)),\n",
    "        \"delta_pct_mean\": float(mean_imp),\n",
    "        \"delta_pct_CI95\": [float(lo_imp), float(hi_imp)],\n",
    "        \"delta_pct_sym_mean\": float(mean_sym),\n",
    "        \"delta_pct_sym_CI95\": [float(lo_sym), float(hi_sym)],\n",
    "        \"sign_test_p\": float(p_val),\n",
    "        \"success_rate_pct\": float(succ),\n",
    "        \"n_windows\": int(er.noisy_mse.size)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590e9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------- small-window visuals ----------\n",
    "def plot_reconstruction_example(er: EvalResult, idx=0, title=\"Reconstruction\"):\n",
    "    c, n, d = er.clean_values[idx], er.noisy_values[idx], er.recon_values[idx]\n",
    "    xs = np.arange(len(c))\n",
    "    plt.figure()\n",
    "    plt.plot(xs, c, label=\"clean\"); plt.plot(xs, n, label=\"noisy\"); plt.plot(xs, d, label=\"denoised\")\n",
    "    plt.title(title); plt.xlabel(\"t\"); plt.ylabel(\"value\"); plt.legend(); plt.show()\n",
    "\n",
    "def plot_delta_distributions(er_by_model, title=\"ΔMSE% (per-window)\"):\n",
    "    plt.figure()\n",
    "    for name, er in er_by_model.items():\n",
    "        plt.hist(er.delta_pct, bins=12, alpha=0.5, label=name)\n",
    "    plt.title(title); plt.xlabel(\"improvement %\"); plt.ylabel(\"count\"); plt.legend(); plt.show()\n",
    "\n",
    "# ---------- robustness (σ list allowed but defaults to 0.10 only) ----------\n",
    "def plot_robustness(entry, ds, sigmas=(0.10,), n_eval=N_EVAL_WINDOWS):\n",
    "    xs, means, lows, highs = [], [], [], []\n",
    "    available = set(ds.noisy_windows_by_sigma.keys())\n",
    "    for s in sigmas:\n",
    "        s_key = float(np.round(float(s), 3))\n",
    "        if s_key not in available:\n",
    "            print(f\"[robustness] skip σ={s_key:.3f} (dataset has {sorted(available)})\")\n",
    "            continue\n",
    "        er = eval_model_on_dataset(entry, ds, n_eval=n_eval, sigma_override=float(s))\n",
    "        m, lo, hi = bootstrap_ci_mean(er.delta_pct)\n",
    "        xs.append(float(s)); means.append(m); lows.append(lo); highs.append(hi)\n",
    "    if not xs:\n",
    "        print(\"[robustness] nothing to plot.\"); return\n",
    "    plt.figure(); plt.plot(xs, means, marker=\"o\"); plt.fill_between(xs, lows, highs, alpha=0.2)\n",
    "    plt.xlabel(\"σ\"); plt.ylabel(\"ΔMSE% (mean, 95%CI)\"); plt.title(f\"Robustness on {ds.name} — {entry.name}\"); plt.show()\n",
    "\n",
    "# ---------- tables & heatmaps ----------\n",
    "def summaries_to_df(all_summaries, registry):\n",
    "    rows=[]\n",
    "    for dsname, bymodel in all_summaries.items():\n",
    "        for name, sm in bymodel.items():\n",
    "            e = registry[name]\n",
    "            layers = e.meta.get(\"n_layers\", getattr(e.impl, \"n_layers\", None))\n",
    "            rows.append({\n",
    "                \"dataset\": dsname, \"model\": name,\n",
    "                \"n_qubits\": e.n_qubits, \"n_latent\": e.n_latent, \"n_trash\": len(e.trash_wires),\n",
    "                \"layers\": layers,\n",
    "                \"delta_pct_mean\": sm[\"delta_pct_mean\"],\n",
    "                \"delta_pct_lo\": sm[\"delta_pct_CI95\"][0],\n",
    "                \"delta_pct_hi\": sm[\"delta_pct_CI95\"][1],\n",
    "                \"success_rate_pct\": sm[\"success_rate_pct\"],\n",
    "                \"p\": sm[\"sign_test_p\"],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def heatmap_by_arch(df, dataset, value_col=\"delta_pct_mean\"):\n",
    "    sub = df[df[\"dataset\"]==dataset]\n",
    "    if sub.empty:\n",
    "        print(f\"[heatmap] no rows for {dataset}\"); return\n",
    "    for (nl, nt), group in sub.groupby([\"n_latent\",\"n_trash\"]):\n",
    "        piv = group.pivot_table(index=\"n_qubits\", columns=\"layers\", values=value_col, aggfunc=\"mean\")\n",
    "        plt.figure(figsize=(5.2,3.6)); plt.imshow(piv.values, aspect=\"auto\")\n",
    "        plt.xticks(range(piv.shape[1]), piv.columns); plt.yticks(range(piv.shape[0]), piv.index)\n",
    "        plt.colorbar(label=value_col)\n",
    "        plt.title(f\"{dataset} — mean {value_col} (latent={nl}, trash={nt})\")\n",
    "        plt.xlabel(\"layers\"); plt.ylabel(\"n_qubits\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------- full-series tools ----------\n",
    "def _flatten_avg(windows, step):\n",
    "    windows = np.asarray(windows, dtype=float)\n",
    "    N, W = windows.shape; L = (N-1)*step + W\n",
    "    acc = np.zeros(L); cnt = np.zeros(L)\n",
    "    for i in range(N):\n",
    "        s=i*step; e=s+W; acc[s:e]+=windows[i]; cnt[s:e]+=1\n",
    "    return acc/np.maximum(cnt,1e-12)\n",
    "\n",
    "def reconstruct_full_series(entry, ds, sigma=EVAL_SIGMA, step=1):\n",
    "    s_key = float(np.round(float(sigma), 3))\n",
    "    if s_key not in ds.noisy_windows_by_sigma:\n",
    "        avail = \", \".join([f\"{k:.3f}\" for k in sorted(ds.noisy_windows_by_sigma)])\n",
    "        raise ValueError(f\"{ds.name}: σ={s_key:.3f} not in [{avail}]\")\n",
    "    cleanW = np.asarray(ds.windows_clean, dtype=float)\n",
    "    noisyW = np.asarray(ds.noisy_windows_by_sigma[s_key], dtype=float)\n",
    "    preds  = np.asarray([entry.impl.map_expZ_to_values(entry.impl.forward_expZ(w)) for w in noisyW])\n",
    "    flat_c = _flatten_avg(cleanW, step); flat_n = _flatten_avg(noisyW, step); flat_d = _flatten_avg(preds, step)\n",
    "    mse_n=float(np.mean((flat_c-flat_n)**2)); mse_d=float(np.mean((flat_c-flat_d)**2))\n",
    "    d_pct=0.0 if mse_n<1e-12 else 100.0*(mse_n-mse_d)/mse_n\n",
    "    return {\"clean\":flat_c,\"noisy\":flat_n,\"deno\":flat_d,\"mse_noisy\":mse_n,\"mse_deno\":mse_d,\"delta_pct\":d_pct,\"sigma\":s_key}\n",
    "\n",
    "def plot_full_series(rec, title):\n",
    "    L=len(rec[\"clean\"]); xs=np.arange(L)\n",
    "    plt.figure(figsize=(9.8,4.0))\n",
    "    plt.plot(xs, rec[\"clean\"], label=\"clean\")\n",
    "    plt.plot(xs, rec[\"noisy\"], label=f\"noisy (MSE={rec['mse_noisy']:.5f})\")\n",
    "    plt.plot(xs, rec[\"deno\"],  label=f\"deno (MSE={rec['mse_deno']:.5f}, Δ%={rec['delta_pct']:+.1f})\")\n",
    "    plt.xlabel(\"t\"); plt.ylabel(\"value\"); plt.title(title); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_full_series_panels(entry, ds, sigma=EVAL_SIGMA, panel_len=180, ncols=3, nrows=2, start=0, sharey=True):\n",
    "    rec = reconstruct_full_series(entry, ds, sigma=sigma)\n",
    "    c,n,d = rec[\"clean\"], rec[\"noisy\"], rec[\"deno\"]; L=len(c)\n",
    "    fig,axes = plt.subplots(nrows, ncols, figsize=(4.5*ncols,2.75*nrows), sharey=sharey)\n",
    "    axes=np.ravel(axes)\n",
    "    for k,ax in enumerate(axes):\n",
    "        s = start + k*panel_len; e=min(s+panel_len, L)\n",
    "        if s>=L: ax.axis(\"off\"); continue\n",
    "        xs=np.arange(s,e); cc=c[s:e]; nn=n[s:e]; dd=d[s:e]\n",
    "        mse_n=float(np.mean((cc-nn)**2)); mse_d=float(np.mean((cc-dd)**2))\n",
    "        d_pct=0.0 if mse_n<1e-12 else 100.0*(mse_n-mse_d)/mse_n\n",
    "        ax.plot(xs,cc,label=\"clean\"); ax.plot(xs,nn,label=f\"noisy (MSE={mse_n:.5f})\")\n",
    "        ax.plot(xs,dd,label=f\"deno (MSE={mse_d:.5f}, Δ%={d_pct:+.1f})\")\n",
    "        if k==0: ax.legend(); ax.set_ylabel(\"value\")\n",
    "        ax.set_title(f\"{s}–{e}\"); ax.set_xlabel(\"t\")\n",
    "    fig.suptitle(f\"{entry.name} — {ds.name} (σ={rec['sigma']:.3f}), panels of {panel_len}\", y=1.02)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def error_lens(entry, ds, sigma=EVAL_SIGMA, seg=150, start=0, length=None):\n",
    "    rec = reconstruct_full_series(entry, ds, sigma=sigma)\n",
    "    c,n,d = rec[\"clean\"], rec[\"noisy\"], rec[\"deno\"]; L=len(c)\n",
    "    if length is None: length=L-start\n",
    "    s,e=int(start),int(min(start+length,L))\n",
    "    se_n=(n-c)**2; se_d=(d-c)**2; imp=se_n-se_d\n",
    "    mean_n=float(se_n.mean()); mean_d=float(se_d.mean())\n",
    "    d_pct=100.0*(mean_n-mean_d)/max(mean_n,1e-12); frac=100.0*float(np.mean(imp>0))\n",
    "    print(f\"{ds.name} | {entry.name} | σ={rec['sigma']:.3f}\\n\"\n",
    "          f\"Global MSE: noisy={mean_n:.5f} deno={mean_d:.5f} Δ%={d_pct:+.1f}\\n\"\n",
    "          f\"Samples improved: {frac:.1f}%\")\n",
    "    xs=np.arange(s,e)\n",
    "    plt.figure(figsize=(10,3.1)); plt.plot(xs, se_n[s:e], label=\"(clean-noisy)^2\")\n",
    "    plt.plot(xs, se_d[s:e], label=\"(clean-deno)^2\"); plt.title(f\"Per-sample squared errors — {ds.name} [{s}:{e}]\")\n",
    "    plt.xlabel(\"t\"); plt.ylabel(\"squared error\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,2.5)); imp_seg=imp[s:e]; plt.plot(xs, imp_seg, lw=0.8)\n",
    "    plt.fill_between(xs,0,imp_seg, where=(imp_seg>=0), alpha=0.35)\n",
    "    plt.fill_between(xs,0,imp_seg, where=(imp_seg<0),  alpha=0.25)\n",
    "    plt.axhline(0,color=\"k\",lw=0.8); plt.title(f\"Improvement per sample — {ds.name} [{s}:{e}]\")\n",
    "    plt.xlabel(\"t\"); plt.ylabel(\"ΔSE\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "    bins=list(range(0,L,seg)); contrib=[imp[i:i+seg].sum() for i in bins]; centers=[i+seg/2 for i in bins]\n",
    "    plt.figure(figsize=(10,3.0)); plt.bar(centers, contrib, width=0.8*seg); plt.axhline(0,color=\"k\",lw=0.8)\n",
    "    plt.title(f\"Contribution to total improvement by {seg}-sample segments — {ds.name}\")\n",
    "    plt.xlabel(\"segment center\"); plt.ylabel(\"Σ ΔSE\"); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16abe379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your monolith JSONs here (standard names help later aggregation).\n",
    "MODEL_PATHS = {\n",
    "    # \"Mentor_4q_2l_2t_3ls_01\": \"/abs/or/rel/path/4q_2l_2t_3ls_01.json\",\n",
    "    # \"Mentor_6q_4l_2t_2ls_07\": \"/abs/or/rel/path/6q_4l_2t_2ls_07.json\",\n",
    "\n",
    "    # (optional) keep Jacob’s old bundle for side-by-side comparisons:\n",
    "    # \"Me_run_20250824_nq4_s010\": \"/path/to/Jacob/.../model_bundle.json\",\n",
    "}\n",
    "\n",
    "REG = load_models_monolith(MODEL_PATHS)\n",
    "\n",
    "def run_benchmark(model_registry, n_eval=N_EVAL_WINDOWS, sigma_override=EVAL_SIGMA):\n",
    "    all_results, all_summaries = {}, {}\n",
    "    for name, entry in model_registry.items():\n",
    "        w = entry.n_qubits\n",
    "        if w not in DATASETS_BY_W:\n",
    "            warnings.warn(f\"No dataset for window_size={w}; skipping {name}.\"); continue\n",
    "        for dsname, ds in DATASETS_BY_W[w].items():\n",
    "            er = eval_model_on_dataset(entry, ds, n_eval=n_eval, sigma_override=sigma_override)\n",
    "            sm = summarize_eval(er)\n",
    "            all_results.setdefault(dsname, {})[name]  = er\n",
    "            all_summaries.setdefault(dsname, {})[name]= sm\n",
    "            print(f\"{dsname} | {name:24s} Δ%={sm['delta_pct_mean']:+5.1f} \"\n",
    "                  f\"(CI {sm['delta_pct_CI95'][0]:+.1f},{sm['delta_pct_CI95'][1]:+.1f})  \"\n",
    "                  f\"succ={sm['success_rate_pct']:4.1f}%  p={sm['sign_test_p']:.4f}\")\n",
    "    return all_results, all_summaries\n",
    "\n",
    "ALL_RESULTS, ALL_SUMMARIES = run_benchmark(REG, n_eval=N_EVAL_WINDOWS, sigma_override=EVAL_SIGMA)\n",
    "\n",
    "# Quick table\n",
    "if ALL_SUMMARIES:\n",
    "    rows=[]\n",
    "    for dsname, d in ALL_SUMMARIES.items():\n",
    "        for name, sm in d.items():\n",
    "            e = REG[name]\n",
    "            rows.append({\n",
    "                \"dataset\": dsname, \"model\": name,\n",
    "                \"n_qubits\": e.n_qubits, \"n_latent\": e.n_latent, \"n_trash\": len(e.trash_wires),\n",
    "                \"layers\": e.meta.get(\"n_layers\", getattr(e.impl, \"n_layers\", None)),\n",
    "                \"delta_pct_mean\": sm[\"delta_pct_mean\"],\n",
    "                \"delta_pct_lo\": sm[\"delta_pct_CI95\"][0],\n",
    "                \"delta_pct_hi\": sm[\"delta_pct_CI95\"][1],\n",
    "                \"success_rate_pct\": sm[\"success_rate_pct\"],\n",
    "                \"p\": sm[\"sign_test_p\"],\n",
    "            })\n",
    "    df = pd.DataFrame(rows).sort_values([\"dataset\",\"n_qubits\",\"layers\",\"model\"])\n",
    "    print(\"\\n== Summary (head) ==\"); print(df.head(12).to_string(index=False))\n",
    "\n",
    "# Quick visuals for the first loaded model\n",
    "if REG:\n",
    "    name0, entry0 = next(iter(REG.items()))\n",
    "    nq = entry0.n_qubits\n",
    "    assert nq in DATASETS_BY_W, f\"Missing datasets for window size {nq}\"\n",
    "    dsA = DATASETS_BY_W[nq][\"MG_A\"]; dsB = DATASETS_BY_W[nq][\"MG_B\"]\n",
    "\n",
    "    erA = ALL_RESULTS[\"MG_A\"][name0]; erB = ALL_RESULTS[\"MG_B\"][name0]\n",
    "    plot_reconstruction_example(erA, idx=0, title=f\"{name0} — MG_A (σ={EVAL_SIGMA:g})\")\n",
    "    plot_reconstruction_example(erB, idx=0, title=f\"{name0} — MG_B (σ={EVAL_SIGMA:g})\")\n",
    "\n",
    "    # full-series\n",
    "    recA = reconstruct_full_series(entry0, dsA, sigma=EVAL_SIGMA); plot_full_series(recA, f\"{name0} — MG_A (σ={recA['sigma']:.3f})\")\n",
    "    recB = reconstruct_full_series(entry0, dsB, sigma=EVAL_SIGMA); plot_full_series(recB, f\"{name0} — MG_B (σ={recB['sigma']:.3f})\")\n",
    "\n",
    "    # panels + error lens on MG_B\n",
    "    plot_full_series_panels(entry0, dsB, sigma=EVAL_SIGMA, panel_len=180, ncols=3, nrows=2, start=0)\n",
    "    error_lens(entry0, dsB, sigma=EVAL_SIGMA, seg=150, start=0,   length=900)\n",
    "    error_lens(entry0, dsB, sigma=EVAL_SIGMA, seg=150, start=900, length=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
