{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b83bee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models registered: 0\n",
      "Heads up: no models found. Check ROOTS paths or add MODEL_PATHS manually.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0 — Quick Start (Sidekick / Stage-3) ===============================\n",
    "# Goal: run standardized, deterministic eval across many models (4q & 6q).\n",
    "# This version auto-discovers your 40 JSONs under Jacob/* and names models\n",
    "# by the filename (e.g., \"4q_2l_2t_1ls_01\").\n",
    "\n",
    "import os, glob, json\n",
    "\n",
    "# ---- Where to scan ----------------------------------------------------------\n",
    "# Point this at the *parent* folders that contain your run subfolders.\n",
    "# The patterns below match the structure in your screenshot.\n",
    "JACOB_ROOT = \"./Jacob\"\n",
    "ROOTS = [\n",
    "    os.path.join(JACOB_ROOT, \"runs_full_qae_2L2T\"),\n",
    "    os.path.join(JACOB_ROOT, \"runs_full_qae_2L4T\"),\n",
    "    os.path.join(JACOB_ROOT, \"runs_full_qae_3L1T\"),\n",
    "    os.path.join(JACOB_ROOT, \"runs_full_qae_4L2T\"),\n",
    "]\n",
    "\n",
    "# We’ll scan for any JSON and keep only ones that contain the expected schema.\n",
    "DISCOVER_PATTERN = \"**/*.json\"\n",
    "\n",
    "# Manual mapping (optional). Anything here overrides auto-discovery name collisions.\n",
    "MODEL_PATHS = {\n",
    "    # \"4q_2l_2t_1ls_01\": \"./Jacob/runs_full_qae_2L2T/q4_l2t2/L1/4q_2l_2t_1ls_01.json\",\n",
    "}\n",
    "\n",
    "def _looks_like_sidekick_json(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            J = json.load(f)\n",
    "        P = J.get(\"parameters\", {})\n",
    "        return (\"psi_stage3\" in P) and (\"phi_stage1\" in P)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _discover_models(roots, pattern=DISCOVER_PATTERN):\n",
    "    d = {}\n",
    "    for r in roots:\n",
    "        if not os.path.isdir(r):\n",
    "            continue\n",
    "        for p in glob.glob(os.path.join(r, pattern), recursive=True):\n",
    "            if not p.lower().endswith(\".json\"):\n",
    "                continue\n",
    "            if not _looks_like_sidekick_json(p):\n",
    "                continue\n",
    "            # Use filename stem as the model tag (e.g., \"4q_2l_2t_1ls_01\")\n",
    "            tag = os.path.splitext(os.path.basename(p))[0]\n",
    "            base = tag; k = 1\n",
    "            while tag in d or tag in MODEL_PATHS:\n",
    "                k += 1; tag = f\"{base}_{k}\"\n",
    "            d[tag] = os.path.abspath(p)\n",
    "    return d\n",
    "\n",
    "# Merge discovered with manual\n",
    "auto = _discover_models(ROOTS)\n",
    "for k, v in auto.items():\n",
    "    if k not in MODEL_PATHS:\n",
    "        MODEL_PATHS[k] = v\n",
    "\n",
    "print(f\"Models registered: {len(MODEL_PATHS)}\")\n",
    "if len(MODEL_PATHS) == 0:\n",
    "    print(\"Heads up: no models found. Check ROOTS paths or add MODEL_PATHS manually.\")\n",
    "\n",
    "# ── Eval knobs ───────────────────────────────────────────────────────────────\n",
    "EVAL_SIGMA = 0.1          # or \"model\" to use each model's train sigma from JSON\n",
    "N_EVAL_WINDOWS = 20       # per dataset per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72926a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1 — Imports & utilities ===========================================\n",
    "import os, re, json, math, time, hashlib, warnings, glob\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.8, 4.2)\n",
    "\n",
    "# ---------- scaling & angles ----------\n",
    "def values_to_v01(v, low, high):\n",
    "    v = np.asarray(v, dtype=float); return (v - low) / max(1e-12, (high - low))\n",
    "\n",
    "def v01_to_values(v01, low, high):\n",
    "    v01 = np.asarray(v01, dtype=float); return low + v01 * (high - low)\n",
    "\n",
    "def readoutZ_to_values(z, low, high):\n",
    "    z = np.clip(np.asarray(z, dtype=float), -0.999999, 0.999999)\n",
    "    v01 = np.arccos(z) / np.pi\n",
    "    return v01_to_values(v01, low, high)\n",
    "\n",
    "def values_to_RY_angles(v, low, high):\n",
    "    return np.pi * values_to_v01(v, low, high)\n",
    "\n",
    "# ---------- deterministic noise ----------\n",
    "def _stable_seed(tag: str) -> int:\n",
    "    h = hashlib.sha256(tag.encode(\"utf-8\")).digest()\n",
    "    return int.from_bytes(h[:8], \"little\") & 0x7FFFFFFF\n",
    "\n",
    "def add_gaussian_noise_series(series, sigma, low, high, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    noise = rng.normal(0.0, sigma * (high - low), size=series.shape)\n",
    "    x = np.clip(series + noise, low, high)\n",
    "    return x\n",
    "\n",
    "# ---------- stats ----------\n",
    "def bootstrap_ci_mean(x, B=3000, alpha=0.05, rng=None):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0: return (np.nan, np.nan, np.nan)\n",
    "    rng = np.random.default_rng(None if rng is None else rng)\n",
    "    n = x.size\n",
    "    xb = np.empty(B, float)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        xb[b] = np.mean(x[idx])\n",
    "    xb.sort()\n",
    "    lo = xb[int((alpha/2)*B)]\n",
    "    hi = xb[int((1 - alpha/2)*B) - 1]\n",
    "    return float(np.mean(x)), float(lo), float(hi)\n",
    "\n",
    "def sign_test_pvalue(diffs):\n",
    "    diffs = np.asarray(diffs, dtype=float)\n",
    "    wins  = int(np.sum(diffs > 0))\n",
    "    losses= int(np.sum(diffs < 0))\n",
    "    n     = wins + losses\n",
    "    if n == 0: return 1.0\n",
    "    tail = sum(math.comb(n, k) for k in range(0, min(wins, losses)+1)) / (2**n)\n",
    "    return float(min(1.0, 2*tail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d84fc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready for window sizes: [4, 6]\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2 — Standardized datasets (for window sizes 4 & 6) ================\n",
    "NOISE_GRID = (EVAL_SIGMA,) \n",
    "\n",
    "def mackey_glass(length=1300, tau=17, beta=0.2, gamma=0.1, n=10, x0=1.2):\n",
    "    x = np.zeros(length + tau + 1, dtype=float); x[:tau+1] = x0\n",
    "    for t in range(tau, length + tau):\n",
    "        xt = x[t]; xt_tau = x[t - tau]\n",
    "        dx = beta * xt_tau / (1.0 + xt_tau**n) - gamma * xt\n",
    "        x[t+1] = xt + dx\n",
    "    return x[tau+1:]\n",
    "\n",
    "def scale_to_range(y, low=0.2, high=0.8):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    ymin, ymax = float(y.min()), float(y.max())\n",
    "    if ymax - ymin < 1e-12: return np.full_like(y, (low+high)/2), (low, high)\n",
    "    z = (y - ymin) / (ymax - ymin)\n",
    "    return low + z * (high - low), (low, high)\n",
    "\n",
    "def make_windows(ts, size, step):\n",
    "    ts = np.asarray(ts, dtype=float)\n",
    "    return np.array([ts[i:i+size] for i in range(0, len(ts)-size+1, step)], dtype=float)\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    name: str\n",
    "    clean: np.ndarray\n",
    "    scale_low: float\n",
    "    scale_high: float\n",
    "    windows_clean: np.ndarray\n",
    "    split_idx: int\n",
    "    noisy_series_by_sigma: dict\n",
    "    noisy_windows_by_sigma: dict\n",
    "\n",
    "def _build_for_window_size(window_size=4, window_step=1, noise_grid=NOISE_GRID):\n",
    "    mgA = mackey_glass(length=1300, tau=17)\n",
    "    mgB = mackey_glass(length=1300, tau=30)\n",
    "    sA,(loA,hiA) = scale_to_range(mgA, 0.2, 0.8)\n",
    "    sB,(loB,hiB) = scale_to_range(mgB, 0.2, 0.8)\n",
    "    winA = make_windows(sA, window_size, window_step)\n",
    "    winB = make_windows(sB, window_size, window_step)\n",
    "    splitA = int(0.75 * len(winA)); splitB = int(0.75 * len(winB))\n",
    "\n",
    "    def make_maps(name, scaled, lo, hi):\n",
    "        series_map, win_map = {}, {}\n",
    "        for sigma in noise_grid:\n",
    "            seed = _stable_seed(f\"{name}|sigma={sigma:.3f}|W={window_size}\")\n",
    "            noisy = add_gaussian_noise_series(scaled, sigma, lo, hi, seed)\n",
    "            series_map[round(sigma,3)] = noisy\n",
    "            win_map[round(sigma,3)] = make_windows(noisy, window_size, window_step)\n",
    "        return series_map, win_map\n",
    "\n",
    "    nsA_s, nsA_w = make_maps(\"MG_A\", sA, loA, hiA)\n",
    "    nsB_s, nsB_w = make_maps(\"MG_B\", sB, loB, hiB)\n",
    "\n",
    "    return {\n",
    "        \"MG_A\": Dataset(\"MG_A\", sA, loA, hiA, winA, splitA, nsA_s, nsA_w),\n",
    "        \"MG_B\": Dataset(\"MG_B\", sB, loB, hiB, winB, splitB, nsB_s, nsB_w),\n",
    "    }\n",
    "\n",
    "DATASETS_BY_W = {\n",
    "    4: _build_for_window_size(4, 1, NOISE_GRID),\n",
    "    6: _build_for_window_size(6, 1, NOISE_GRID),\n",
    "}\n",
    "print(\"Datasets ready for window sizes:\", list(DATASETS_BY_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31c32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3 — Sidekick class (Stage-3, fixed decoder = adjoint(teacher)) ====\n",
    "class Stage3FixedDecoderQAE:\n",
    "    \"\"\"\n",
    "    Input embedding: RY(pi * v01(values)) per qubit.\n",
    "    Encoder (student): E_psi — YOUR ANSATZ HERE (default RX/RY/RZ + ring CNOT).\n",
    "    Decoder (fixed):   D = adjoint(E_phi) — must mirror *the same* topology.\n",
    "    Readout: expZ on all wires → arccos mapping back to [low,high] values.\n",
    "\n",
    "    Exposes:\n",
    "      - forward_expZ(values_window)            → expZ after full pipeline\n",
    "      - latents_after_encoder_expZ(values)     → expZ on latent wires after E_psi\n",
    "      - trash_probs_after_encoder(values)      → probs over trash wires after E_psi\n",
    "      - predict_values(values)                 → value-domain reconstruction\n",
    "      - describe()                             → prints architecture summary\n",
    "    \"\"\"\n",
    "    def __init__(self, *, n_qubits, n_layers, n_latent=2, trash_wires=None,\n",
    "                 scale_low=0.2, scale_high=0.8,\n",
    "                 psi_flat: np.ndarray, theta_fixed: np.ndarray):\n",
    "        self.n_qubits=int(n_qubits); self.n_layers=int(n_layers)\n",
    "        self.n_latent=int(n_latent)\n",
    "        self.latent_wires=list(range(self.n_latent))\n",
    "        self.trash_wires=list(trash_wires) if trash_wires is not None else list(range(self.n_latent, self.n_qubits))\n",
    "        self.scale_low=float(scale_low); self.scale_high=float(scale_high)\n",
    "\n",
    "        self.psi = pnp.asarray(np.asarray(psi_flat, dtype=float).ravel(), requires_grad=False)\n",
    "        self.theta_fixed = pnp.asarray(np.asarray(theta_fixed, dtype=float).reshape(self.n_layers, self.n_qubits, 3),\n",
    "                                       requires_grad=False)\n",
    "\n",
    "        self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        self._q_fwd=None; self._q_lat=None; self._q_trash=None\n",
    "\n",
    "    # --- >>> YOUR CIRCUIT ANSATZ HERE (encoder & decoder) <<< -----------------\n",
    "    # Replace the RX/RY/RZ+ring with your template.\n",
    "    def _enc_student(self, flat):\n",
    "        L,Q=self.n_layers,self.n_qubits; fp=flat\n",
    "        for l in range(L):\n",
    "            for q in range(Q):\n",
    "                i=l*Q*3+q*3\n",
    "                qml.RX(fp[i+0],wires=q); qml.RY(fp[i+1],wires=q); qml.RZ(fp[i+2],wires=q)\n",
    "            for q in range(Q-1): qml.CNOT(wires=[q,q+1])\n",
    "            qml.CNOT(wires=[Q-1,0])\n",
    "\n",
    "    def _enc_teacher_body(self, theta):\n",
    "        L,Q=self.n_layers,self.n_qubits; T=theta\n",
    "        for l in range(L):\n",
    "            for q in range(Q):\n",
    "                qml.RX(T[l,q,0],wires=q); qml.RY(T[l,q,1],wires=q); qml.RZ(T[l,q,2],wires=q)\n",
    "            for q in range(Q-1): qml.CNOT(wires=[q,q+1])\n",
    "            qml.CNOT(wires=[Q-1,0])\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    def _dec_fixed(self):  # D = E_phi^\\dagger\n",
    "        qml.adjoint(self._enc_teacher_body)(self.theta_fixed)\n",
    "\n",
    "    def _embed(self, values):\n",
    "        for i, th in enumerate(values_to_RY_angles(values, self.scale_low, self.scale_high)):\n",
    "            qml.RY(float(th), wires=i)\n",
    "\n",
    "    def _build_forward(self):\n",
    "        @qml.qnode(self.dev, diff_method=None)\n",
    "        def qnode(values_window):\n",
    "            self._embed(values_window); self._enc_student(self.psi); self._dec_fixed()\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "        return qnode\n",
    "\n",
    "    def _build_lat(self):\n",
    "        @qml.qnode(self.dev, diff_method=None)\n",
    "        def qnode(values_window):\n",
    "            self._embed(values_window); self._enc_student(self.psi)\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in self.latent_wires]\n",
    "        return qnode\n",
    "\n",
    "    def _build_trash(self):\n",
    "        @qml.qnode(self.dev, diff_method=None)\n",
    "        def qnode(values_window):\n",
    "            self._embed(values_window); self._enc_student(self.psi)\n",
    "            return qml.probs(wires=self.trash_wires)\n",
    "        return qnode\n",
    "\n",
    "    # Framework API\n",
    "    def forward_expZ(self, values_window):\n",
    "        if self._q_fwd is None: self._q_fwd=self._build_forward()\n",
    "        return np.asarray(self._q_fwd(values_window), dtype=float)\n",
    "\n",
    "    def latents_after_encoder_expZ(self, values_window):\n",
    "        if self._q_lat is None: self._q_lat=self._build_lat()\n",
    "        return np.asarray(self._q_lat(values_window), dtype=float)\n",
    "\n",
    "    def trash_probs_after_encoder(self, values_window):\n",
    "        if self._q_trash is None: self._q_trash=self._build_trash()\n",
    "        return np.asarray(self._q_trash(values_window), dtype=float)\n",
    "\n",
    "    def map_expZ_to_values(self, z_all):\n",
    "        return readoutZ_to_values(z_all, self.scale_low, self.scale_high)\n",
    "\n",
    "    def predict_values(self, values_window):\n",
    "        return self.map_expZ_to_values(self.forward_expZ(values_window))\n",
    "\n",
    "    def describe(self):\n",
    "        print(f\"QAE Sidekick — nq={self.n_qubits}, layers={self.n_layers}, \"\n",
    "              f\"latent={self.n_latent}, trash={len(self.trash_wires)} \"\n",
    "              f\"({self.trash_wires}), scale=[{self.scale_low},{self.scale_high}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b536d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sidekick loader ready.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4 — Loader (schema-flexible + standardized filename parsing) =======\n",
    "# EXPECTED BUNDLE (sidekick):\n",
    "#   parameters:\n",
    "#     - psi_stage3 : flat vector for Stage-3 noisy encoder E_ψ   (shape = L*Q*3) \n",
    "#     - phi_stage1 : flat vector for Stage-1 clean encoder E_φ   (shape = L*Q*3)\n",
    "#   We construct the fixed decoder for Stage-3 as D = adjoint(E_φ).\n",
    "\n",
    "import re, json, hashlib\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def _sha256_params(obj):\n",
    "    buf = bytearray()\n",
    "    def walk(x):\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            for y in x: walk(y)\n",
    "        elif isinstance(x, np.ndarray):\n",
    "            buf.extend(np.ascontiguousarray(x, dtype=np.float64).tobytes())\n",
    "        elif isinstance(x, (float, int, np.floating, np.integer)):\n",
    "            buf.extend(np.asarray([x], dtype=np.float64).tobytes())\n",
    "        else:\n",
    "            buf.extend(str(x).encode(\"utf-8\"))\n",
    "    walk(obj)\n",
    "    return hashlib.sha256(buf).hexdigest()\n",
    "\n",
    "# Standardized filename pattern:  4q_2l_2t_3ls_01.json\n",
    "_STD_RE = re.compile(r\"(?P<nq>\\d+)q_(?P<nl>\\d+)l_(?P<nt>\\d+)t_(?P<L>\\d+)ls_(?P<inst>\\d+)\\.json$\", re.IGNORECASE)\n",
    "\n",
    "def parse_std_filename(path):\n",
    "    m = _STD_RE.search(os.path.basename(path))\n",
    "    if not m: return None\n",
    "    d = {k:int(v) for k,v in m.groupdict().items()}\n",
    "    return {\"n_qubits\": d[\"nq\"], \"n_latent\": d[\"nl\"], \"n_trash\": d[\"nt\"], \"n_layers\": d[\"L\"], \"instance\": d[\"inst\"]}\n",
    "\n",
    "@dataclass\n",
    "class ModelEntry:\n",
    "    name: str\n",
    "    path: str\n",
    "    n_qubits: int\n",
    "    n_latent: int\n",
    "    trash_wires: list\n",
    "    scale_low: float\n",
    "    scale_high: float\n",
    "    noise_sigma_train: float\n",
    "    meta: dict\n",
    "    params: dict\n",
    "    fingerprint: str\n",
    "    impl: object\n",
    "\n",
    "def parse_model_json_sidekick(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        J = json.load(f)\n",
    "\n",
    "    if \"parameters\" in J and \"psi_stage3\" in J[\"parameters\"] and \"phi_stage1\" in J[\"parameters\"]:\n",
    "        P = J[\"parameters\"]\n",
    "        psi = np.asarray(P[\"psi_stage3\"], dtype=float).ravel()\n",
    "        phi = np.asarray(P[\"phi_stage1\"], dtype=float).ravel()\n",
    "\n",
    "        arch = J.get(\"architecture\", {})\n",
    "        ds   = J.get(\"dataset\", {})\n",
    "        run  = J.get(\"run\", {})\n",
    "\n",
    "        # If meta missing, try filename\n",
    "        fn_meta = parse_std_filename(path) or {}\n",
    "        n_qubits = int(arch.get(\"n_qubits\", ds.get(\"window_size\", fn_meta.get(\"n_qubits\", 4))))\n",
    "        if psi.size % (n_qubits*3) != 0:\n",
    "            raise ValueError(f\"{os.path.basename(path)}: psi_stage3 length {psi.size} not divisible by n_qubits*3={n_qubits*3}\")\n",
    "        n_layers = int(arch.get(\"n_layers\", fn_meta.get(\"n_layers\", psi.size // (n_qubits*3))))\n",
    "        n_latent = int(arch.get(\"n_latent\", fn_meta.get(\"n_latent\", max(1, n_qubits//2))))\n",
    "        n_trash  = int(arch.get(\"n_trash\",  fn_meta.get(\"n_trash\", n_qubits - n_latent)))\n",
    "\n",
    "        latent_wires = arch.get(\"latent_wires\", list(range(n_latent)))\n",
    "        trash_wires  = arch.get(\"trash_wires\", list(range(n_latent, n_qubits)))\n",
    "        scale_low  = float(ds.get(\"scale_low\", 0.2))\n",
    "        scale_high = float(ds.get(\"scale_high\", 0.8))\n",
    "        theta_fixed = phi.reshape(n_layers, n_qubits, 3)\n",
    "\n",
    "        info = {\n",
    "            \"n_qubits\": n_qubits, \"n_layers\": n_layers,\n",
    "            \"n_latent\": n_latent, \"n_trash\": n_trash,\n",
    "            \"trash_wires\": list(trash_wires),\n",
    "            \"latent_wires\": list(latent_wires),\n",
    "            \"scale_low\": scale_low, \"scale_high\": scale_high,\n",
    "            \"noise_sigma_train\": float(run.get(\"sigma_train\", np.nan)),\n",
    "            \"source_schema\": J.get(\"schema\", {}).get(\"name\", \"sidekick_bundle\"),\n",
    "            \"filename\": os.path.basename(path),\n",
    "        }\n",
    "        return {\"psi\": psi, \"theta_fixed\": theta_fixed, \"info\": info}\n",
    "\n",
    "    raise ValueError(f\"{os.path.basename(path)}: unsupported JSON (missing parameters.psi_stage3 & parameters.phi_stage1)\")\n",
    "\n",
    "def instantiate_model_sidekick(parsed):\n",
    "    i=parsed[\"info\"]\n",
    "    impl = Stage3FixedDecoderQAE(\n",
    "        n_qubits=i[\"n_qubits\"], n_layers=i[\"n_layers\"],\n",
    "        n_latent=i[\"n_latent\"], trash_wires=i[\"trash_wires\"],\n",
    "        scale_low=i[\"scale_low\"], scale_high=i[\"scale_high\"],\n",
    "        psi_flat=parsed[\"psi\"], theta_fixed=parsed[\"theta_fixed\"]\n",
    "    )\n",
    "    return impl\n",
    "\n",
    "def load_models_sidekick(model_paths: dict):\n",
    "    registry = {}\n",
    "    for name, path in model_paths.items():\n",
    "        try:\n",
    "            P = parse_model_json_sidekick(path)\n",
    "            impl = instantiate_model_sidekick(P)\n",
    "            fp = _sha256_params([P[\"psi\"], P[\"theta_fixed\"]])\n",
    "            i = P[\"info\"]\n",
    "            entry = ModelEntry(\n",
    "                name=name, path=path, n_qubits=i[\"n_qubits\"], n_latent=i[\"n_latent\"],\n",
    "                trash_wires=list(i[\"trash_wires\"]), scale_low=i[\"scale_low\"], scale_high=i[\"scale_high\"],\n",
    "                noise_sigma_train=float(i[\"noise_sigma_train\"]), meta=i,\n",
    "                params={\"psi_flat\": P[\"psi\"], \"theta_fixed\": P[\"theta_fixed\"]},\n",
    "                fingerprint=fp, impl=impl\n",
    "            )\n",
    "            registry[name] = entry\n",
    "            print(f\"✓ Loaded {name:<24s} file={i['filename']} \"\n",
    "                  f\"(nq={entry.n_qubits}, L={i['n_layers']}, latent={entry.n_latent}, \"\n",
    "                  f\"trash={len(entry.trash_wires)}, σ_train={entry.noise_sigma_train:g})\")\n",
    "            impl.describe()\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {name}: {e}\")\n",
    "    return registry\n",
    "\n",
    "print(\"Sidekick loader ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09d42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5 — Evaluation core ===============================================\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    noisy_mse: np.ndarray\n",
    "    model_mse: np.ndarray\n",
    "    delta_pct: np.ndarray\n",
    "    delta_pct_sym: np.ndarray\n",
    "    recon_values: np.ndarray\n",
    "    noisy_values: np.ndarray\n",
    "    clean_values: np.ndarray\n",
    "    lat_clean: np.ndarray|None\n",
    "    lat_noisy: np.ndarray|None\n",
    "    p00_trash: np.ndarray|None\n",
    "\n",
    "def _sigma_key(s): return float(np.round(float(s), 3))\n",
    "\n",
    "def _pick_sigma(entry, override):\n",
    "    return float(entry.noise_sigma_train) if (override == \"model\") else float(override)\n",
    "\n",
    "def eval_model_on_dataset(entry, ds, n_eval=20, sigma_override=\"model\"):\n",
    "    s = _pick_sigma(entry, sigma_override)\n",
    "    s_key = _sigma_key(s)\n",
    "    if s_key not in ds.noisy_windows_by_sigma:\n",
    "        available = \", \".join([f\"{k:.3f}\" for k in sorted(ds.noisy_windows_by_sigma.keys())])\n",
    "        raise ValueError(f\"{ds.name}: σ={s_key:.3f} not in [{available}]\")\n",
    "    cleanW = ds.windows_clean[ds.split_idx:]\n",
    "    noisyW = ds.noisy_windows_by_sigma[s_key][ds.split_idx:]\n",
    "    N = min(int(n_eval), len(cleanW))\n",
    "    cleanW = cleanW[:N]; noisyW = noisyW[:N]\n",
    "\n",
    "    impl = entry.impl\n",
    "    recon=[]; nmse=[]; dmse=[]; d_pct=[]; d_sym=[]\n",
    "    lat_c, lat_n, p00 = [], [], []\n",
    "\n",
    "    for w_clean, w_noisy in zip(cleanW, noisyW):\n",
    "        z = impl.forward_expZ(w_noisy)\n",
    "        y = impl.map_expZ_to_values(z)\n",
    "        recon.append(y)\n",
    "        mse_n = float(np.mean((w_clean - w_noisy)**2))\n",
    "        mse_d = float(np.mean((w_clean - y)**2))\n",
    "        nmse.append(mse_n); dmse.append(mse_d)\n",
    "        d_pct.append(0.0 if mse_n<1e-12 else 100.0 * (mse_n - mse_d) / mse_n)\n",
    "        d_sym.append(200.0 * (mse_n - mse_d) / max(mse_n + mse_d, 1e-12))\n",
    "\n",
    "        try:\n",
    "            lc = impl.latents_after_encoder_expZ(w_clean)\n",
    "            ln = impl.latents_after_encoder_expZ(w_noisy)\n",
    "            lat_c.append(lc); lat_n.append(ln)\n",
    "        except Exception:\n",
    "            lat_c = lat_n = None\n",
    "        try:\n",
    "            P = impl.trash_probs_after_encoder(w_noisy); p00.append(float(P[0]))\n",
    "        except Exception:\n",
    "            p00 = None\n",
    "\n",
    "    return EvalResult(\n",
    "        noisy_mse=np.asarray(nmse), model_mse=np.asarray(dmse),\n",
    "        delta_pct=np.asarray(d_pct), delta_pct_sym=np.asarray(d_sym),\n",
    "        recon_values=np.asarray(recon), noisy_values=noisyW, clean_values=cleanW,\n",
    "        lat_clean=None if lat_c is None else np.asarray(lat_c),\n",
    "        lat_noisy=None if lat_n is None else np.asarray(lat_n),\n",
    "        p00_trash=None if p00 is None else np.asarray(p00)\n",
    "    )\n",
    "\n",
    "def summarize_eval(er):\n",
    "    mean_imp, lo_imp, hi_imp = bootstrap_ci_mean(er.delta_pct)\n",
    "    mean_sym, lo_sym, hi_sym = bootstrap_ci_mean(er.delta_pct_sym)\n",
    "    p_val = sign_test_pvalue(er.noisy_mse - er.model_mse)\n",
    "    succ  = 100.0 * float(np.mean(er.delta_pct > 0))\n",
    "    return {\n",
    "        \"noisy_MSE_mean\": float(np.mean(er.noisy_mse)),\n",
    "        \"model_MSE_mean\": float(np.mean(er.model_mse)),\n",
    "        \"delta_pct_mean\": float(mean_imp),\n",
    "        \"delta_pct_CI95\": [float(lo_imp), float(hi_imp)],\n",
    "        \"delta_pct_sym_mean\": float(mean_sym),\n",
    "        \"delta_pct_sym_CI95\": [float(lo_sym), float(hi_sym)],\n",
    "        \"sign_test_p\": float(p_val),\n",
    "        \"success_rate_pct\": float(succ),\n",
    "        \"n_windows\": int(er.noisy_mse.size)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f511e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6 — Plotting & extra analytics ====================================\n",
    "def plot_reconstruction_example(er: EvalResult, idx=0, title=\"Reconstruction\"):\n",
    "    c,n,d = er.clean_values[idx], er.noisy_values[idx], er.recon_values[idx]\n",
    "    xs = np.arange(len(c)); plt.figure()\n",
    "    plt.plot(xs, c, label=\"clean\"); plt.plot(xs, n, label=\"noisy\"); plt.plot(xs, d, label=\"denoised\")\n",
    "    plt.title(title); plt.xlabel(\"t\"); plt.ylabel(\"value\"); plt.legend(); plt.show()\n",
    "\n",
    "def plot_delta_distributions(er_by_model, title=\"ΔMSE% (per-window)\"):\n",
    "    plt.figure()\n",
    "    for name, er in er_by_model.items(): plt.hist(er.delta_pct, bins=12, alpha=0.5, label=name)\n",
    "    plt.title(title); plt.xlabel(\"improvement %\"); plt.ylabel(\"count\"); plt.legend(); plt.show()\n",
    "\n",
    "def plot_robustness(entry, ds, sigmas=(0.05,0.075,0.10), n_eval=20):\n",
    "    means,lows,highs=[],[],[]\n",
    "    for s in sigmas:\n",
    "        er = eval_model_on_dataset(entry, ds, n_eval=n_eval, sigma_override=s)\n",
    "        m,lo,hi = bootstrap_ci_mean(er.delta_pct); means.append(m); lows.append(lo); highs.append(hi)\n",
    "    plt.figure(); plt.plot(sigmas, means, marker=\"o\"); plt.fill_between(sigmas, lows, highs, alpha=0.2)\n",
    "    plt.xlabel(\"σ\"); plt.ylabel(\"ΔMSE% (mean, 95%CI)\"); plt.title(f\"Robustness on {ds.name} — {entry.name}\"); plt.show()\n",
    "\n",
    "def plot_latent_trash(er, entry, dsname):\n",
    "    if er.lat_clean is not None and er.lat_noisy is not None:\n",
    "        diffs = np.mean((er.lat_clean - er.lat_noisy)**2, axis=1)\n",
    "        plt.figure(); plt.hist(diffs, bins=12)\n",
    "        plt.title(f\"Latent drift MSE — {entry.name} on {dsname}\"); plt.xlabel(\"MSE\"); plt.ylabel(\"count\"); plt.show()\n",
    "    if er.p00_trash is not None:\n",
    "        plt.figure(); plt.hist(er.p00_trash, bins=12)\n",
    "        plt.title(f\"Trash P(|00⟩) after encoder — {entry.name} on {dsname}\")\n",
    "        plt.xlabel(\"P00\"); plt.ylabel(\"count\"); plt.show()\n",
    "\n",
    "# Tables & heatmaps\n",
    "def summaries_to_df(all_summaries, registry):\n",
    "    rows=[]\n",
    "    for dsname, bymodel in all_summaries.items():\n",
    "        for name, sm in bymodel.items():\n",
    "            e = registry[name]\n",
    "            rows.append({\n",
    "                \"dataset\": dsname, \"model\": name,\n",
    "                \"n_qubits\": e.n_qubits, \"n_latent\": e.n_latent, \"n_trash\": len(e.trash_wires),\n",
    "                \"layers\": e.meta.get(\"n_layers\", e.params[\"psi_flat\"].size // (e.n_qubits*3)),\n",
    "                \"delta_pct_mean\": sm[\"delta_pct_mean\"],\n",
    "                \"delta_pct_lo\": sm[\"delta_pct_CI95\"][0],\n",
    "                \"delta_pct_hi\": sm[\"delta_pct_CI95\"][1],\n",
    "                \"success_rate_pct\": sm[\"success_rate_pct\"],\n",
    "                \"p\": sm[\"sign_test_p\"],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def heatmap_by_arch(df, dataset, value_col=\"delta_pct_mean\"):\n",
    "    sub = df[df[\"dataset\"]==dataset]\n",
    "    for (nl, nt), group in sub.groupby([\"n_latent\",\"n_trash\"]):\n",
    "        piv = group.pivot_table(index=\"n_qubits\", columns=\"layers\", values=value_col, aggfunc=\"mean\")\n",
    "        plt.figure(figsize=(5.2,3.6)); plt.imshow(piv.values, aspect=\"auto\")\n",
    "        plt.xticks(range(piv.shape[1]), piv.columns); plt.yticks(range(piv.shape[0]), piv.index)\n",
    "        plt.colorbar(label=value_col); plt.title(f\"{dataset} — mean {value_col} (latent={nl}, trash={nt})\")\n",
    "        plt.xlabel(\"layers\"); plt.ylabel(\"n_qubits\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Full-series tools\n",
    "def _flatten_avg(windows, step):\n",
    "    N,W = windows.shape; L=(N-1)*step+W\n",
    "    acc=np.zeros(L); cnt=np.zeros(L)\n",
    "    for i in range(N): s=i*step; e=s+W; acc[s:e]+=windows[i]; cnt[s:e]+=1\n",
    "    return acc/np.maximum(cnt,1e-12)\n",
    "\n",
    "def reconstruct_full_series(entry, ds, sigma_override=\"model\", step=1):\n",
    "    s = _pick_sigma(entry, sigma_override); s_key=_sigma_key(s)\n",
    "    cleanW = np.asarray(ds.windows_clean, dtype=float)\n",
    "    noisyW = np.asarray(ds.noisy_windows_by_sigma[s_key], dtype=float)\n",
    "    preds = [entry.impl.map_expZ_to_values(entry.impl.forward_expZ(w)) for w in noisyW]\n",
    "    preds = np.asarray(preds)\n",
    "    flat_c=_flatten_avg(cleanW, step); flat_n=_flatten_avg(noisyW, step); flat_d=_flatten_avg(preds, step)\n",
    "    mse_n=float(np.mean((flat_c-flat_n)**2)); mse_d=float(np.mean((flat_c-flat_d)**2))\n",
    "    d_pct=0.0 if mse_n<1e-12 else 100.0*(mse_n-mse_d)/mse_n\n",
    "    return {\"clean\":flat_c,\"noisy\":flat_n,\"deno\":flat_d,\"mse_noisy\":mse_n,\"mse_deno\":mse_d,\"delta_pct\":d_pct,\"sigma\":s_key}\n",
    "\n",
    "def plot_full_series(rec, title):\n",
    "    L=len(rec[\"clean\"]); xs=np.arange(L)\n",
    "    plt.figure(figsize=(9.8,4.0))\n",
    "    plt.plot(xs, rec[\"clean\"], label=\"clean\")\n",
    "    plt.plot(xs, rec[\"noisy\"], label=f\"noisy (MSE={rec['mse_noisy']:.5f})\")\n",
    "    plt.plot(xs, rec[\"deno\"],  label=f\"deno (MSE={rec['mse_deno']:.5f}, Δ%={rec['delta_pct']:+.1f})\")\n",
    "    plt.xlabel(\"t\"); plt.ylabel(\"value\"); plt.title(title); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_full_series_panels(entry, ds, sigma_override=\"model\", panel_len=180, ncols=3, nrows=2, start=0, sharey=True):\n",
    "    rec = reconstruct_full_series(entry, ds, sigma_override)\n",
    "    c,n,d = rec[\"clean\"], rec[\"noisy\"], rec[\"deno\"]; L=len(c)\n",
    "    fig,axes = plt.subplots(nrows, ncols, figsize=(4.5*ncols,2.75*nrows), sharey=sharey)\n",
    "    axes=np.ravel(axes)\n",
    "    for k,ax in enumerate(axes):\n",
    "        s = start + k*panel_len; e=min(s+panel_len, L)\n",
    "        if s>=L: ax.axis(\"off\"); continue\n",
    "        xs=np.arange(s,e); cc=c[s:e]; nn=n[s:e]; dd=d[s:e]\n",
    "        mse_n=float(np.mean((cc-nn)**2)); mse_d=float(np.mean((cc-dd)**2))\n",
    "        d_pct=0.0 if mse_n<1e-12 else 100.0*(mse_n-mse_d)/mse_n\n",
    "        ax.plot(xs,cc,label=\"clean\"); ax.plot(xs,nn,label=f\"noisy (MSE={mse_n:.5f})\")\n",
    "        ax.plot(xs,dd,label=f\"deno (MSE={mse_d:.5f}, Δ%={d_pct:+.1f})\")\n",
    "        if k==0: ax.legend(); ax.set_ylabel(\"value\")\n",
    "        ax.set_title(f\"{s}–{e}\"); ax.set_xlabel(\"t\")\n",
    "    fig.suptitle(f\"{entry.name} — {ds.name} (σ={rec['sigma']:.3f}), panels of {panel_len}\", y=1.02)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def error_lens(entry, ds, sigma_override=\"model\", seg=150, start=0, length=None):\n",
    "    rec = reconstruct_full_series(entry, ds, sigma_override)\n",
    "    c,n,d = rec[\"clean\"], rec[\"noisy\"], rec[\"deno\"]; L=len(c)\n",
    "    if length is None: length=L-start\n",
    "    s,e=int(start),int(min(start+length,L))\n",
    "    se_n=(n-c)**2; se_d=(d-c)**2; imp=se_n-se_d\n",
    "    mean_n=float(se_n.mean()); mean_d=float(se_d.mean()); d_pct=100.0*(mean_n-mean_d)/max(mean_n,1e-12)\n",
    "    frac=100.0*float(np.mean(imp>0))\n",
    "    print(f\"{ds.name} | {entry.name} | σ={rec['sigma']:.3f}\\nGlobal MSE: noisy={mean_n:.5f} deno={mean_d:.5f} Δ%={d_pct:+.1f}\\nSamples improved: {frac:.1f}%\")\n",
    "    xs=np.arange(s,e); plt.figure(figsize=(10,3.1)); plt.plot(xs, se_n[s:e], label=\"(clean-noisy)^2\"); plt.plot(xs, se_d[s:e], label=\"(clean-deno)^2\")\n",
    "    plt.title(f\"Per-sample squared errors — {ds.name} [{s}:{e}]\"); plt.xlabel(\"t\"); plt.ylabel(\"squared error\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "    plt.figure(figsize=(10,2.5)); imp_seg=imp[s:e]; plt.plot(xs, imp_seg, lw=0.8)\n",
    "    plt.fill_between(xs,0,imp_seg, where=(imp_seg>=0), alpha=0.35); plt.fill_between(xs,0,imp_seg, where=(imp_seg<0), alpha=0.25)\n",
    "    plt.axhline(0,color=\"k\",lw=0.8); plt.title(f\"Improvement per sample — {ds.name} [{s}:{e}]\"); plt.xlabel(\"t\"); plt.ylabel(\"ΔSE\"); plt.tight_layout(); plt.show()\n",
    "    bins=list(range(0,L,seg)); contrib=[imp[i:i+seg].sum() for i in bins]; centers=[i+seg/2 for i in bins]\n",
    "    plt.figure(figsize=(10,3.0)); plt.bar(centers, contrib, width=0.8*seg); plt.axhline(0,color=\"k\",lw=0.8)\n",
    "    plt.title(f\"Contribution to total improvement by {seg}-sample segments — {ds.name}\"); plt.xlabel(\"segment center\"); plt.ylabel(\"Σ ΔSE\"); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80065d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7 — Run benchmark (mixed-qubit aware) ==============================\n",
    "def run_benchmark(registry, n_eval=20, sigma_override=\"model\"):\n",
    "    all_results = {}   # {dsname: {model: EvalResult}}\n",
    "    all_summaries = {}# {dsname: {model: summary}}\n",
    "    for name, entry in registry.items():\n",
    "        w = int(entry.n_qubits)\n",
    "        if w not in DATASETS_BY_W:\n",
    "            warnings.warn(f\"No dataset for window_size={w}; skipping {name}.\"); continue\n",
    "        for dsname, ds in DATASETS_BY_W[w].items():\n",
    "            er = eval_model_on_dataset(entry, ds, n_eval=n_eval, sigma_override=sigma_override)\n",
    "            sm = summarize_eval(er)\n",
    "            all_results.setdefault(dsname, {})[name] = er\n",
    "            all_summaries.setdefault(dsname, {})[name] = sm\n",
    "            print(f\"{dsname} | {name:24s} Δ%={sm['delta_pct_mean']:+5.1f} \"\n",
    "                  f\"(CI {sm['delta_pct_CI95'][0]:+.1f},{sm['delta_pct_CI95'][1]:+.1f}) \"\n",
    "                  f\"p={sm['sign_test_p']:.4f}\")\n",
    "    return all_results, all_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e65019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8 — Load models & standard analyses ===============================\n",
    "REG = load_models_sidekick(MODEL_PATHS)\n",
    "\n",
    "ALL_RESULTS, ALL_SUMMARIES = run_benchmark(REG, n_eval=N_EVAL_WINDOWS, sigma_override=EVAL_SIGMA)\n",
    "\n",
    "# Table + quick heatmaps\n",
    "if REG:\n",
    "    df = summaries_to_df(ALL_SUMMARIES, REG)\n",
    "    print(\"\\n== Summary (head) ==\")\n",
    "    print(df.head(12).to_string(index=False))\n",
    "\n",
    "    for dsname in sorted(ALL_SUMMARIES):\n",
    "        heatmap_by_arch(df, dsname, value_col=\"delta_pct_mean\")\n",
    "\n",
    "    # Show a few visuals per window size (first model hitting that size)\n",
    "    shown=set()\n",
    "    for name, entry in REG.items():\n",
    "        w = entry.n_qubits\n",
    "        if w in shown: continue\n",
    "        dsname = \"MG_A\"\n",
    "        er = ALL_RESULTS[dsname][name]\n",
    "        plot_reconstruction_example(er, idx=0, title=f\"{name} — {dsname} (σ={'model' if EVAL_SIGMA=='model' else EVAL_SIGMA})\")\n",
    "        plot_robustness(entry, DATASETS_BY_W[w][dsname], sigmas=(0.10,), n_eval=N_EVAL_WINDOWS)\n",
    "        # Full series & panels on MG_B\n",
    "        rec = reconstruct_full_series(entry, DATASETS_BY_W[w][\"MG_B\"], sigma_override=EVAL_SIGMA)\n",
    "        plot_full_series(rec, title=f\"{name} — MG_B (σ={rec['sigma']:.3f})\")\n",
    "        plot_full_series_panels(entry, DATASETS_BY_W[w][\"MG_B\"], sigma_override=EVAL_SIGMA,\n",
    "                                panel_len=180, ncols=3, nrows=2, start=0, sharey=True)\n",
    "        error_lens(entry, DATASETS_BY_W[w][\"MG_B\"], sigma_override=EVAL_SIGMA, seg=150, start=0, length=900)\n",
    "        shown.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca325b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9 — Append eval metrics to existing CSVs ===========================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- 1) Build eval summary DF per model/file --------------------------------\n",
    "def _collect_eval_df(all_summaries, registry):\n",
    "    rows=[]\n",
    "    for dsname, bymodel in all_summaries.items():\n",
    "        for name, sm in bymodel.items():\n",
    "            e = registry[name]\n",
    "            filename = os.path.basename(e.path)                # e.g., \"4q_2l_2t_1ls_01.json\"\n",
    "            rows.append({\n",
    "                \"filename\": filename,\n",
    "                \"model_tag\": name,                              # usually filename stem\n",
    "                \"dataset\": dsname,                              # MG_A or MG_B\n",
    "                \"n_qubits\": e.n_qubits,\n",
    "                \"n_layers\": e.meta.get(\"n_layers\", e.params[\"psi_flat\"].size // (e.n_qubits*3)),\n",
    "                \"n_latent\": e.n_latent,\n",
    "                \"n_trash\": len(e.trash_wires),\n",
    "                \"delta_pct_mean\": sm[\"delta_pct_mean\"],\n",
    "                \"success_rate_pct\": sm[\"success_rate_pct\"],\n",
    "                \"sign_test_p\": sm[\"sign_test_p\"],\n",
    "                \"n_windows\": sm[\"n_windows\"],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "eval_df = _collect_eval_df(ALL_SUMMARIES, REG)\n",
    "if eval_df.empty:\n",
    "    raise RuntimeError(\"No eval summaries collected. Did models load & run?\")\n",
    "\n",
    "# pivot to have both datasets as columns\n",
    "pivot = eval_df.pivot_table(index=\"filename\", columns=\"dataset\",\n",
    "                            values=[\"delta_pct_mean\",\"success_rate_pct\",\"sign_test_p\"],\n",
    "                            aggfunc=\"first\")\n",
    "# flatten columns\n",
    "pivot.columns = [f\"{a}_{b}\" for a,b in pivot.columns.to_flat_index()]\n",
    "pivot = pivot.reset_index()\n",
    "\n",
    "print(\"Eval pivot (head):\")\n",
    "print(pivot.head().to_string(index=False))\n",
    "\n",
    "# ---- 2) Locate your CSVs ----------------------------------------------------\n",
    "def _find_first(path_glob):\n",
    "    matches = glob.glob(path_glob, recursive=False)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "# If you keep them in Jacob/, set this root; else place absolute paths below.\n",
    "CSV_ROOT = JACOB_ROOT  # same root you used above\n",
    "\n",
    "TRAIN_CSV_IN  = _find_first(os.path.join(CSV_ROOT, \"all_training_instances_v2.csv\"))\n",
    "LAYER_CSV_IN  = _find_first(os.path.join(CSV_ROOT, \"summary_by_layers_v2.csv\"))\n",
    "\n",
    "if TRAIN_CSV_IN is None:\n",
    "    raise FileNotFoundError(\"Could not find all_training_instances_v2.csv next to your Jacob folder.\")\n",
    "if LAYER_CSV_IN is None:\n",
    "    raise FileNotFoundError(\"Could not find summary_by_layers_v2.csv next to your Jacob folder.\")\n",
    "\n",
    "TRAIN_CSV_OUT = TRAIN_CSV_IN.replace(\".csv\", \"_with_eval.csv\")\n",
    "LAYER_CSV_OUT = LAYER_CSV_IN.replace(\".csv\", \"_with_eval.csv\")\n",
    "\n",
    "# ---- 3) Merge eval into training instances ----------------------------------\n",
    "train_df = pd.read_csv(TRAIN_CSV_IN)\n",
    "\n",
    "# Ensure the training table has 'filename' (your earlier build does)\n",
    "if \"filename\" not in train_df.columns:\n",
    "    # try to synthesize from run_tag if needed — but your CSVs already have 'filename'\n",
    "    raise KeyError(\"Training CSV missing 'filename' column; cannot merge eval results.\")\n",
    "\n",
    "merged_train = train_df.merge(pivot, on=\"filename\", how=\"left\")\n",
    "\n",
    "# helpful fill for models not evaluated\n",
    "for c in [\"delta_pct_mean_MG_A\",\"delta_pct_mean_MG_B\",\"success_rate_pct_MG_A\",\"success_rate_pct_MG_B\",\"sign_test_p_MG_A\",\"sign_test_p_MG_B\"]:\n",
    "    if c in merged_train.columns:\n",
    "        merged_train[c] = merged_train[c].astype(float)\n",
    "\n",
    "merged_train.to_csv(TRAIN_CSV_OUT, index=False)\n",
    "print(f\"✓ Wrote training+eval → {TRAIN_CSV_OUT}\")\n",
    "\n",
    "# ---- 4) Recompute per-layer summary with eval columns and merge -------------\n",
    "def _mean_std_safe(x):\n",
    "    v = pd.to_numeric(x, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    return \"n/a\" if v.size == 0 else f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "# Group the *merged_train* to get layer-level eval summaries\n",
    "eval_cols = {\n",
    "    \"delta_MG_A\": (\"delta_pct_mean_MG_A\", _mean_std_safe),\n",
    "    \"delta_MG_B\": (\"delta_pct_mean_MG_B\", _mean_std_safe),\n",
    "    \"succ_MG_A\":  (\"success_rate_pct_MG_A\", _mean_std_safe),\n",
    "    \"succ_MG_B\":  (\"success_rate_pct_MG_B\", _mean_std_safe),\n",
    "    # p-values are not Gaussian-like; report median\n",
    "    \"p50_MG_A\":   (\"sign_test_p_MG_A\", lambda s: f\"{pd.to_numeric(s, errors='coerce').median():.6f}\"),\n",
    "    \"p50_MG_B\":   (\"sign_test_p_MG_B\", lambda s: f\"{pd.to_numeric(s, errors='coerce').median():.6f}\"),\n",
    "}\n",
    "\n",
    "layer_eval = (merged_train\n",
    "              .groupby(\"n_layers\", dropna=False)\n",
    "              .apply(lambda g: pd.Series({k: f(g[c]) for k,(c,f) in eval_cols.items()}))\n",
    "              .reset_index())\n",
    "\n",
    "# Read the existing per-layer summary and attach our new columns\n",
    "layer_df = pd.read_csv(LAYER_CSV_IN)\n",
    "# If its index is n_layers, reset to have a column:\n",
    "if \"n_layers\" not in layer_df.columns and layer_df.index.name == \"n_layers\":\n",
    "    layer_df = layer_df.reset_index()\n",
    "\n",
    "layer_merged = layer_df.merge(layer_eval, on=\"n_layers\", how=\"left\")\n",
    "layer_merged.to_csv(LAYER_CSV_OUT, index=False)\n",
    "print(f\"✓ Wrote per-layer summary+eval → {LAYER_CSV_OUT}\")\n",
    "\n",
    "# ---- 5) Friendly printout ---------------------------------------------------\n",
    "cols_show = [\"n_layers\",\"delta_MG_A\",\"delta_MG_B\",\"succ_MG_A\",\"succ_MG_B\",\"p50_MG_A\",\"p50_MG_B\"]\n",
    "print(\"\\n== Per-layer eval (head) ==\")\n",
    "print(layer_merged[ [c for c in cols_show if c in layer_merged.columns] ].head(10).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
