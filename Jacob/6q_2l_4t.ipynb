{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e979dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# We'll run 5 instances and two depths: 1 and 3 layers.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # used in filenames as ..._ls_01.json, ..._ls_02.json, ...\n",
    "LAYER_OPTIONS  = [1, 3]            # train 1-layer first, then 3-layers\n",
    "EVAL_SIGMA     = 0.10              # fixed noise everywhere (train & eval)\n",
    "\n",
    "# where to save artifacts (JSON bundles, instance records, CSV summary)\n",
    "# tip: new folder so these runs don't mix with your 2L/2T ones\n",
    "OUT_BASE = \"./runs_halfqae_2L4T\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458cfa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils import OK\n",
      "Seed/filename utils ready.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Cell 1 — Imports, utils, reproducibility (fixed seed)\n",
    "# =====================================================\n",
    "import os, sys, json, math, random, time, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- repo utils (your existing readers) -----\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "parent_dir = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'Jacob' else current_dir\n",
    "sys.path.insert(0, parent_dir); sys.path.insert(0, '../')\n",
    "try:\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Utils import OK\")\n",
    "except Exception as e:\n",
    "    print(\"Import error:\", e)\n",
    "    qae_utils_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(''))), 'qae_utils')\n",
    "    sys.path.insert(0, os.path.dirname(qae_utils_path))\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Absolute path fallback OK\")\n",
    "\n",
    "assert callable(read_ts_file) and callable(read_json_file)\n",
    "\n",
    "# ----- plotting defaults -----\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4)\n",
    "\n",
    "# ----- reproducibility -----\n",
    "def set_global_seed(instance_id: int):\n",
    "    \"\"\"\n",
    "    Derive all RNGs from a simple instance ID (1..5).\n",
    "    Keep the mapping stable across notebooks.\n",
    "    \"\"\"\n",
    "    base = 10_000 + int(instance_id)  # simple, memorable\n",
    "    random.seed(base + 11)\n",
    "    np.random.seed(base + 22)\n",
    "    try:\n",
    "        pnp.random.seed(base + 33)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Pennylane's default.qubit is deterministic given params; no device seeding needed.\n",
    "    return dict(global_seed=base, numpy_seed=base+22, pnp_seed=base+33)\n",
    "\n",
    "def std_instance_name(nq, n_latent, n_trash, n_layers, instance_id):\n",
    "    \"\"\"\n",
    "    Standardized filename pattern used across the project.\n",
    "    Example: 4q_2l_2t_3ls_01.json\n",
    "    \"\"\"\n",
    "    return f\"{int(nq)}q_{int(n_latent)}l_{int(n_trash)}t_{int(n_layers)}ls_{int(instance_id):02d}.json\"\n",
    "\n",
    "def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "print(\"Seed/filename utils ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a103b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: mackey_glass_n100\n",
      "Loaded 100 samples; scale [0.200,0.800]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 2 — Data loading (deterministic)\n",
    "# =========================================\n",
    "DATA_PATH = '../jacobs_examples/aintern/data'\n",
    "DATA_NAME = 'mackey_glass'  # your folder prefix\n",
    "\n",
    "# fixed split across all instances/layers (so results are comparable)\n",
    "SPLIT_RANDOM_STATE = 42\n",
    "WINDOW_STRIDE = 1\n",
    "\n",
    "# ---- pick most recent MG folder starting with mackey_glass* ----\n",
    "data_folders = [f for f in os.listdir(DATA_PATH) if f.startswith(DATA_NAME)]\n",
    "if not data_folders:\n",
    "    raise FileNotFoundError(\"No Mackey-Glass data found. Generate it first.\")\n",
    "data_folders.sort()\n",
    "data_folder = data_folders[-1]     # take the last one if multiple\n",
    "print(f\"Using data folder: {data_folder}\")\n",
    "\n",
    "# ---- load series + scaling info ----\n",
    "X_idx = read_ts_file(f'{DATA_PATH}/{data_folder}/x_org.arr')   # indices for plotting, not used\n",
    "y_all = read_ts_file(f'{DATA_PATH}/{data_folder}/y_org.arr')   # values\n",
    "info  = read_json_file(f'{DATA_PATH}/{data_folder}/info.json')\n",
    "\n",
    "print(f\"Loaded {len(y_all)} samples; scale [{info['scale_low']:.3f},{info['scale_high']:.3f}]\")\n",
    "\n",
    "# ---- helper: uniform embed wrapper (works with/without explicit info param)\n",
    "def embed_input(x, info_=None):\n",
    "    \"\"\"\n",
    "    Map value-domain window x (in [lo,hi]) to RY(π·v01).\n",
    "    Accepts optional info to match Stage-3 call signatures.\n",
    "    \"\"\"\n",
    "    if info_ is None:\n",
    "        info_ = info\n",
    "    lo, hi = info_['scale_low'], info_['scale_high']\n",
    "    xn = (pnp.array(x) - lo) / max(hi - lo, 1e-12)   # -> [0,1]\n",
    "    for i, v in enumerate(xn):\n",
    "        qml.RY(v * pnp.pi, wires=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2caea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture set: 6q (2 latent, 4 trash).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — Architecture (do NOT change your brick/entanglers)\n",
    "# ============================================================\n",
    "# This notebook = 6 qubits, 2 latent, 4 trash.\n",
    "n_qubits = 6\n",
    "n_latent = 2\n",
    "n_trash  = n_qubits - n_latent          # 4\n",
    "\n",
    "latent_wires = list(range(n_latent))              # [0,1]\n",
    "trash_wires  = list(range(n_latent, n_qubits))    # [2,3,4,5]\n",
    "signal_wires = list(range(n_qubits))              # [0..5] for diagnostics\n",
    "\n",
    "assert n_latent + n_trash == n_qubits\n",
    "\n",
    "# --- device factory (simple; default.qubit) ---\n",
    "def make_device(nq):\n",
    "    return qml.device('default.qubit', wires=nq)\n",
    "\n",
    "# --- Stage-1 encoder template (unchanged architecture) ---\n",
    "def encoder_template(params, n_layers):\n",
    "    \"\"\"RX/RY/RZ per qubit + ring CNOT per layer.\"\"\"\n",
    "    assert len(params) == n_layers * n_qubits * 3\n",
    "    for l in range(n_layers):\n",
    "        # local rotations\n",
    "        for q in range(n_qubits):\n",
    "            idx = l * n_qubits * 3 + q * 3\n",
    "            qml.RX(params[idx + 0], wires=q)\n",
    "            qml.RY(params[idx + 1], wires=q)\n",
    "            qml.RZ(params[idx + 2], wires=q)\n",
    "        # ring entanglers\n",
    "        for q in range(n_qubits - 1):\n",
    "            qml.CNOT(wires=[q, q + 1])\n",
    "        qml.CNOT(wires=[n_qubits - 1, 0])\n",
    "\n",
    "print(\"Architecture set: 6q (2 latent, 4 trash).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539c5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Cell 4 — Stage-1 QNodes, loss, and training (seeded)\n",
    "# ====================================================\n",
    "def stage1_qnodes(n_layers):\n",
    "    dev = make_device(n_qubits)\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def trash_expectations(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in trash_wires]\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def recon_EdagE(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        qml.adjoint(encoder_template)(phi, n_layers)   # E†\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return trash_expectations, recon_EdagE\n",
    "\n",
    "def stage1_batch_loss(trash_expectations, phi, clean_batch):\n",
    "    # L = mean_{batch,trash} P(|1>) = (1 - Z)/2\n",
    "    zs = []\n",
    "    for c in clean_batch:\n",
    "        z = pnp.array(trash_expectations(phi, c))  # shape (n_trash,)\n",
    "        zs.append(z)\n",
    "    zs = pnp.stack(zs, axis=0)\n",
    "    prob_one = (1.0 - zs) * 0.5\n",
    "    return pnp.mean(prob_one)\n",
    "\n",
    "def train_stage1(X_train, X_val, n_layers, instance_id, \n",
    "                 n_epochs=120, batch_size=32, lr_init=0.010,\n",
    "                 patience=10, lr_patience=8, min_delta=1e-6):\n",
    "    set_global_seed(instance_id)\n",
    "    # init\n",
    "    enc_shape = n_layers * n_qubits * 3\n",
    "    phi = pnp.array(np.random.normal(0, 0.5, enc_shape), requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr_init)\n",
    "    lr = lr_init\n",
    "\n",
    "    trash_expectations, recon_EdagE = stage1_qnodes(n_layers)\n",
    "\n",
    "    # helper\n",
    "    def minibatches(N, B, rng_seed=123456):\n",
    "        rng = np.random.default_rng(rng_seed)  # fixed per-epoch seed below\n",
    "        idx = rng.permutation(N)\n",
    "        for i in range(0, N, B):\n",
    "            yield idx[i:i+B]\n",
    "\n",
    "    train_hist, val_hist, lr_hist = [], [], []\n",
    "    best_phi, best_val = None, float(\"inf\")\n",
    "    no_improve = 0\n",
    "    for ep in range(n_epochs):\n",
    "        # batch order deterministic per-epoch per-instance\n",
    "        seed_ep = 77_000 + 100*instance_id + ep\n",
    "        acc = 0.0; nb = 0\n",
    "        for ix in minibatches(len(X_train), batch_size, rng_seed=seed_ep):\n",
    "            clean_batch = X_train[ix]\n",
    "            def loss_fn(p): return stage1_batch_loss(trash_expectations, p, clean_batch)\n",
    "            phi, cost = opt.step_and_cost(loss_fn, phi)\n",
    "            acc += float(cost); nb += 1\n",
    "        train_cost = acc / max(nb, 1)\n",
    "\n",
    "        # validation\n",
    "        v_costs = []\n",
    "        for c in X_val:\n",
    "            v_costs.append(stage1_batch_loss(trash_expectations, phi, pnp.array([c])))\n",
    "        val_cost = float(pnp.mean(pnp.stack(v_costs)))\n",
    "\n",
    "        train_hist.append(train_cost); val_hist.append(val_cost); lr_hist.append(lr)\n",
    "\n",
    "        if val_cost + min_delta < best_val:\n",
    "            best_val, best_phi = val_cost, pnp.array(phi, requires_grad=False); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % lr_patience) == 0:\n",
    "                lr = max(lr * 0.5, 1e-4)\n",
    "                opt = qml.AdamOptimizer(stepsize=lr)\n",
    "                print(f\"[Stage1] ↓ LR → {lr:.5f}\")\n",
    "            if no_improve >= patience:\n",
    "                print(\"[Stage1] Early stopping.\"); break\n",
    "\n",
    "        print(f\"[Stage1] L={n_layers} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {lr:.5f}\")\n",
    "\n",
    "    phi_best = best_phi if best_phi is not None else phi\n",
    "    return dict(\n",
    "        phi=phi_best, best_val=float(best_val),\n",
    "        hist_train=list(map(float, train_hist)),\n",
    "        hist_val=list(map(float, val_hist)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        recon_EdagE=recon_EdagE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3943eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Cell 5 — Stage-3 (ψ) with fixed decoder = adjoint(Stage-1 encoder φ)\n",
    "#          (records hist_noisy, hist_delta, best_epoch, epochs, seconds)\n",
    "# ======================================================================\n",
    "import time\n",
    "\n",
    "def stage3_qnodes(n_layers, phi_stage1):\n",
    "    dev3 = make_device(n_qubits)\n",
    "    theta_fixed = pnp.array(phi_stage1, requires_grad=False).reshape((n_layers, n_qubits, 3))\n",
    "\n",
    "    def encoder_fixed_body(theta):\n",
    "        for l in range(n_layers):\n",
    "            for q in range(n_qubits):\n",
    "                qml.RX(theta[l, q, 0], wires=q)\n",
    "                qml.RY(theta[l, q, 1], wires=q)\n",
    "                qml.RZ(theta[l, q, 2], wires=q)\n",
    "            for q in range(n_qubits-1):\n",
    "                qml.CNOT(wires=[q, q+1])\n",
    "            qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "    def decoder_fixed():\n",
    "        qml.adjoint(encoder_fixed_body)(theta_fixed)\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def encoder_only_expZ_all(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def teacher_code_latents(x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_fixed_body(theta_fixed)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def student_code_latents(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def denoiser_qnode_all(flat_params, x_noisy):\n",
    "        embed_input(x_noisy)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        decoder_fixed()\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return dict(\n",
    "        theta_fixed=theta_fixed,\n",
    "        encoder_only_expZ_all=encoder_only_expZ_all,\n",
    "        teacher_code_latents=teacher_code_latents,\n",
    "        student_code_latents=student_code_latents,\n",
    "        denoiser_qnode_all=denoiser_qnode_all\n",
    "    )\n",
    "\n",
    "# ----- value readout helpers (unchanged)\n",
    "def Z_to_values_autograd(z_all):\n",
    "    z_all = pnp.clip(pnp.asarray(z_all), -0.999999, 0.999999)\n",
    "    v01 = pnp.arccos(z_all) / pnp.pi\n",
    "    return v01 * (info[\"scale_high\"] - info[\"scale_low\"]) + info[\"scale_low\"]\n",
    "\n",
    "def first_diff(x): \n",
    "    x = pnp.array(x); return x[1:] - x[:-1]\n",
    "\n",
    "def p1_from_expZ(z): \n",
    "    return (1 - pnp.asarray(z)) * 0.5\n",
    "\n",
    "# ----- deterministic noisy window (shared with eval)\n",
    "def ts_add_noise_window_det(x, sigma, seed):\n",
    "    low, high = float(info[\"scale_low\"]), float(info[\"scale_high\"])\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    noise = rng.normal(0.0, sigma * (high - low), size=np.asarray(x).shape)\n",
    "    return np.clip(np.asarray(x) + noise, low, high)\n",
    "\n",
    "# ----- validation with fixed per-window seeds (unchanged)\n",
    "VAL_BASE_SEED = 12345\n",
    "\n",
    "def stage3_val_values_det(psi, X_clean, sigma=EVAL_SIGMA):\n",
    "    ms_noisy, ms_deno = [], []\n",
    "    for i, c in enumerate(X_clean):\n",
    "        n = ts_add_noise_window_det(c, sigma, seed=VAL_BASE_SEED + i)\n",
    "        zD = np.array(stage3_handles[\"denoiser_qnode_all\"](psi, n))\n",
    "        v_hat = np.array(Z_to_values_autograd(zD))\n",
    "        ms_noisy.append(np.mean((np.asarray(c) - np.asarray(n))**2))\n",
    "        ms_deno.append(np.mean((np.asarray(c) - v_hat)**2))\n",
    "    mN, mD = float(np.mean(ms_noisy)), float(np.mean(ms_deno))\n",
    "    d_pct = 100.0 * (1.0 - mD / max(mN, 1e-12))\n",
    "    return mN, mD, d_pct\n",
    "\n",
    "# ----- small Huber\n",
    "def huber(residual, delta):\n",
    "    r = pnp.abs(residual)\n",
    "    return pnp.where(r <= delta, 0.5*r**2, delta*(r - 0.5*delta))\n",
    "\n",
    "\n",
    "def train_stage3(X_train, X_val, phi_stage1, n_layers, instance_id,\n",
    "                 TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16, \n",
    "                 LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "                 CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99):\n",
    "\n",
    "    # ---- seeds: varied but reproducible across (instance, layers, epoch, window)\n",
    "    def make_train_seed(instance_id, layers, ep, k, view=0):\n",
    "        return (1_000_003 * (instance_id * 10 + layers) + 97 * ep + 31 * int(k) + view) % 2_147_483_647\n",
    "\n",
    "    set_global_seed(instance_id)\n",
    "\n",
    "    global stage3_handles\n",
    "    stage3_handles = stage3_qnodes(n_layers, phi_stage1)\n",
    "    enc_all = stage3_handles[\"encoder_only_expZ_all\"]\n",
    "    teacher_lat = stage3_handles[\"teacher_code_latents\"]\n",
    "    denoise_all = stage3_handles[\"denoiser_qnode_all\"]\n",
    "\n",
    "    # ---- init ψ near φ\n",
    "    phi_flat = pnp.array(phi_stage1, requires_grad=False)\n",
    "    psi = pnp.array(np.array(phi_flat) + 0.05*np.random.randn(len(phi_flat)), requires_grad=True)\n",
    "\n",
    "    # ---- loss weights\n",
    "    ALPHA_REC, BETA_TF, GAMMA_TRASH, L_TV, L_ANCH = 1.0, 0.05, 0.5, 0.05, 2e-4\n",
    "    DELTA_TV, DELTA_Z = 0.02, 0.25\n",
    "\n",
    "    # loss on a single window with a specific noise seed\n",
    "    def loss_on_window_seeded(params, clean_values, seed):\n",
    "        v_noisy = pnp.array(ts_add_noise_window_det(clean_values, TARGET_NOISE, seed=seed))\n",
    "        z_all = pnp.array(enc_all(params, v_noisy))\n",
    "        z_sig, z_tr = z_all[:n_latent], z_all[n_latent:]\n",
    "        zD = pnp.array(denoise_all(params, v_noisy))\n",
    "        v_hat = Z_to_values_autograd(zD)\n",
    "\n",
    "        L_rec = pnp.mean((pnp.array(clean_values) - v_hat)**2)\n",
    "        z_t_sig = pnp.array(teacher_lat(clean_values))\n",
    "        L_tf = pnp.mean(huber(z_t_sig - z_sig, DELTA_Z))\n",
    "        L_tr = pnp.mean(p1_from_expZ(z_tr))\n",
    "        L_tv = pnp.mean(huber(first_diff(clean_values) - first_diff(v_hat), DELTA_TV))\n",
    "        L_anchor = pnp.mean((params - phi_flat)**2)\n",
    "        return (ALPHA_REC*L_rec + BETA_TF*L_tf + GAMMA_TRASH*L_tr + L_TV*L_tv + L_ANCH*L_anchor)\n",
    "\n",
    "    # manual Adam\n",
    "    m = pnp.zeros_like(psi); v = pnp.zeros_like(psi)\n",
    "    b1, b2, eps = 0.9, 0.999, 1e-8\n",
    "    t = 0\n",
    "    def adam_step(params, grad, lr):\n",
    "        nonlocal m, v, t\n",
    "        t += 1\n",
    "        m = b1*m + (1-b1)*grad\n",
    "        v = b2*v + (1-b2)*(grad*grad)\n",
    "        mhat = m/(1-b1**t); vhat = v/(1-b2**t)\n",
    "        return params - lr * (mhat/(pnp.sqrt(vhat)+eps))\n",
    "\n",
    "    # batches deterministic per-epoch\n",
    "    def batch_indices(N, B, ep_seed):\n",
    "        rng = np.random.default_rng(ep_seed)\n",
    "        idx = rng.permutation(N)\n",
    "        for s in range(0, N, B):\n",
    "            yield idx[s:s+B]\n",
    "\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve, lr = 0, LR_START\n",
    "    ema = pnp.array(psi, requires_grad=False) if USE_EMA else None\n",
    "\n",
    "    # history buffers (for CSV/reporting)\n",
    "    hist_train, hist_val = [], []\n",
    "    hist_noisy, hist_delta = [], []\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(MAX_EPOCHS):\n",
    "        seed_ep = 88_000 + 100*instance_id + ep  # reproducible shuffling\n",
    "        acc, nb = 0.0, 0\n",
    "        for ix in batch_indices(len(X_train), BATCH, ep_seed=seed_ep):\n",
    "            for k in ix:                                   # k = absolute index in X_train\n",
    "                c = X_train[k]\n",
    "                seed = make_train_seed(instance_id, n_layers, ep, int(k))\n",
    "                L = loss_on_window_seeded(psi, c, seed)\n",
    "                if not pnp.isfinite(L): \n",
    "                    continue\n",
    "                g = qml.grad(lambda p: loss_on_window_seeded(p, c, seed))(psi)\n",
    "                if not pnp.all(pnp.isfinite(g)): \n",
    "                    continue\n",
    "                # clip\n",
    "                gnorm = pnp.linalg.norm(g) + 1e-12\n",
    "                if gnorm > CLIP_NORM:\n",
    "                    g = g * (CLIP_NORM / gnorm)\n",
    "                psi = adam_step(psi, g, lr)\n",
    "                if USE_EMA: \n",
    "                    ema = EMA_DECAY*ema + (1-EMA_DECAY)*psi\n",
    "                acc += float(L); nb += 1\n",
    "\n",
    "        train_loss = acc / max(nb, 1)\n",
    "        eval_params = ema if USE_EMA else psi\n",
    "\n",
    "        # strict value-domain validation at σ=EVAL_SIGMA (deterministic per window)\n",
    "        mN, mD, dV = stage3_val_values_det(eval_params, X_val, sigma=EVAL_SIGMA)\n",
    "        hist_train.append(train_loss); hist_val.append(mD)\n",
    "        hist_noisy.append(mN);        hist_delta.append(dV)\n",
    "\n",
    "        if mD < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = mD, pnp.array(eval_params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % PLATEAU_STEPS) == 0:\n",
    "                lr *= PLATEAU_FACTOR\n",
    "                print(f\"[Stage3] Plateau → LR {lr:.5f}\")\n",
    "\n",
    "        norm_diff = float(pnp.linalg.norm((eval_params - phi_flat)))\n",
    "        print(f\"[Stage3] L={n_layers} ep {ep:03d} | train {train_loss:.5f} | \"\n",
    "              f\"val {mD:.5f} | noisy {mN:.5f} | Δ {dV:+.1f}% | LR {lr:.5f} | ||ψ-φ|| {norm_diff:.3f}\")\n",
    "\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"[Stage3] Early stopping.\"); break\n",
    "\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    epochs_run = len(hist_val)\n",
    "\n",
    "    psi_best = best_params if best_params is not None else (ema if USE_EMA else psi)\n",
    "\n",
    "    return dict(\n",
    "        psi=psi_best, \n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=int(epochs_run),\n",
    "        hist_train=list(map(float, hist_train)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50129aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows built: 95 (W=6, step=1)\n",
      "Split sizes → train=57, val=19, test=19\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# Cell 6 — Build windows & deterministic train/val/test\n",
    "# ===================================================\n",
    "window_size = n_qubits\n",
    "stride = WINDOW_STRIDE\n",
    "\n",
    "X_windows = np.array([y_all[i:i+window_size] for i in range(0, len(y_all)-window_size+1, stride)], dtype=float)\n",
    "print(f\"Total windows built: {len(X_windows)} (W={window_size}, step={stride})\")\n",
    "\n",
    "# 60/20/20 split (deterministic)\n",
    "X_temp, X_test = train_test_split(X_windows, test_size=0.20, random_state=SPLIT_RANDOM_STATE)\n",
    "X_train, X_val = train_test_split(X_temp,   test_size=0.25, random_state=SPLIT_RANDOM_STATE)  # 0.25 of 0.8 = 0.2\n",
    "print(f\"Split sizes → train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6711326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.487434 | val 0.492487 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.485484 | val 0.491245 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.484006 | val 0.489948 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.482430 | val 0.488599 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.478694 | val 0.487204 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.479340 | val 0.485724 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.476426 | val 0.484179 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.475501 | val 0.482546 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.473686 | val 0.480833 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.472411 | val 0.479034 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.467951 | val 0.477172 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.468741 | val 0.475221 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.466663 | val 0.473216 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.462804 | val 0.471141 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.461262 | val 0.468989 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.458004 | val 0.466765 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.456978 | val 0.464476 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.453497 | val 0.462138 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.450732 | val 0.459766 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.450335 | val 0.457311 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.449166 | val 0.454849 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.446134 | val 0.452342 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.442603 | val 0.449862 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.441358 | val 0.447317 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.436127 | val 0.444801 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.436319 | val 0.442279 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.434588 | val 0.439774 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.431648 | val 0.437380 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.430611 | val 0.434960 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.428318 | val 0.432481 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.424231 | val 0.430062 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.422918 | val 0.427678 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.420526 | val 0.425232 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.417559 | val 0.422863 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.416055 | val 0.420493 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.409900 | val 0.418199 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.410715 | val 0.415896 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.413005 | val 0.413673 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.407814 | val 0.411546 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.407316 | val 0.409558 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.404693 | val 0.407543 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.401628 | val 0.405559 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.402882 | val 0.403571 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.397398 | val 0.401567 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.397703 | val 0.399650 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.396708 | val 0.397753 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.396713 | val 0.395844 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.388876 | val 0.393983 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.390930 | val 0.392273 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.388502 | val 0.390552 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.387871 | val 0.388836 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.384021 | val 0.387181 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.387178 | val 0.385652 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.380703 | val 0.384076 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.377129 | val 0.382581 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.375382 | val 0.381079 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.376248 | val 0.379587 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.372600 | val 0.378160 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.376607 | val 0.376733 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.370307 | val 0.375348 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.364756 | val 0.373941 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.370373 | val 0.372566 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.366210 | val 0.371147 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.361943 | val 0.369683 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.364325 | val 0.368233 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.363166 | val 0.366743 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.363926 | val 0.365219 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.357642 | val 0.363664 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.352833 | val 0.362050 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.353643 | val 0.360506 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.352116 | val 0.358874 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.345681 | val 0.357124 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.351705 | val 0.355280 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.348518 | val 0.353399 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.342286 | val 0.351461 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.340676 | val 0.349386 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.338169 | val 0.347184 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.340398 | val 0.345001 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.339415 | val 0.342656 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.333364 | val 0.340338 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.334869 | val 0.337891 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.330008 | val 0.335287 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.328721 | val 0.332550 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.331603 | val 0.329858 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.327854 | val 0.327123 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.327391 | val 0.324300 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.319857 | val 0.321270 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.320119 | val 0.318149 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.316973 | val 0.315086 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.316285 | val 0.311823 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.314789 | val 0.308611 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.314511 | val 0.305474 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.311680 | val 0.302309 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.309068 | val 0.298965 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.307852 | val 0.295633 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.309746 | val 0.292321 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.302896 | val 0.288969 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.303244 | val 0.285824 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.302569 | val 0.282598 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.300654 | val 0.279311 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.300854 | val 0.276212 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.291158 | val 0.273297 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.296822 | val 0.270407 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.295509 | val 0.267649 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.290666 | val 0.264947 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.289686 | val 0.262320 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.294674 | val 0.259871 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.289161 | val 0.257630 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.289538 | val 0.255453 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.287070 | val 0.253366 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.287734 | val 0.251428 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.287367 | val 0.249572 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.284587 | val 0.247896 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.282765 | val 0.246238 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.284061 | val 0.244766 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.286792 | val 0.243351 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.282054 | val 0.242055 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.284948 | val 0.240859 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.284515 | val 0.239838 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.287299 | val 0.238915 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.16095 | val 0.00338 | noisy 0.00347 | Δ +2.8% | LR 0.00300 | ||ψ-φ|| 0.202\n",
      "[Stage3] L=1 ep 001 | train 0.16983 | val 0.00335 | noisy 0.00347 | Δ +3.6% | LR 0.00300 | ||ψ-φ|| 0.255\n",
      "[Stage3] L=1 ep 002 | train 0.17059 | val 0.00331 | noisy 0.00347 | Δ +4.9% | LR 0.00300 | ||ψ-φ|| 0.349\n",
      "[Stage3] L=1 ep 003 | train 0.17107 | val 0.00322 | noisy 0.00347 | Δ +7.4% | LR 0.00300 | ||ψ-φ|| 0.456\n",
      "[Stage3] L=1 ep 004 | train 0.16588 | val 0.00315 | noisy 0.00347 | Δ +9.4% | LR 0.00300 | ||ψ-φ|| 0.558\n",
      "[Stage3] L=1 ep 005 | train 0.16434 | val 0.00308 | noisy 0.00347 | Δ +11.2% | LR 0.00300 | ||ψ-φ|| 0.661\n",
      "[Stage3] L=1 ep 006 | train 0.16707 | val 0.00302 | noisy 0.00347 | Δ +13.0% | LR 0.00300 | ||ψ-φ|| 0.760\n",
      "[Stage3] L=1 ep 007 | train 0.16814 | val 0.00298 | noisy 0.00347 | Δ +14.3% | LR 0.00300 | ||ψ-φ|| 0.838\n",
      "[Stage3] L=1 ep 008 | train 0.16675 | val 0.00296 | noisy 0.00347 | Δ +14.9% | LR 0.00300 | ||ψ-φ|| 0.897\n",
      "[Stage3] L=1 ep 009 | train 0.16220 | val 0.00294 | noisy 0.00347 | Δ +15.4% | LR 0.00300 | ||ψ-φ|| 0.951\n",
      "[Stage3] L=1 ep 010 | train 0.16501 | val 0.00295 | noisy 0.00347 | Δ +15.0% | LR 0.00300 | ||ψ-φ|| 0.995\n",
      "[Stage3] L=1 ep 011 | train 0.15606 | val 0.00296 | noisy 0.00347 | Δ +14.9% | LR 0.00300 | ||ψ-φ|| 1.022\n",
      "[Stage3] L=1 ep 012 | train 0.16838 | val 0.00297 | noisy 0.00347 | Δ +14.5% | LR 0.00300 | ||ψ-φ|| 1.050\n",
      "[Stage3] L=1 ep 013 | train 0.16932 | val 0.00299 | noisy 0.00347 | Δ +14.0% | LR 0.00300 | ||ψ-φ|| 1.079\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 014 | train 0.16192 | val 0.00299 | noisy 0.00347 | Δ +13.8% | LR 0.00150 | ||ψ-φ|| 1.113\n",
      "[Stage3] L=1 ep 015 | train 0.16736 | val 0.00300 | noisy 0.00347 | Δ +13.6% | LR 0.00150 | ||ψ-φ|| 1.138\n",
      "[Stage3] L=1 ep 016 | train 0.16320 | val 0.00299 | noisy 0.00347 | Δ +13.9% | LR 0.00150 | ||ψ-φ|| 1.155\n",
      "[Stage3] L=1 ep 017 | train 0.16472 | val 0.00299 | noisy 0.00347 | Δ +14.1% | LR 0.00150 | ||ψ-φ|| 1.164\n",
      "[Stage3] L=1 ep 018 | train 0.16502 | val 0.00296 | noisy 0.00347 | Δ +14.7% | LR 0.00150 | ||ψ-φ|| 1.170\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 019 | train 0.16393 | val 0.00296 | noisy 0.00347 | Δ +14.9% | LR 0.00075 | ||ψ-φ|| 1.173\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.499447 | val 0.507954 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.498157 | val 0.506386 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.497780 | val 0.504866 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.494916 | val 0.503320 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.491893 | val 0.501743 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.491389 | val 0.500165 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.488027 | val 0.498569 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.486207 | val 0.496916 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.483477 | val 0.495212 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.482887 | val 0.493429 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.479599 | val 0.491587 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.477018 | val 0.489659 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.475183 | val 0.487637 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.472739 | val 0.485535 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.470222 | val 0.483361 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.467060 | val 0.481100 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.464094 | val 0.478751 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.461962 | val 0.476326 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.458394 | val 0.473820 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.456254 | val 0.471241 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.455284 | val 0.468570 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.452114 | val 0.465870 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.452114 | val 0.463111 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.447784 | val 0.460306 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.442800 | val 0.457434 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.439965 | val 0.454550 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.443903 | val 0.451655 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.439122 | val 0.448774 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.433225 | val 0.445855 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.429046 | val 0.442949 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.426561 | val 0.440058 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.425553 | val 0.437170 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.423048 | val 0.434341 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.424325 | val 0.431503 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.416522 | val 0.428747 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.417204 | val 0.426003 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.413746 | val 0.423293 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.405921 | val 0.420674 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.408598 | val 0.418140 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.405487 | val 0.415673 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.405827 | val 0.413238 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.403407 | val 0.410867 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.398304 | val 0.408527 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.399728 | val 0.406240 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.398063 | val 0.404027 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.395275 | val 0.401883 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.398198 | val 0.399792 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.391409 | val 0.397757 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.387433 | val 0.395778 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.389984 | val 0.393844 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.384970 | val 0.391941 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.381054 | val 0.390108 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.384087 | val 0.388284 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.381923 | val 0.386572 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.378011 | val 0.384821 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.377698 | val 0.383144 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.366931 | val 0.381496 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.376138 | val 0.379865 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.370204 | val 0.378256 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.365197 | val 0.376639 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.366426 | val 0.375143 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.365019 | val 0.373609 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.366190 | val 0.372052 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.359158 | val 0.370487 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.364968 | val 0.368921 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.359101 | val 0.367294 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.358845 | val 0.365637 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.357152 | val 0.363959 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.353789 | val 0.362239 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.351892 | val 0.360423 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.348024 | val 0.358568 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.347975 | val 0.356664 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.346962 | val 0.354659 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.348331 | val 0.352633 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.342651 | val 0.350532 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.341082 | val 0.348287 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.342424 | val 0.345963 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.337610 | val 0.343515 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.341610 | val 0.341082 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.334991 | val 0.338584 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.333411 | val 0.335951 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.328527 | val 0.333121 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.329112 | val 0.330258 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.328913 | val 0.327381 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.324195 | val 0.324289 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.321512 | val 0.321275 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.321240 | val 0.318225 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.319239 | val 0.315038 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.316839 | val 0.311795 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.315498 | val 0.308462 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.313898 | val 0.305010 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.313795 | val 0.301738 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.310554 | val 0.298436 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.312558 | val 0.295167 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.309099 | val 0.291869 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.304300 | val 0.288581 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.305601 | val 0.285181 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.306474 | val 0.281938 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.299251 | val 0.278553 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.300428 | val 0.275493 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.296188 | val 0.272391 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.297583 | val 0.269344 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.296886 | val 0.266453 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.293226 | val 0.263757 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.291255 | val 0.261155 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.287936 | val 0.258708 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.293222 | val 0.256323 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.292544 | val 0.254060 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.288719 | val 0.252016 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.288264 | val 0.250092 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.284529 | val 0.248362 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.288844 | val 0.246669 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.282919 | val 0.245091 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.288006 | val 0.243568 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.285819 | val 0.242214 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.285328 | val 0.240947 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.283040 | val 0.239836 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.285331 | val 0.238833 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.288646 | val 0.237840 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.287508 | val 0.237044 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.15939 | val 0.00355 | noisy 0.00347 | Δ -2.1% | LR 0.00300 | ||ψ-φ|| 0.199\n",
      "[Stage3] L=1 ep 001 | train 0.16184 | val 0.00350 | noisy 0.00347 | Δ -0.8% | LR 0.00300 | ||ψ-φ|| 0.238\n",
      "[Stage3] L=1 ep 002 | train 0.16865 | val 0.00342 | noisy 0.00347 | Δ +1.6% | LR 0.00300 | ||ψ-φ|| 0.319\n",
      "[Stage3] L=1 ep 003 | train 0.16536 | val 0.00330 | noisy 0.00347 | Δ +5.0% | LR 0.00300 | ||ψ-φ|| 0.428\n",
      "[Stage3] L=1 ep 004 | train 0.16374 | val 0.00319 | noisy 0.00347 | Δ +8.2% | LR 0.00300 | ||ψ-φ|| 0.537\n",
      "[Stage3] L=1 ep 005 | train 0.16789 | val 0.00311 | noisy 0.00347 | Δ +10.5% | LR 0.00300 | ||ψ-φ|| 0.644\n",
      "[Stage3] L=1 ep 006 | train 0.16345 | val 0.00306 | noisy 0.00347 | Δ +12.0% | LR 0.00300 | ||ψ-φ|| 0.730\n",
      "[Stage3] L=1 ep 007 | train 0.16156 | val 0.00303 | noisy 0.00347 | Δ +12.8% | LR 0.00300 | ||ψ-φ|| 0.809\n",
      "[Stage3] L=1 ep 008 | train 0.16334 | val 0.00299 | noisy 0.00347 | Δ +14.1% | LR 0.00300 | ||ψ-φ|| 0.867\n",
      "[Stage3] L=1 ep 009 | train 0.16104 | val 0.00293 | noisy 0.00347 | Δ +15.6% | LR 0.00300 | ||ψ-φ|| 0.914\n",
      "[Stage3] L=1 ep 010 | train 0.17273 | val 0.00289 | noisy 0.00347 | Δ +16.8% | LR 0.00300 | ||ψ-φ|| 0.959\n",
      "[Stage3] L=1 ep 011 | train 0.17143 | val 0.00284 | noisy 0.00347 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 1.010\n",
      "[Stage3] L=1 ep 012 | train 0.16571 | val 0.00285 | noisy 0.00347 | Δ +18.0% | LR 0.00300 | ||ψ-φ|| 1.039\n",
      "[Stage3] L=1 ep 013 | train 0.16542 | val 0.00286 | noisy 0.00347 | Δ +17.5% | LR 0.00300 | ||ψ-φ|| 1.067\n",
      "[Stage3] L=1 ep 014 | train 0.16171 | val 0.00288 | noisy 0.00347 | Δ +17.1% | LR 0.00300 | ||ψ-φ|| 1.092\n",
      "[Stage3] L=1 ep 015 | train 0.16176 | val 0.00289 | noisy 0.00347 | Δ +16.8% | LR 0.00300 | ||ψ-φ|| 1.101\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 016 | train 0.16296 | val 0.00288 | noisy 0.00347 | Δ +17.2% | LR 0.00150 | ||ψ-φ|| 1.110\n",
      "[Stage3] L=1 ep 017 | train 0.16393 | val 0.00286 | noisy 0.00347 | Δ +17.6% | LR 0.00150 | ||ψ-φ|| 1.117\n",
      "[Stage3] L=1 ep 018 | train 0.15927 | val 0.00286 | noisy 0.00347 | Δ +17.7% | LR 0.00150 | ||ψ-φ|| 1.121\n",
      "[Stage3] L=1 ep 019 | train 0.16814 | val 0.00285 | noisy 0.00347 | Δ +17.9% | LR 0.00150 | ||ψ-φ|| 1.126\n",
      "[Stage3] L=1 ep 020 | train 0.15965 | val 0.00285 | noisy 0.00347 | Δ +18.1% | LR 0.00150 | ||ψ-φ|| 1.132\n",
      "[Stage3] L=1 ep 021 | train 0.15904 | val 0.00284 | noisy 0.00347 | Δ +18.2% | LR 0.00150 | ||ψ-φ|| 1.130\n",
      "[Stage3] L=1 ep 022 | train 0.16480 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00150 | ||ψ-φ|| 1.133\n",
      "[Stage3] L=1 ep 023 | train 0.15951 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00150 | ||ψ-φ|| 1.131\n",
      "[Stage3] L=1 ep 024 | train 0.16840 | val 0.00284 | noisy 0.00347 | Δ +18.1% | LR 0.00150 | ||ψ-φ|| 1.132\n",
      "[Stage3] L=1 ep 025 | train 0.16403 | val 0.00286 | noisy 0.00347 | Δ +17.7% | LR 0.00150 | ||ψ-φ|| 1.138\n",
      "[Stage3] L=1 ep 026 | train 0.15950 | val 0.00286 | noisy 0.00347 | Δ +17.7% | LR 0.00150 | ||ψ-φ|| 1.140\n",
      "[Stage3] L=1 ep 027 | train 0.16890 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00150 | ||ψ-φ|| 1.142\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 028 | train 0.15515 | val 0.00288 | noisy 0.00347 | Δ +17.1% | LR 0.00075 | ||ψ-φ|| 1.145\n",
      "[Stage3] L=1 ep 029 | train 0.16458 | val 0.00290 | noisy 0.00347 | Δ +16.5% | LR 0.00075 | ||ψ-φ|| 1.147\n",
      "[Stage3] L=1 ep 030 | train 0.15863 | val 0.00292 | noisy 0.00347 | Δ +16.1% | LR 0.00075 | ||ψ-φ|| 1.150\n",
      "[Stage3] L=1 ep 031 | train 0.16409 | val 0.00292 | noisy 0.00347 | Δ +15.9% | LR 0.00075 | ||ψ-φ|| 1.154\n",
      "[Stage3] L=1 ep 032 | train 0.16577 | val 0.00293 | noisy 0.00347 | Δ +15.7% | LR 0.00075 | ||ψ-φ|| 1.157\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=1 ep 033 | train 0.15628 | val 0.00294 | noisy 0.00347 | Δ +15.5% | LR 0.00038 | ||ψ-φ|| 1.159\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.487872 | val 0.494599 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.486283 | val 0.492449 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.484046 | val 0.490268 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.483449 | val 0.488071 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.478442 | val 0.485851 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.476070 | val 0.483583 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.472511 | val 0.481254 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.471806 | val 0.478857 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.469843 | val 0.476382 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.466498 | val 0.473805 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.461819 | val 0.471130 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.462204 | val 0.468218 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.457183 | val 0.465159 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.455263 | val 0.461849 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.449941 | val 0.458323 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.447709 | val 0.454508 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.445027 | val 0.450392 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.439937 | val 0.446004 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.436022 | val 0.441327 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.432778 | val 0.436408 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.426620 | val 0.431250 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.422406 | val 0.425839 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.417158 | val 0.420231 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.410863 | val 0.414497 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.408353 | val 0.408623 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.400842 | val 0.402648 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.394964 | val 0.396632 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.389471 | val 0.390560 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.386063 | val 0.384471 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.382969 | val 0.378374 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.370337 | val 0.372367 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.369365 | val 0.366434 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.363257 | val 0.360578 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.357117 | val 0.354875 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.354777 | val 0.349285 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.348748 | val 0.343877 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.350847 | val 0.338658 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.342956 | val 0.333747 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.339787 | val 0.329048 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.334135 | val 0.324607 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.330914 | val 0.320391 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.323172 | val 0.316441 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.325533 | val 0.312682 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.318747 | val 0.309156 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.318285 | val 0.305839 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.319230 | val 0.302758 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.313935 | val 0.299749 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.311178 | val 0.296850 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.312046 | val 0.294087 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.313670 | val 0.291406 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.305680 | val 0.288894 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.307186 | val 0.286446 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.302405 | val 0.283989 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.305474 | val 0.281608 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.298595 | val 0.279271 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.302369 | val 0.276897 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.302999 | val 0.274585 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.294929 | val 0.272363 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.300425 | val 0.270157 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.298579 | val 0.267968 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.294707 | val 0.265917 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.301241 | val 0.263944 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.293233 | val 0.262014 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.290106 | val 0.260149 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.288922 | val 0.258304 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.290691 | val 0.256445 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.289240 | val 0.254676 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.292013 | val 0.253106 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.289886 | val 0.251478 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.289981 | val 0.249967 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.290404 | val 0.248400 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.283635 | val 0.247004 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.289840 | val 0.245624 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.287742 | val 0.244286 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.292760 | val 0.243004 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.288362 | val 0.241924 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.291052 | val 0.240896 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.289463 | val 0.239876 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.285552 | val 0.239030 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.279957 | val 0.238240 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.284990 | val 0.237435 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.283178 | val 0.236754 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.286142 | val 0.236204 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.285270 | val 0.235697 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.288631 | val 0.235196 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.281738 | val 0.234769 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.282771 | val 0.234342 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.283153 | val 0.233956 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.283484 | val 0.233597 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.282010 | val 0.233301 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.288299 | val 0.233056 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.280167 | val 0.232787 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.283966 | val 0.232520 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.282354 | val 0.232274 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.286443 | val 0.232099 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.285856 | val 0.232021 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.281014 | val 0.231917 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.283327 | val 0.231774 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.282299 | val 0.231694 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.287603 | val 0.231568 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.281986 | val 0.231473 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.281073 | val 0.231364 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.278728 | val 0.231301 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.282169 | val 0.231215 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.284748 | val 0.231154 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.282652 | val 0.231093 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.283421 | val 0.231052 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.279825 | val 0.231004 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.283567 | val 0.230916 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.284157 | val 0.230852 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.278205 | val 0.230807 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.284070 | val 0.230686 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.284390 | val 0.230610 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.280859 | val 0.230534 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.284933 | val 0.230527 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.280108 | val 0.230525 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.281992 | val 0.230526 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.281144 | val 0.230565 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.283900 | val 0.230600 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.281808 | val 0.230658 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.17085 | val 0.00326 | noisy 0.00347 | Δ +6.1% | LR 0.00300 | ||ψ-φ|| 0.213\n",
      "[Stage3] L=1 ep 001 | train 0.16278 | val 0.00323 | noisy 0.00347 | Δ +7.0% | LR 0.00300 | ||ψ-φ|| 0.251\n",
      "[Stage3] L=1 ep 002 | train 0.17074 | val 0.00318 | noisy 0.00347 | Δ +8.6% | LR 0.00300 | ||ψ-φ|| 0.326\n",
      "[Stage3] L=1 ep 003 | train 0.16698 | val 0.00311 | noisy 0.00347 | Δ +10.6% | LR 0.00300 | ||ψ-φ|| 0.433\n",
      "[Stage3] L=1 ep 004 | train 0.16479 | val 0.00303 | noisy 0.00347 | Δ +12.7% | LR 0.00300 | ||ψ-φ|| 0.547\n",
      "[Stage3] L=1 ep 005 | train 0.17289 | val 0.00299 | noisy 0.00347 | Δ +14.1% | LR 0.00300 | ||ψ-φ|| 0.640\n",
      "[Stage3] L=1 ep 006 | train 0.16493 | val 0.00293 | noisy 0.00347 | Δ +15.7% | LR 0.00300 | ||ψ-φ|| 0.729\n",
      "[Stage3] L=1 ep 007 | train 0.15327 | val 0.00291 | noisy 0.00347 | Δ +16.4% | LR 0.00300 | ||ψ-φ|| 0.785\n",
      "[Stage3] L=1 ep 008 | train 0.16517 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00300 | ||ψ-φ|| 0.837\n",
      "[Stage3] L=1 ep 009 | train 0.16540 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00300 | ||ψ-φ|| 0.885\n",
      "[Stage3] L=1 ep 010 | train 0.16741 | val 0.00283 | noisy 0.00347 | Δ +18.7% | LR 0.00300 | ||ψ-φ|| 0.935\n",
      "[Stage3] L=1 ep 011 | train 0.16453 | val 0.00283 | noisy 0.00347 | Δ +18.6% | LR 0.00300 | ||ψ-φ|| 0.976\n",
      "[Stage3] L=1 ep 012 | train 0.16330 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00300 | ||ψ-φ|| 1.020\n",
      "[Stage3] L=1 ep 013 | train 0.16799 | val 0.00282 | noisy 0.00347 | Δ +19.0% | LR 0.00300 | ||ψ-φ|| 1.059\n",
      "[Stage3] L=1 ep 014 | train 0.16612 | val 0.00282 | noisy 0.00347 | Δ +19.0% | LR 0.00300 | ||ψ-φ|| 1.084\n",
      "[Stage3] L=1 ep 015 | train 0.16461 | val 0.00281 | noisy 0.00347 | Δ +19.0% | LR 0.00300 | ||ψ-φ|| 1.084\n",
      "[Stage3] L=1 ep 016 | train 0.15560 | val 0.00280 | noisy 0.00347 | Δ +19.5% | LR 0.00300 | ||ψ-φ|| 1.092\n",
      "[Stage3] L=1 ep 017 | train 0.16619 | val 0.00279 | noisy 0.00347 | Δ +19.6% | LR 0.00300 | ||ψ-φ|| 1.094\n",
      "[Stage3] L=1 ep 018 | train 0.15927 | val 0.00281 | noisy 0.00347 | Δ +19.2% | LR 0.00300 | ||ψ-φ|| 1.089\n",
      "[Stage3] L=1 ep 019 | train 0.16097 | val 0.00281 | noisy 0.00347 | Δ +19.1% | LR 0.00300 | ||ψ-φ|| 1.093\n",
      "[Stage3] L=1 ep 020 | train 0.16228 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00300 | ||ψ-φ|| 1.095\n",
      "[Stage3] L=1 ep 021 | train 0.16790 | val 0.00285 | noisy 0.00347 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 1.098\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 022 | train 0.16770 | val 0.00286 | noisy 0.00347 | Δ +17.8% | LR 0.00150 | ||ψ-φ|| 1.102\n",
      "[Stage3] L=1 ep 023 | train 0.17114 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00150 | ||ψ-φ|| 1.110\n",
      "[Stage3] L=1 ep 024 | train 0.15807 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00150 | ||ψ-φ|| 1.115\n",
      "[Stage3] L=1 ep 025 | train 0.15976 | val 0.00286 | noisy 0.00347 | Δ +17.7% | LR 0.00150 | ||ψ-φ|| 1.116\n",
      "[Stage3] L=1 ep 026 | train 0.16389 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00150 | ||ψ-φ|| 1.110\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 027 | train 0.16714 | val 0.00286 | noisy 0.00347 | Δ +17.6% | LR 0.00075 | ||ψ-φ|| 1.115\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.486988 | val 0.489703 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.485993 | val 0.488586 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.484290 | val 0.487393 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.483641 | val 0.486124 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.480678 | val 0.484801 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.479474 | val 0.483409 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.478595 | val 0.481929 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.475988 | val 0.480393 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.475411 | val 0.478796 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.473167 | val 0.477134 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.472385 | val 0.475418 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.469715 | val 0.473670 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.467395 | val 0.471873 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.467598 | val 0.470003 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.464178 | val 0.468090 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.464762 | val 0.466105 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.462071 | val 0.464069 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.458373 | val 0.461990 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.455983 | val 0.459915 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.455355 | val 0.457749 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.452257 | val 0.455535 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.450243 | val 0.453328 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.449721 | val 0.451120 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.447987 | val 0.448849 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.442515 | val 0.446578 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.440911 | val 0.444276 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.437740 | val 0.441963 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.437485 | val 0.439613 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.437265 | val 0.437336 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.436189 | val 0.434963 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.429258 | val 0.432653 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.426413 | val 0.430358 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.424854 | val 0.428024 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.423417 | val 0.425767 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.421285 | val 0.423558 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.421795 | val 0.421313 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.416304 | val 0.419145 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.416362 | val 0.416981 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.408750 | val 0.414932 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.408740 | val 0.412916 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.412045 | val 0.410910 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.406504 | val 0.408928 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.402303 | val 0.406954 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.407063 | val 0.404959 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.401603 | val 0.402951 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.397573 | val 0.401071 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.395217 | val 0.399169 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.390328 | val 0.397358 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.396531 | val 0.395570 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.388380 | val 0.393762 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.388166 | val 0.392059 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.392086 | val 0.390470 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.388705 | val 0.388852 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.383482 | val 0.387290 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.384006 | val 0.385721 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.383201 | val 0.384234 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.378866 | val 0.382758 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.381493 | val 0.381224 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.375530 | val 0.379713 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.372909 | val 0.378211 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.370958 | val 0.376690 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.372473 | val 0.375191 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.372597 | val 0.373690 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.367189 | val 0.372175 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.368932 | val 0.370653 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.366809 | val 0.369115 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.362450 | val 0.367570 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.358100 | val 0.365960 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.359737 | val 0.364310 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.356441 | val 0.362584 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.350983 | val 0.360832 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.356040 | val 0.359049 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.351756 | val 0.357218 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.351810 | val 0.355306 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.343734 | val 0.353309 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.345411 | val 0.351212 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.341775 | val 0.349024 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.336417 | val 0.346787 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.339602 | val 0.344354 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.335564 | val 0.342014 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.339333 | val 0.339468 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.332098 | val 0.336745 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.332433 | val 0.334010 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.329346 | val 0.331148 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.328767 | val 0.328252 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.324235 | val 0.325346 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.322160 | val 0.322236 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.323424 | val 0.319098 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.318113 | val 0.315932 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.317509 | val 0.312650 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.315089 | val 0.309515 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.314017 | val 0.306346 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.314962 | val 0.303024 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.309625 | val 0.299709 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.308797 | val 0.296360 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.303540 | val 0.293080 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.304998 | val 0.289755 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.303428 | val 0.286475 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.303415 | val 0.283388 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.301252 | val 0.280311 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.297975 | val 0.277262 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.295806 | val 0.274421 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.297907 | val 0.271656 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.290761 | val 0.268953 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.294862 | val 0.266295 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.294453 | val 0.263797 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.292318 | val 0.261334 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.291970 | val 0.258918 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.292508 | val 0.256721 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.291215 | val 0.254618 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.292363 | val 0.252693 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.284723 | val 0.250890 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.286935 | val 0.249111 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.291053 | val 0.247486 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.287867 | val 0.245945 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.287313 | val 0.244581 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.280583 | val 0.243304 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.282374 | val 0.242120 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.289037 | val 0.241077 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.282734 | val 0.240131 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.16905 | val 0.00350 | noisy 0.00347 | Δ -0.8% | LR 0.00300 | ||ψ-φ|| 0.208\n",
      "[Stage3] L=1 ep 001 | train 0.16527 | val 0.00345 | noisy 0.00347 | Δ +0.8% | LR 0.00300 | ||ψ-φ|| 0.254\n",
      "[Stage3] L=1 ep 002 | train 0.16165 | val 0.00336 | noisy 0.00347 | Δ +3.3% | LR 0.00300 | ||ψ-φ|| 0.330\n",
      "[Stage3] L=1 ep 003 | train 0.17269 | val 0.00325 | noisy 0.00347 | Δ +6.5% | LR 0.00300 | ||ψ-φ|| 0.429\n",
      "[Stage3] L=1 ep 004 | train 0.16600 | val 0.00317 | noisy 0.00347 | Δ +8.9% | LR 0.00300 | ||ψ-φ|| 0.527\n",
      "[Stage3] L=1 ep 005 | train 0.15470 | val 0.00310 | noisy 0.00347 | Δ +10.9% | LR 0.00300 | ||ψ-φ|| 0.610\n",
      "[Stage3] L=1 ep 006 | train 0.16087 | val 0.00305 | noisy 0.00347 | Δ +12.4% | LR 0.00300 | ||ψ-φ|| 0.684\n",
      "[Stage3] L=1 ep 007 | train 0.15851 | val 0.00300 | noisy 0.00347 | Δ +13.6% | LR 0.00300 | ||ψ-φ|| 0.754\n",
      "[Stage3] L=1 ep 008 | train 0.16206 | val 0.00299 | noisy 0.00347 | Δ +14.0% | LR 0.00300 | ||ψ-φ|| 0.817\n",
      "[Stage3] L=1 ep 009 | train 0.16391 | val 0.00299 | noisy 0.00347 | Δ +13.8% | LR 0.00300 | ||ψ-φ|| 0.875\n",
      "[Stage3] L=1 ep 010 | train 0.16111 | val 0.00299 | noisy 0.00347 | Δ +13.8% | LR 0.00300 | ||ψ-φ|| 0.936\n",
      "[Stage3] L=1 ep 011 | train 0.16407 | val 0.00301 | noisy 0.00347 | Δ +13.5% | LR 0.00300 | ||ψ-φ|| 0.983\n",
      "[Stage3] L=1 ep 012 | train 0.15758 | val 0.00302 | noisy 0.00347 | Δ +13.0% | LR 0.00300 | ||ψ-φ|| 1.011\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 013 | train 0.16090 | val 0.00304 | noisy 0.00347 | Δ +12.6% | LR 0.00150 | ||ψ-φ|| 1.031\n",
      "[Stage3] L=1 ep 014 | train 0.16112 | val 0.00303 | noisy 0.00347 | Δ +12.7% | LR 0.00150 | ||ψ-φ|| 1.055\n",
      "[Stage3] L=1 ep 015 | train 0.16269 | val 0.00302 | noisy 0.00347 | Δ +13.0% | LR 0.00150 | ||ψ-φ|| 1.076\n",
      "[Stage3] L=1 ep 016 | train 0.16634 | val 0.00301 | noisy 0.00347 | Δ +13.4% | LR 0.00150 | ||ψ-φ|| 1.097\n",
      "[Stage3] L=1 ep 017 | train 0.16162 | val 0.00300 | noisy 0.00347 | Δ +13.6% | LR 0.00150 | ||ψ-φ|| 1.111\n",
      "[Stage3] L=1 ep 018 | train 0.16498 | val 0.00298 | noisy 0.00347 | Δ +14.2% | LR 0.00150 | ||ψ-φ|| 1.126\n",
      "[Stage3] L=1 ep 019 | train 0.16105 | val 0.00297 | noisy 0.00347 | Δ +14.4% | LR 0.00150 | ||ψ-φ|| 1.134\n",
      "[Stage3] L=1 ep 020 | train 0.16960 | val 0.00297 | noisy 0.00347 | Δ +14.5% | LR 0.00150 | ||ψ-φ|| 1.143\n",
      "[Stage3] L=1 ep 021 | train 0.16259 | val 0.00296 | noisy 0.00347 | Δ +14.9% | LR 0.00150 | ||ψ-φ|| 1.151\n",
      "[Stage3] L=1 ep 022 | train 0.16219 | val 0.00295 | noisy 0.00347 | Δ +15.0% | LR 0.00150 | ||ψ-φ|| 1.154\n",
      "[Stage3] L=1 ep 023 | train 0.16056 | val 0.00296 | noisy 0.00347 | Δ +14.9% | LR 0.00150 | ||ψ-φ|| 1.151\n",
      "[Stage3] L=1 ep 024 | train 0.16266 | val 0.00295 | noisy 0.00347 | Δ +15.2% | LR 0.00150 | ||ψ-φ|| 1.151\n",
      "[Stage3] L=1 ep 025 | train 0.17240 | val 0.00293 | noisy 0.00347 | Δ +15.6% | LR 0.00150 | ||ψ-φ|| 1.152\n",
      "[Stage3] L=1 ep 026 | train 0.16186 | val 0.00292 | noisy 0.00347 | Δ +16.0% | LR 0.00150 | ||ψ-φ|| 1.157\n",
      "[Stage3] L=1 ep 027 | train 0.16601 | val 0.00292 | noisy 0.00347 | Δ +16.0% | LR 0.00150 | ||ψ-φ|| 1.164\n",
      "[Stage3] L=1 ep 028 | train 0.16463 | val 0.00292 | noisy 0.00347 | Δ +15.9% | LR 0.00150 | ||ψ-φ|| 1.164\n",
      "[Stage3] L=1 ep 029 | train 0.15715 | val 0.00292 | noisy 0.00347 | Δ +16.0% | LR 0.00150 | ||ψ-φ|| 1.167\n",
      "[Stage3] L=1 ep 030 | train 0.15792 | val 0.00293 | noisy 0.00347 | Δ +15.6% | LR 0.00150 | ||ψ-φ|| 1.166\n",
      "[Stage3] L=1 ep 031 | train 0.16848 | val 0.00294 | noisy 0.00347 | Δ +15.5% | LR 0.00150 | ||ψ-φ|| 1.167\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 032 | train 0.16228 | val 0.00294 | noisy 0.00347 | Δ +15.5% | LR 0.00075 | ||ψ-φ|| 1.173\n",
      "[Stage3] L=1 ep 033 | train 0.16332 | val 0.00293 | noisy 0.00347 | Δ +15.6% | LR 0.00075 | ||ψ-φ|| 1.176\n",
      "[Stage3] L=1 ep 034 | train 0.16343 | val 0.00293 | noisy 0.00347 | Δ +15.7% | LR 0.00075 | ||ψ-φ|| 1.180\n",
      "[Stage3] L=1 ep 035 | train 0.16908 | val 0.00292 | noisy 0.00347 | Δ +15.9% | LR 0.00075 | ||ψ-φ|| 1.184\n",
      "[Stage3] L=1 ep 036 | train 0.16093 | val 0.00292 | noisy 0.00347 | Δ +15.9% | LR 0.00075 | ||ψ-φ|| 1.188\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=1 ep 037 | train 0.15960 | val 0.00293 | noisy 0.00347 | Δ +15.7% | LR 0.00038 | ||ψ-φ|| 1.191\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.476066 | val 0.484584 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.474744 | val 0.482783 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.471802 | val 0.480895 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.470874 | val 0.478901 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.467343 | val 0.476792 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.464358 | val 0.474573 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.463274 | val 0.472268 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.460085 | val 0.469914 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.456212 | val 0.467446 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.454589 | val 0.464878 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.450436 | val 0.462279 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.448016 | val 0.459572 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.446789 | val 0.456779 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.441753 | val 0.453930 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.439619 | val 0.451010 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.436910 | val 0.448031 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.436893 | val 0.444977 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.432047 | val 0.441898 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.426087 | val 0.438834 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.425459 | val 0.435766 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.421992 | val 0.432703 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.420440 | val 0.429727 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.416779 | val 0.426683 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.416473 | val 0.423741 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.412865 | val 0.420871 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.412711 | val 0.417962 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.407593 | val 0.415095 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.408870 | val 0.412296 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.403826 | val 0.409519 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.397810 | val 0.406791 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.397523 | val 0.404121 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.398164 | val 0.401563 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.394233 | val 0.399035 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.390268 | val 0.396635 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.386995 | val 0.394281 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.384190 | val 0.391969 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.385230 | val 0.389762 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.382556 | val 0.387599 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.380418 | val 0.385495 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.379783 | val 0.383510 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.378358 | val 0.381643 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.375796 | val 0.379822 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.376581 | val 0.378004 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.368235 | val 0.376252 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.369569 | val 0.374473 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.370031 | val 0.372710 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.364893 | val 0.370968 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.362081 | val 0.369238 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.362537 | val 0.367560 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.361561 | val 0.365873 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.355059 | val 0.364166 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.357861 | val 0.362413 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.352852 | val 0.360595 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.352873 | val 0.358770 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.354784 | val 0.356880 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.347142 | val 0.354979 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.347375 | val 0.352984 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.345296 | val 0.350864 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.341939 | val 0.348753 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.343298 | val 0.346528 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.343423 | val 0.344215 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.333199 | val 0.341831 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.335117 | val 0.339259 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.329533 | val 0.336605 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.329665 | val 0.333829 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.329125 | val 0.331103 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.328465 | val 0.328182 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.325803 | val 0.325094 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.323073 | val 0.321885 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.320232 | val 0.318704 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.318519 | val 0.315405 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.320329 | val 0.312075 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.317235 | val 0.308688 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.312440 | val 0.305181 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.313727 | val 0.301812 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.309154 | val 0.298402 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.308938 | val 0.294937 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.305818 | val 0.291502 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.307268 | val 0.288042 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.303450 | val 0.284639 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.302547 | val 0.281216 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.300711 | val 0.277975 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.298191 | val 0.274877 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.299261 | val 0.271804 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.295135 | val 0.268836 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.291111 | val 0.265895 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.293775 | val 0.263083 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.293639 | val 0.260393 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.290770 | val 0.257958 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.288191 | val 0.255514 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.286919 | val 0.253260 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.285843 | val 0.251180 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.286726 | val 0.249246 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.290410 | val 0.247444 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.287911 | val 0.245694 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.286374 | val 0.244082 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.284645 | val 0.242651 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.283849 | val 0.241385 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.283732 | val 0.240205 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.280877 | val 0.239135 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.276313 | val 0.238235 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.281516 | val 0.237349 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.282999 | val 0.236598 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.280831 | val 0.235823 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.281541 | val 0.235136 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.286317 | val 0.234512 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.283985 | val 0.234034 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.284423 | val 0.233535 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.282466 | val 0.233109 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.282342 | val 0.232897 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.282967 | val 0.232626 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.281170 | val 0.232416 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.278532 | val 0.232261 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.279563 | val 0.232145 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.281468 | val 0.231999 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.282588 | val 0.231837 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.279659 | val 0.231664 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.282742 | val 0.231530 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.279569 | val 0.231345 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.285476 | val 0.231161 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.16342 | val 0.00341 | noisy 0.00347 | Δ +1.8% | LR 0.00300 | ||ψ-φ|| 0.175\n",
      "[Stage3] L=1 ep 001 | train 0.16274 | val 0.00338 | noisy 0.00347 | Δ +2.8% | LR 0.00300 | ||ψ-φ|| 0.183\n",
      "[Stage3] L=1 ep 002 | train 0.15994 | val 0.00332 | noisy 0.00347 | Δ +4.4% | LR 0.00300 | ||ψ-φ|| 0.253\n",
      "[Stage3] L=1 ep 003 | train 0.17100 | val 0.00326 | noisy 0.00347 | Δ +6.1% | LR 0.00300 | ||ψ-φ|| 0.345\n",
      "[Stage3] L=1 ep 004 | train 0.16525 | val 0.00319 | noisy 0.00347 | Δ +8.3% | LR 0.00300 | ||ψ-φ|| 0.454\n",
      "[Stage3] L=1 ep 005 | train 0.16992 | val 0.00309 | noisy 0.00347 | Δ +11.0% | LR 0.00300 | ||ψ-φ|| 0.567\n",
      "[Stage3] L=1 ep 006 | train 0.17006 | val 0.00302 | noisy 0.00347 | Δ +13.1% | LR 0.00300 | ||ψ-φ|| 0.669\n",
      "[Stage3] L=1 ep 007 | train 0.16062 | val 0.00297 | noisy 0.00347 | Δ +14.5% | LR 0.00300 | ||ψ-φ|| 0.750\n",
      "[Stage3] L=1 ep 008 | train 0.16523 | val 0.00294 | noisy 0.00347 | Δ +15.4% | LR 0.00300 | ||ψ-φ|| 0.818\n",
      "[Stage3] L=1 ep 009 | train 0.16767 | val 0.00291 | noisy 0.00347 | Δ +16.3% | LR 0.00300 | ||ψ-φ|| 0.887\n",
      "[Stage3] L=1 ep 010 | train 0.16174 | val 0.00288 | noisy 0.00347 | Δ +17.3% | LR 0.00300 | ||ψ-φ|| 0.938\n",
      "[Stage3] L=1 ep 011 | train 0.15997 | val 0.00286 | noisy 0.00347 | Δ +17.7% | LR 0.00300 | ||ψ-φ|| 0.967\n",
      "[Stage3] L=1 ep 012 | train 0.16356 | val 0.00286 | noisy 0.00347 | Δ +17.8% | LR 0.00300 | ||ψ-φ|| 0.975\n",
      "[Stage3] L=1 ep 013 | train 0.16237 | val 0.00286 | noisy 0.00347 | Δ +17.7% | LR 0.00300 | ||ψ-φ|| 0.989\n",
      "[Stage3] L=1 ep 014 | train 0.16277 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00300 | ||ψ-φ|| 1.012\n",
      "[Stage3] L=1 ep 015 | train 0.15814 | val 0.00283 | noisy 0.00347 | Δ +18.7% | LR 0.00300 | ||ψ-φ|| 1.025\n",
      "[Stage3] L=1 ep 016 | train 0.16670 | val 0.00281 | noisy 0.00347 | Δ +19.0% | LR 0.00300 | ||ψ-φ|| 1.037\n",
      "[Stage3] L=1 ep 017 | train 0.16254 | val 0.00281 | noisy 0.00347 | Δ +19.1% | LR 0.00300 | ||ψ-φ|| 1.051\n",
      "[Stage3] L=1 ep 018 | train 0.16237 | val 0.00282 | noisy 0.00347 | Δ +18.9% | LR 0.00300 | ||ψ-φ|| 1.059\n",
      "[Stage3] L=1 ep 019 | train 0.16420 | val 0.00283 | noisy 0.00347 | Δ +18.6% | LR 0.00300 | ||ψ-φ|| 1.050\n",
      "[Stage3] L=1 ep 020 | train 0.16495 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00300 | ||ψ-φ|| 1.044\n",
      "[Stage3] L=1 ep 021 | train 0.17130 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00300 | ||ψ-φ|| 1.040\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 022 | train 0.16995 | val 0.00281 | noisy 0.00347 | Δ +19.0% | LR 0.00150 | ||ψ-φ|| 1.061\n",
      "[Stage3] L=1 ep 023 | train 0.16547 | val 0.00282 | noisy 0.00347 | Δ +18.9% | LR 0.00150 | ||ψ-φ|| 1.068\n",
      "[Stage3] L=1 ep 024 | train 0.15885 | val 0.00282 | noisy 0.00347 | Δ +18.7% | LR 0.00150 | ||ψ-φ|| 1.067\n",
      "[Stage3] L=1 ep 025 | train 0.16587 | val 0.00283 | noisy 0.00347 | Δ +18.4% | LR 0.00150 | ||ψ-φ|| 1.069\n",
      "[Stage3] L=1 ep 026 | train 0.16819 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00150 | ||ψ-φ|| 1.072\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 027 | train 0.16823 | val 0.00283 | noisy 0.00347 | Δ +18.6% | LR 0.00075 | ||ψ-φ|| 1.071\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 1 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.500266 | val 0.486830 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.492769 | val 0.478756 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.485701 | val 0.471070 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.478323 | val 0.463759 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.471635 | val 0.456960 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.465810 | val 0.450804 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.461159 | val 0.445296 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.455003 | val 0.440450 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.450080 | val 0.435990 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.445952 | val 0.431801 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.441869 | val 0.427639 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.438299 | val 0.423501 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.434274 | val 0.419231 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.431623 | val 0.414747 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.427224 | val 0.410120 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.422670 | val 0.405232 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.418643 | val 0.400140 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.414539 | val 0.394910 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.410009 | val 0.389644 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.404975 | val 0.384471 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.401902 | val 0.379428 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.396935 | val 0.374653 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.392843 | val 0.370150 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.388492 | val 0.365961 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.385472 | val 0.362087 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.379949 | val 0.358629 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.377986 | val 0.355449 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.374135 | val 0.352657 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.369912 | val 0.350211 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.367994 | val 0.348055 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.365955 | val 0.346137 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.362262 | val 0.344402 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.360938 | val 0.342714 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.358403 | val 0.341031 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.355637 | val 0.339293 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.353111 | val 0.337471 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.351605 | val 0.335498 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.348611 | val 0.333348 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.346026 | val 0.330997 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.342076 | val 0.328445 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.339336 | val 0.325756 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.336699 | val 0.322907 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.332760 | val 0.320035 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.330941 | val 0.317108 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.326919 | val 0.314240 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.323849 | val 0.311507 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.321398 | val 0.308823 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.320546 | val 0.306160 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.316232 | val 0.303642 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.315147 | val 0.301270 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.314236 | val 0.298931 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.312676 | val 0.296694 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.309070 | val 0.294575 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.308608 | val 0.292467 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.308408 | val 0.290411 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.305957 | val 0.288452 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.304087 | val 0.286599 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.303214 | val 0.284714 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.302691 | val 0.282878 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.299773 | val 0.281064 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.298800 | val 0.279244 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.297918 | val 0.277477 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.295165 | val 0.275783 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.295232 | val 0.274040 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.292556 | val 0.272333 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.293040 | val 0.270554 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.291380 | val 0.268717 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.289447 | val 0.266811 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.289166 | val 0.264849 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.283896 | val 0.262931 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.283848 | val 0.261004 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.282198 | val 0.259048 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.282542 | val 0.257100 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.280107 | val 0.255203 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.275817 | val 0.253358 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.275242 | val 0.251632 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.275855 | val 0.249884 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.273672 | val 0.248410 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.273039 | val 0.247030 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.268638 | val 0.245836 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.268740 | val 0.244584 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.268189 | val 0.243270 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.267222 | val 0.242004 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.265952 | val 0.240860 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.263792 | val 0.239708 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.262150 | val 0.238580 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.259837 | val 0.237272 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.259475 | val 0.235940 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.255475 | val 0.234672 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.256035 | val 0.233093 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.252911 | val 0.231531 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.250913 | val 0.229798 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.249081 | val 0.227923 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.246299 | val 0.225802 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.243635 | val 0.223490 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.242180 | val 0.221081 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.239754 | val 0.218497 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.237399 | val 0.215922 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.234740 | val 0.213280 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.231092 | val 0.210597 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.230292 | val 0.207871 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.224305 | val 0.205393 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.223901 | val 0.202919 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.220829 | val 0.200393 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.219447 | val 0.197857 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.216802 | val 0.195470 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.216666 | val 0.193279 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.213771 | val 0.191380 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.212072 | val 0.189625 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.210058 | val 0.188041 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.211371 | val 0.186350 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.208256 | val 0.184796 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.206680 | val 0.183449 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.206089 | val 0.182171 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.205381 | val 0.181001 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.205675 | val 0.179933 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.203532 | val 0.179039 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.203326 | val 0.178215 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.201890 | val 0.177654 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.201267 | val 0.177114 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.11953 | val 0.00320 | noisy 0.00347 | Δ +8.0% | LR 0.00300 | ||ψ-φ|| 0.331\n",
      "[Stage3] L=3 ep 001 | train 0.11724 | val 0.00310 | noisy 0.00347 | Δ +10.9% | LR 0.00300 | ||ψ-φ|| 0.359\n",
      "[Stage3] L=3 ep 002 | train 0.12036 | val 0.00296 | noisy 0.00347 | Δ +14.7% | LR 0.00300 | ||ψ-φ|| 0.448\n",
      "[Stage3] L=3 ep 003 | train 0.11429 | val 0.00287 | noisy 0.00347 | Δ +17.3% | LR 0.00300 | ||ψ-φ|| 0.550\n",
      "[Stage3] L=3 ep 004 | train 0.10939 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00300 | ||ψ-φ|| 0.643\n",
      "[Stage3] L=3 ep 005 | train 0.12471 | val 0.00284 | noisy 0.00347 | Δ +18.4% | LR 0.00300 | ||ψ-φ|| 0.734\n",
      "[Stage3] L=3 ep 006 | train 0.11523 | val 0.00279 | noisy 0.00347 | Δ +19.6% | LR 0.00300 | ||ψ-φ|| 0.831\n",
      "[Stage3] L=3 ep 007 | train 0.11812 | val 0.00271 | noisy 0.00347 | Δ +22.1% | LR 0.00300 | ||ψ-φ|| 0.926\n",
      "[Stage3] L=3 ep 008 | train 0.11094 | val 0.00266 | noisy 0.00347 | Δ +23.5% | LR 0.00300 | ||ψ-φ|| 1.024\n",
      "[Stage3] L=3 ep 009 | train 0.11348 | val 0.00262 | noisy 0.00347 | Δ +24.6% | LR 0.00300 | ||ψ-φ|| 1.102\n",
      "[Stage3] L=3 ep 010 | train 0.11347 | val 0.00258 | noisy 0.00347 | Δ +25.7% | LR 0.00300 | ||ψ-φ|| 1.181\n",
      "[Stage3] L=3 ep 011 | train 0.11011 | val 0.00254 | noisy 0.00347 | Δ +26.9% | LR 0.00300 | ||ψ-φ|| 1.252\n",
      "[Stage3] L=3 ep 012 | train 0.11110 | val 0.00254 | noisy 0.00347 | Δ +26.8% | LR 0.00300 | ||ψ-φ|| 1.322\n",
      "[Stage3] L=3 ep 013 | train 0.10984 | val 0.00254 | noisy 0.00347 | Δ +26.8% | LR 0.00300 | ||ψ-φ|| 1.391\n",
      "[Stage3] L=3 ep 014 | train 0.11087 | val 0.00254 | noisy 0.00347 | Δ +26.9% | LR 0.00300 | ||ψ-φ|| 1.468\n",
      "[Stage3] L=3 ep 015 | train 0.10750 | val 0.00260 | noisy 0.00347 | Δ +25.2% | LR 0.00300 | ||ψ-φ|| 1.545\n",
      "[Stage3] L=3 ep 016 | train 0.10700 | val 0.00262 | noisy 0.00347 | Δ +24.7% | LR 0.00300 | ||ψ-φ|| 1.623\n",
      "[Stage3] L=3 ep 017 | train 0.10608 | val 0.00263 | noisy 0.00347 | Δ +24.3% | LR 0.00300 | ||ψ-φ|| 1.695\n",
      "[Stage3] L=3 ep 018 | train 0.10176 | val 0.00269 | noisy 0.00347 | Δ +22.5% | LR 0.00300 | ||ψ-φ|| 1.774\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 019 | train 0.10148 | val 0.00275 | noisy 0.00347 | Δ +20.7% | LR 0.00150 | ||ψ-φ|| 1.844\n",
      "[Stage3] L=3 ep 020 | train 0.10624 | val 0.00278 | noisy 0.00347 | Δ +19.9% | LR 0.00150 | ||ψ-φ|| 1.900\n",
      "[Stage3] L=3 ep 021 | train 0.10417 | val 0.00280 | noisy 0.00347 | Δ +19.5% | LR 0.00150 | ||ψ-φ|| 1.949\n",
      "[Stage3] L=3 ep 022 | train 0.10711 | val 0.00280 | noisy 0.00347 | Δ +19.5% | LR 0.00150 | ||ψ-φ|| 1.992\n",
      "[Stage3] L=3 ep 023 | train 0.10591 | val 0.00281 | noisy 0.00347 | Δ +19.2% | LR 0.00150 | ||ψ-φ|| 2.031\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 024 | train 0.10332 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00075 | ||ψ-φ|| 2.072\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.499120 | val 0.481273 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.488306 | val 0.472217 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.480860 | val 0.463851 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.472098 | val 0.456616 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.465392 | val 0.450689 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.460579 | val 0.446056 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.456440 | val 0.442480 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.452342 | val 0.439538 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.449937 | val 0.436768 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.447445 | val 0.433850 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.445272 | val 0.430641 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.442839 | val 0.427262 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.439127 | val 0.423878 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.436861 | val 0.420621 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.434273 | val 0.417631 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.432995 | val 0.414903 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.429948 | val 0.412423 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.428964 | val 0.410046 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.426268 | val 0.407735 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.425133 | val 0.405450 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.422320 | val 0.403200 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.419283 | val 0.401036 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.418345 | val 0.398899 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.416385 | val 0.396818 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.414387 | val 0.394737 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.411812 | val 0.392652 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.409530 | val 0.390508 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.407215 | val 0.388342 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.406302 | val 0.386099 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.404970 | val 0.383847 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.403491 | val 0.381573 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.400553 | val 0.379287 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.398528 | val 0.376906 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.394416 | val 0.374482 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.395016 | val 0.371899 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.393465 | val 0.369264 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.389115 | val 0.366572 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.389231 | val 0.363723 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.383747 | val 0.360793 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.383117 | val 0.357666 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.380363 | val 0.354517 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.379063 | val 0.351199 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.376147 | val 0.347824 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.372653 | val 0.344390 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.368961 | val 0.340887 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.367189 | val 0.337345 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.359845 | val 0.333818 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.361652 | val 0.330125 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.355980 | val 0.326429 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.352984 | val 0.322649 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.350817 | val 0.318788 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.345430 | val 0.314884 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.341424 | val 0.310894 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.337310 | val 0.306690 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.331942 | val 0.302353 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.326915 | val 0.297936 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.325990 | val 0.293465 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.318547 | val 0.289137 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.314149 | val 0.284893 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.311991 | val 0.280845 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.304493 | val 0.277017 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.300474 | val 0.273413 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.297155 | val 0.269979 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.294082 | val 0.266668 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.287443 | val 0.263587 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.284739 | val 0.260554 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.281796 | val 0.257644 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.276257 | val 0.254867 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.273882 | val 0.252111 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.270584 | val 0.249495 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.267854 | val 0.246961 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.263557 | val 0.244581 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.262595 | val 0.242298 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.259893 | val 0.240150 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.255648 | val 0.238135 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.255028 | val 0.236211 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.254605 | val 0.234417 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.252251 | val 0.232703 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.249822 | val 0.231094 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.246375 | val 0.229477 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.244115 | val 0.227856 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.241228 | val 0.226148 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.240165 | val 0.224367 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.238701 | val 0.222454 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.236589 | val 0.220377 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.234141 | val 0.218170 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.231263 | val 0.215733 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.228512 | val 0.212927 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.224795 | val 0.209672 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.222766 | val 0.205852 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.218468 | val 0.201503 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.215388 | val 0.196627 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.207756 | val 0.191302 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.204302 | val 0.185434 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.199475 | val 0.179156 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.192818 | val 0.172604 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.188157 | val 0.165821 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.182230 | val 0.159141 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.177393 | val 0.152662 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.172026 | val 0.146499 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.167421 | val 0.140697 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.164218 | val 0.135352 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.159980 | val 0.130640 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.151522 | val 0.126622 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.150832 | val 0.123019 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.146636 | val 0.119973 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.147923 | val 0.117132 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.145757 | val 0.114751 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.139959 | val 0.112886 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.139934 | val 0.111215 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.139445 | val 0.109757 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.137480 | val 0.108481 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.135685 | val 0.107324 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.137407 | val 0.106228 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.135583 | val 0.105321 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.133888 | val 0.104629 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.132247 | val 0.104170 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.133386 | val 0.103687 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.133828 | val 0.103189 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.133532 | val 0.102892 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.09022 | val 0.00341 | noisy 0.00347 | Δ +1.7% | LR 0.00300 | ||ψ-φ|| 0.326\n",
      "[Stage3] L=3 ep 001 | train 0.08862 | val 0.00328 | noisy 0.00347 | Δ +5.5% | LR 0.00300 | ||ψ-φ|| 0.394\n",
      "[Stage3] L=3 ep 002 | train 0.09053 | val 0.00314 | noisy 0.00347 | Δ +9.6% | LR 0.00300 | ||ψ-φ|| 0.513\n",
      "[Stage3] L=3 ep 003 | train 0.08930 | val 0.00304 | noisy 0.00347 | Δ +12.5% | LR 0.00300 | ||ψ-φ|| 0.641\n",
      "[Stage3] L=3 ep 004 | train 0.08239 | val 0.00302 | noisy 0.00347 | Δ +13.0% | LR 0.00300 | ||ψ-φ|| 0.751\n",
      "[Stage3] L=3 ep 005 | train 0.08600 | val 0.00298 | noisy 0.00347 | Δ +14.2% | LR 0.00300 | ||ψ-φ|| 0.864\n",
      "[Stage3] L=3 ep 006 | train 0.08500 | val 0.00294 | noisy 0.00347 | Δ +15.4% | LR 0.00300 | ||ψ-φ|| 0.964\n",
      "[Stage3] L=3 ep 007 | train 0.08292 | val 0.00291 | noisy 0.00347 | Δ +16.3% | LR 0.00300 | ||ψ-φ|| 1.069\n",
      "[Stage3] L=3 ep 008 | train 0.08032 | val 0.00292 | noisy 0.00347 | Δ +16.0% | LR 0.00300 | ||ψ-φ|| 1.168\n",
      "[Stage3] L=3 ep 009 | train 0.08126 | val 0.00297 | noisy 0.00347 | Δ +14.6% | LR 0.00300 | ||ψ-φ|| 1.259\n",
      "[Stage3] L=3 ep 010 | train 0.07821 | val 0.00302 | noisy 0.00347 | Δ +13.2% | LR 0.00300 | ||ψ-φ|| 1.345\n",
      "[Stage3] L=3 ep 011 | train 0.08269 | val 0.00296 | noisy 0.00347 | Δ +14.8% | LR 0.00300 | ||ψ-φ|| 1.422\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 012 | train 0.07813 | val 0.00293 | noisy 0.00347 | Δ +15.6% | LR 0.00150 | ||ψ-φ|| 1.489\n",
      "[Stage3] L=3 ep 013 | train 0.08469 | val 0.00293 | noisy 0.00347 | Δ +15.8% | LR 0.00150 | ||ψ-φ|| 1.542\n",
      "[Stage3] L=3 ep 014 | train 0.08116 | val 0.00294 | noisy 0.00347 | Δ +15.3% | LR 0.00150 | ||ψ-φ|| 1.580\n",
      "[Stage3] L=3 ep 015 | train 0.07882 | val 0.00294 | noisy 0.00347 | Δ +15.2% | LR 0.00150 | ||ψ-φ|| 1.611\n",
      "[Stage3] L=3 ep 016 | train 0.08629 | val 0.00293 | noisy 0.00347 | Δ +15.8% | LR 0.00150 | ||ψ-φ|| 1.641\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 017 | train 0.08387 | val 0.00292 | noisy 0.00347 | Δ +15.8% | LR 0.00075 | ||ψ-φ|| 1.667\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.485201 | val 0.464369 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.473793 | val 0.452383 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.463117 | val 0.441175 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.450438 | val 0.430753 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.442312 | val 0.420939 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.432281 | val 0.411757 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.427222 | val 0.402965 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.418903 | val 0.394568 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.409457 | val 0.386471 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.403678 | val 0.378634 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.396858 | val 0.371283 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.391087 | val 0.364207 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.384504 | val 0.357683 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.382151 | val 0.351607 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.376901 | val 0.346033 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.372192 | val 0.340811 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.366692 | val 0.335913 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.364483 | val 0.331187 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.362864 | val 0.326613 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.357573 | val 0.322184 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.352668 | val 0.317816 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.349424 | val 0.313483 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.346012 | val 0.309182 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.341073 | val 0.304928 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.333917 | val 0.300815 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.333687 | val 0.296774 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.326799 | val 0.292926 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.324231 | val 0.289186 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.320627 | val 0.285579 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.318443 | val 0.282107 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.313065 | val 0.278782 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.309259 | val 0.275508 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.306555 | val 0.272317 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.301252 | val 0.269216 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.300288 | val 0.266187 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.295874 | val 0.263318 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.297595 | val 0.260469 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.290670 | val 0.257886 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.289488 | val 0.255410 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.285927 | val 0.253091 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.284201 | val 0.250799 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.278597 | val 0.248656 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.280766 | val 0.246565 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.277817 | val 0.244597 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.275340 | val 0.242802 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.272571 | val 0.241164 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.271657 | val 0.239561 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.268963 | val 0.238022 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.269496 | val 0.236601 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.269901 | val 0.235223 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.263576 | val 0.234004 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.264118 | val 0.232780 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.262134 | val 0.231653 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.261671 | val 0.230544 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.258631 | val 0.229479 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.260629 | val 0.228472 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.259875 | val 0.227534 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.256360 | val 0.226723 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.257474 | val 0.225911 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.255856 | val 0.225079 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.253780 | val 0.224352 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.255318 | val 0.223689 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.253068 | val 0.223064 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.250396 | val 0.222426 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.250255 | val 0.221767 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.250561 | val 0.221042 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.248447 | val 0.220292 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.248615 | val 0.219716 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.247248 | val 0.219080 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.247372 | val 0.218483 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.246820 | val 0.217842 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.243969 | val 0.217319 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.246125 | val 0.216765 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.244288 | val 0.216141 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.245252 | val 0.215481 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.243072 | val 0.214874 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.243407 | val 0.214250 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.242783 | val 0.213577 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.240341 | val 0.212980 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.237065 | val 0.212302 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.238384 | val 0.211500 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.236755 | val 0.210712 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.236520 | val 0.209936 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.235246 | val 0.209108 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.235483 | val 0.208203 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.233212 | val 0.207280 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.231926 | val 0.206293 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.230210 | val 0.205280 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.229760 | val 0.204244 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.227950 | val 0.203166 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.228732 | val 0.202049 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.224580 | val 0.200963 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.223994 | val 0.199842 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.222590 | val 0.198723 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.222817 | val 0.197608 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.221171 | val 0.196543 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.218563 | val 0.195518 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.217458 | val 0.194463 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.216332 | val 0.193453 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.215801 | val 0.192439 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.213857 | val 0.191514 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.211849 | val 0.190611 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.209181 | val 0.189769 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.208701 | val 0.188933 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.208840 | val 0.188155 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.207616 | val 0.187421 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.206644 | val 0.186750 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.204968 | val 0.186130 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.203331 | val 0.185479 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.203248 | val 0.184849 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.201385 | val 0.184267 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.200396 | val 0.183614 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.198889 | val 0.183037 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.196942 | val 0.182502 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.197260 | val 0.181955 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.194627 | val 0.181434 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.194373 | val 0.180875 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.194343 | val 0.180383 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.193290 | val 0.179888 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.192372 | val 0.179441 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.10991 | val 0.00330 | noisy 0.00347 | Δ +5.0% | LR 0.00300 | ||ψ-φ|| 0.350\n",
      "[Stage3] L=3 ep 001 | train 0.11067 | val 0.00335 | noisy 0.00347 | Δ +3.6% | LR 0.00300 | ||ψ-φ|| 0.342\n",
      "[Stage3] L=3 ep 002 | train 0.11396 | val 0.00337 | noisy 0.00347 | Δ +2.9% | LR 0.00300 | ||ψ-φ|| 0.396\n",
      "[Stage3] L=3 ep 003 | train 0.10757 | val 0.00342 | noisy 0.00347 | Δ +1.5% | LR 0.00300 | ||ψ-φ|| 0.487\n",
      "[Stage3] L=3 ep 004 | train 0.10562 | val 0.00347 | noisy 0.00347 | Δ +0.0% | LR 0.00300 | ||ψ-φ|| 0.588\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 005 | train 0.11095 | val 0.00355 | noisy 0.00347 | Δ -2.1% | LR 0.00150 | ||ψ-φ|| 0.678\n",
      "[Stage3] L=3 ep 006 | train 0.11049 | val 0.00362 | noisy 0.00347 | Δ -4.1% | LR 0.00150 | ||ψ-φ|| 0.758\n",
      "[Stage3] L=3 ep 007 | train 0.10479 | val 0.00369 | noisy 0.00347 | Δ -6.1% | LR 0.00150 | ||ψ-φ|| 0.814\n",
      "[Stage3] L=3 ep 008 | train 0.10552 | val 0.00375 | noisy 0.00347 | Δ -8.0% | LR 0.00150 | ||ψ-φ|| 0.861\n",
      "[Stage3] L=3 ep 009 | train 0.10624 | val 0.00381 | noisy 0.00347 | Δ -9.6% | LR 0.00150 | ||ψ-φ|| 0.903\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 010 | train 0.10954 | val 0.00387 | noisy 0.00347 | Δ -11.4% | LR 0.00075 | ||ψ-φ|| 0.947\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.502575 | val 0.500997 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.492912 | val 0.490521 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.484001 | val 0.480064 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.475370 | val 0.469558 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.466015 | val 0.459129 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.457941 | val 0.448848 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.450121 | val 0.438768 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.440052 | val 0.428929 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.432336 | val 0.419222 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.424160 | val 0.409804 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.416205 | val 0.400868 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.408902 | val 0.392379 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.401281 | val 0.384380 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.394245 | val 0.376890 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.389419 | val 0.369832 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.382256 | val 0.363249 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.377652 | val 0.357114 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.372167 | val 0.351526 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.368642 | val 0.346493 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.363767 | val 0.342028 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.361312 | val 0.338080 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.357253 | val 0.334572 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.353828 | val 0.331458 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.350054 | val 0.328615 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.347485 | val 0.325986 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.344167 | val 0.323574 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.344867 | val 0.321326 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.339616 | val 0.319248 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.338267 | val 0.317263 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.334453 | val 0.315331 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.333467 | val 0.313361 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.330764 | val 0.311332 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.329161 | val 0.309210 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.326744 | val 0.307001 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.324674 | val 0.304663 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.321618 | val 0.302195 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.320119 | val 0.299536 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.316882 | val 0.296690 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.315541 | val 0.293628 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.311748 | val 0.290366 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.306242 | val 0.286911 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.304403 | val 0.283205 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.301544 | val 0.279310 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.294782 | val 0.275227 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.293204 | val 0.270970 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.288060 | val 0.266588 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.286226 | val 0.262107 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.283292 | val 0.257633 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.278107 | val 0.253213 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.274830 | val 0.248826 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.269724 | val 0.244530 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.263671 | val 0.240373 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.260284 | val 0.236291 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.256232 | val 0.232332 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.254283 | val 0.228581 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.249475 | val 0.225068 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.246725 | val 0.221699 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.244583 | val 0.218517 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.240346 | val 0.215456 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.237263 | val 0.212517 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.234379 | val 0.209627 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.231445 | val 0.206812 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.227259 | val 0.204031 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.224974 | val 0.201140 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.222413 | val 0.198251 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.218335 | val 0.195195 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.213754 | val 0.192046 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.213560 | val 0.188662 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.209413 | val 0.185278 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.203857 | val 0.181901 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.202120 | val 0.178457 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.198852 | val 0.175050 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.194034 | val 0.171691 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.191031 | val 0.168351 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.192707 | val 0.165062 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.187103 | val 0.161884 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.182442 | val 0.158828 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.180475 | val 0.155859 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.178523 | val 0.152936 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.175180 | val 0.150251 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.176931 | val 0.147662 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.170633 | val 0.145261 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.171282 | val 0.142979 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.169217 | val 0.140817 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.167521 | val 0.138746 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.164047 | val 0.136760 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.163803 | val 0.134792 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.164623 | val 0.132869 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.157986 | val 0.131097 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.161567 | val 0.129351 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.155683 | val 0.127684 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.155162 | val 0.126085 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.151765 | val 0.124560 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.151956 | val 0.123104 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.148407 | val 0.121722 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.147546 | val 0.120333 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.148235 | val 0.118958 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.147912 | val 0.117640 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.145039 | val 0.116409 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.143466 | val 0.115317 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.142676 | val 0.114327 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.140264 | val 0.113463 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.140332 | val 0.112659 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.135997 | val 0.111985 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.137381 | val 0.111272 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.138031 | val 0.110550 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.137748 | val 0.109907 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.135172 | val 0.109296 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.137247 | val 0.108658 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.135273 | val 0.108170 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.131952 | val 0.107806 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.130790 | val 0.107462 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.132710 | val 0.107078 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.133694 | val 0.106758 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.132079 | val 0.106515 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.130171 | val 0.106299 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.130290 | val 0.106068 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.129938 | val 0.105803 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.131382 | val 0.105542 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.128515 | val 0.105410 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.08786 | val 0.00335 | noisy 0.00347 | Δ +3.5% | LR 0.00300 | ||ψ-φ|| 0.327\n",
      "[Stage3] L=3 ep 001 | train 0.08142 | val 0.00334 | noisy 0.00347 | Δ +4.0% | LR 0.00300 | ||ψ-φ|| 0.309\n",
      "[Stage3] L=3 ep 002 | train 0.08995 | val 0.00326 | noisy 0.00347 | Δ +6.1% | LR 0.00300 | ||ψ-φ|| 0.357\n",
      "[Stage3] L=3 ep 003 | train 0.08504 | val 0.00312 | noisy 0.00347 | Δ +10.2% | LR 0.00300 | ||ψ-φ|| 0.436\n",
      "[Stage3] L=3 ep 004 | train 0.08469 | val 0.00302 | noisy 0.00347 | Δ +13.1% | LR 0.00300 | ||ψ-φ|| 0.500\n",
      "[Stage3] L=3 ep 005 | train 0.08828 | val 0.00296 | noisy 0.00347 | Δ +14.7% | LR 0.00300 | ||ψ-φ|| 0.553\n",
      "[Stage3] L=3 ep 006 | train 0.08838 | val 0.00290 | noisy 0.00347 | Δ +16.4% | LR 0.00300 | ||ψ-φ|| 0.596\n",
      "[Stage3] L=3 ep 007 | train 0.08810 | val 0.00286 | noisy 0.00347 | Δ +17.6% | LR 0.00300 | ||ψ-φ|| 0.633\n",
      "[Stage3] L=3 ep 008 | train 0.08603 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00300 | ||ψ-φ|| 0.666\n",
      "[Stage3] L=3 ep 009 | train 0.07898 | val 0.00279 | noisy 0.00347 | Δ +19.6% | LR 0.00300 | ||ψ-φ|| 0.687\n",
      "[Stage3] L=3 ep 010 | train 0.07896 | val 0.00276 | noisy 0.00347 | Δ +20.6% | LR 0.00300 | ||ψ-φ|| 0.711\n",
      "[Stage3] L=3 ep 011 | train 0.07925 | val 0.00275 | noisy 0.00347 | Δ +20.8% | LR 0.00300 | ||ψ-φ|| 0.737\n",
      "[Stage3] L=3 ep 012 | train 0.08395 | val 0.00278 | noisy 0.00347 | Δ +20.0% | LR 0.00300 | ||ψ-φ|| 0.756\n",
      "[Stage3] L=3 ep 013 | train 0.08121 | val 0.00282 | noisy 0.00347 | Δ +18.9% | LR 0.00300 | ||ψ-φ|| 0.773\n",
      "[Stage3] L=3 ep 014 | train 0.07767 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00300 | ||ψ-φ|| 0.794\n",
      "[Stage3] L=3 ep 015 | train 0.08695 | val 0.00281 | noisy 0.00347 | Δ +19.0% | LR 0.00300 | ||ψ-φ|| 0.819\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 016 | train 0.07909 | val 0.00279 | noisy 0.00347 | Δ +19.8% | LR 0.00150 | ||ψ-φ|| 0.844\n",
      "[Stage3] L=3 ep 017 | train 0.08181 | val 0.00279 | noisy 0.00347 | Δ +19.7% | LR 0.00150 | ||ψ-φ|| 0.865\n",
      "[Stage3] L=3 ep 018 | train 0.08182 | val 0.00279 | noisy 0.00347 | Δ +19.7% | LR 0.00150 | ||ψ-φ|| 0.882\n",
      "[Stage3] L=3 ep 019 | train 0.08491 | val 0.00278 | noisy 0.00347 | Δ +20.0% | LR 0.00150 | ||ψ-φ|| 0.898\n",
      "[Stage3] L=3 ep 020 | train 0.08635 | val 0.00278 | noisy 0.00347 | Δ +19.9% | LR 0.00150 | ||ψ-φ|| 0.919\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 021 | train 0.08336 | val 0.00278 | noisy 0.00347 | Δ +20.1% | LR 0.00075 | ||ψ-φ|| 0.942\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.487481 | val 0.478413 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.483043 | val 0.472811 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.476046 | val 0.467140 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.470840 | val 0.461517 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.465275 | val 0.455843 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.458986 | val 0.450171 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.453558 | val 0.444526 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.447008 | val 0.438876 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.441159 | val 0.433294 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.436259 | val 0.427787 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.430634 | val 0.422341 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.424561 | val 0.417036 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.419126 | val 0.411992 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.414277 | val 0.407359 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.409886 | val 0.403215 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.405821 | val 0.399528 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.401261 | val 0.396165 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.398570 | val 0.392964 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.395899 | val 0.389761 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.393024 | val 0.386458 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.389959 | val 0.382871 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.386004 | val 0.379091 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.383544 | val 0.375034 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.378810 | val 0.370829 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.375368 | val 0.366395 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.371971 | val 0.361870 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.367644 | val 0.357355 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.363099 | val 0.352983 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.360446 | val 0.348693 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.358503 | val 0.344564 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.355351 | val 0.340708 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.351079 | val 0.337136 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.348639 | val 0.333795 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.344705 | val 0.330752 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.343070 | val 0.327980 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.340524 | val 0.325546 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.337513 | val 0.323452 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.336389 | val 0.321551 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.336288 | val 0.319804 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.334297 | val 0.318184 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.331275 | val 0.316624 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.329311 | val 0.315048 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.329866 | val 0.313414 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.327383 | val 0.311692 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.325873 | val 0.309920 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.324466 | val 0.308070 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.323783 | val 0.306056 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.323710 | val 0.303878 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.319987 | val 0.301642 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.319087 | val 0.299217 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.316814 | val 0.296565 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.313475 | val 0.293788 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.312773 | val 0.290688 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.310808 | val 0.287370 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.309373 | val 0.283825 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.303345 | val 0.280078 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.302361 | val 0.276055 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.301254 | val 0.271740 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.295375 | val 0.267247 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.294435 | val 0.262484 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.290192 | val 0.257561 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.283161 | val 0.252419 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.280799 | val 0.247102 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.274249 | val 0.241709 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.272775 | val 0.236282 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.265455 | val 0.230990 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.263500 | val 0.225669 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.258027 | val 0.220437 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.255315 | val 0.215347 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.250738 | val 0.210511 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.245151 | val 0.206041 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.243919 | val 0.201799 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.239169 | val 0.197865 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.234427 | val 0.194162 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.230951 | val 0.190906 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.226798 | val 0.187887 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.226665 | val 0.185222 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.223278 | val 0.182846 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.222744 | val 0.180740 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.219234 | val 0.178919 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.217706 | val 0.177282 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.217382 | val 0.176011 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.215228 | val 0.174873 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.216233 | val 0.173849 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.212969 | val 0.172905 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.210890 | val 0.172073 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.211598 | val 0.171320 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.212698 | val 0.170577 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.212084 | val 0.170140 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.208981 | val 0.169484 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.207935 | val 0.168967 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.209224 | val 0.168520 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.206832 | val 0.168133 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.211890 | val 0.167751 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.208387 | val 0.167315 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.205897 | val 0.166907 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.206923 | val 0.166531 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.206650 | val 0.166185 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.205447 | val 0.165870 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.202491 | val 0.165598 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.203487 | val 0.165404 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.204683 | val 0.165065 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.205676 | val 0.164780 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.203235 | val 0.164385 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.205301 | val 0.164038 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.207609 | val 0.163726 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.206174 | val 0.163587 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.205572 | val 0.163395 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.204282 | val 0.163322 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.205628 | val 0.163385 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.205560 | val 0.163280 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.202831 | val 0.163255 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.202791 | val 0.163227 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.203666 | val 0.163226 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.202815 | val 0.163172 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.205876 | val 0.163056 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.204336 | val 0.162885 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.206213 | val 0.162765 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.203414 | val 0.162557 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.206383 | val 0.162336 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.11549 | val 0.00325 | noisy 0.00347 | Δ +6.6% | LR 0.00300 | ||ψ-φ|| 0.381\n",
      "[Stage3] L=3 ep 001 | train 0.11328 | val 0.00308 | noisy 0.00347 | Δ +11.3% | LR 0.00300 | ||ψ-φ|| 0.391\n",
      "[Stage3] L=3 ep 002 | train 0.11370 | val 0.00290 | noisy 0.00347 | Δ +16.5% | LR 0.00300 | ||ψ-φ|| 0.417\n",
      "[Stage3] L=3 ep 003 | train 0.11803 | val 0.00283 | noisy 0.00347 | Δ +18.6% | LR 0.00300 | ||ψ-φ|| 0.450\n",
      "[Stage3] L=3 ep 004 | train 0.12506 | val 0.00278 | noisy 0.00347 | Δ +19.9% | LR 0.00300 | ||ψ-φ|| 0.500\n",
      "[Stage3] L=3 ep 005 | train 0.11392 | val 0.00277 | noisy 0.00347 | Δ +20.2% | LR 0.00300 | ||ψ-φ|| 0.547\n",
      "[Stage3] L=3 ep 006 | train 0.12319 | val 0.00279 | noisy 0.00347 | Δ +19.8% | LR 0.00300 | ||ψ-φ|| 0.589\n",
      "[Stage3] L=3 ep 007 | train 0.11522 | val 0.00278 | noisy 0.00347 | Δ +20.0% | LR 0.00300 | ||ψ-φ|| 0.636\n",
      "[Stage3] L=3 ep 008 | train 0.12509 | val 0.00279 | noisy 0.00347 | Δ +19.6% | LR 0.00300 | ||ψ-φ|| 0.673\n",
      "[Stage3] L=3 ep 009 | train 0.11739 | val 0.00277 | noisy 0.00347 | Δ +20.2% | LR 0.00300 | ||ψ-φ|| 0.707\n",
      "[Stage3] L=3 ep 010 | train 0.11529 | val 0.00275 | noisy 0.00347 | Δ +20.8% | LR 0.00300 | ||ψ-φ|| 0.735\n",
      "[Stage3] L=3 ep 011 | train 0.11411 | val 0.00275 | noisy 0.00347 | Δ +20.9% | LR 0.00300 | ||ψ-φ|| 0.756\n",
      "[Stage3] L=3 ep 012 | train 0.11463 | val 0.00275 | noisy 0.00347 | Δ +20.7% | LR 0.00300 | ||ψ-φ|| 0.788\n",
      "[Stage3] L=3 ep 013 | train 0.11915 | val 0.00275 | noisy 0.00347 | Δ +20.7% | LR 0.00300 | ||ψ-φ|| 0.826\n",
      "[Stage3] L=3 ep 014 | train 0.11903 | val 0.00274 | noisy 0.00347 | Δ +21.1% | LR 0.00300 | ||ψ-φ|| 0.863\n",
      "[Stage3] L=3 ep 015 | train 0.11423 | val 0.00275 | noisy 0.00347 | Δ +20.9% | LR 0.00300 | ||ψ-φ|| 0.901\n",
      "[Stage3] L=3 ep 016 | train 0.12117 | val 0.00275 | noisy 0.00347 | Δ +20.8% | LR 0.00300 | ||ψ-φ|| 0.944\n",
      "[Stage3] L=3 ep 017 | train 0.12493 | val 0.00272 | noisy 0.00347 | Δ +21.7% | LR 0.00300 | ||ψ-φ|| 0.991\n",
      "[Stage3] L=3 ep 018 | train 0.11794 | val 0.00271 | noisy 0.00347 | Δ +22.1% | LR 0.00300 | ||ψ-φ|| 1.025\n",
      "[Stage3] L=3 ep 019 | train 0.11947 | val 0.00271 | noisy 0.00347 | Δ +22.1% | LR 0.00300 | ||ψ-φ|| 1.058\n",
      "[Stage3] L=3 ep 020 | train 0.11425 | val 0.00272 | noisy 0.00347 | Δ +21.7% | LR 0.00300 | ||ψ-φ|| 1.089\n",
      "[Stage3] L=3 ep 021 | train 0.11770 | val 0.00273 | noisy 0.00347 | Δ +21.6% | LR 0.00300 | ||ψ-φ|| 1.110\n",
      "[Stage3] L=3 ep 022 | train 0.12128 | val 0.00272 | noisy 0.00347 | Δ +21.8% | LR 0.00300 | ||ψ-φ|| 1.134\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 023 | train 0.11561 | val 0.00274 | noisy 0.00347 | Δ +21.3% | LR 0.00150 | ||ψ-φ|| 1.151\n",
      "[Stage3] L=3 ep 024 | train 0.11972 | val 0.00274 | noisy 0.00347 | Δ +21.1% | LR 0.00150 | ||ψ-φ|| 1.172\n",
      "[Stage3] L=3 ep 025 | train 0.11491 | val 0.00274 | noisy 0.00347 | Δ +21.0% | LR 0.00150 | ||ψ-φ|| 1.187\n",
      "[Stage3] L=3 ep 026 | train 0.11579 | val 0.00274 | noisy 0.00347 | Δ +21.1% | LR 0.00150 | ||ψ-φ|| 1.203\n",
      "[Stage3] L=3 ep 027 | train 0.11042 | val 0.00275 | noisy 0.00347 | Δ +20.9% | LR 0.00150 | ||ψ-φ|| 1.217\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 028 | train 0.11738 | val 0.00275 | noisy 0.00347 | Δ +20.8% | LR 0.00075 | ||ψ-φ|| 1.227\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "Completed 10 runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7 — Train runs (instances × layers)\n",
    "# ============================================\n",
    "RUNS = []  # we’ll save each run in the next cell\n",
    "\n",
    "for L in LAYER_OPTIONS:\n",
    "    for inst in INSTANCE_IDS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Instance {inst} | Layers {L}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        s1 = train_stage1(\n",
    "            X_train, X_val,\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=120, batch_size=32,\n",
    "            lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        s3 = train_stage3(\n",
    "            X_train, X_val,\n",
    "            phi_stage1=s1[\"phi\"],\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16,\n",
    "            LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "            CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        RUNS.append({\n",
    "            \"instance_id\": inst,\n",
    "            \"n_layers\": L,\n",
    "            \"stage1\": {\n",
    "                \"phi\": s1[\"phi\"],\n",
    "                \"best_val\": s1[\"best_val\"],\n",
    "                \"hist_train\": s1[\"hist_train\"],\n",
    "                \"hist_val\": s1[\"hist_val\"],\n",
    "                \"hist_lr\": s1[\"hist_lr\"],\n",
    "                \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                \"epochs\": s1.get(\"epochs\"),\n",
    "                \"train_seconds\": float(t1 - t0),\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"psi\": s3[\"psi\"],\n",
    "                \"best_val\": s3[\"best_val\"],\n",
    "                \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                \"epochs\": s3.get(\"epochs\"),\n",
    "                \"hist_train\": s3[\"hist_train\"],\n",
    "                \"hist_val\": s3[\"hist_val\"],\n",
    "                # NEW: capture these so Cell 8 has them\n",
    "                \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": float(t2 - t1),\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c039220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_1ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_1ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_1ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_1ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_1ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_3ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_3ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_3ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_3ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved bundle → ./runs_halfqae_2L4T/q6_l2t4/6q_2l_4t_3ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "\n",
      "All runs saved and recorded.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 8 — Save artifacts (JSON) and append a paper-ready CSV per run\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams logged (keep in sync with training cells) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.003\n",
    "S3_MAX_EPOCHS    = 60\n",
    "S3_PATIENCE      = 10\n",
    "S3_PLATEAU_STEPS = 5\n",
    "S3_PLATEAU_FACT  = 0.5\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v5\"  # keep same as earlier runs so we reuse the same CSV\n",
    "\n",
    "# --- ensure dirs ---\n",
    "ensure_dir(OUT_BASE)\n",
    "# one folder per architecture (e.g., runs_halfqae/q4_l3t1)\n",
    "subroot = ensure_dir(f\"{OUT_BASE}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "\n",
    "# --- CSV path (shared across ALL architectures/runs) ---\n",
    "CSV_PATH = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "\n",
    "# --- header for the full, paper-friendly table ---\n",
    "CSV_HEADER = [\n",
    "    # id / naming\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    # architecture\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    # noise & window\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    # stage-1 hyperparams + outcomes\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    # stage-3 hyperparams + outcomes\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\"s3_patience\",\"s3_plateau_steps\",\"s3_plateau_factor\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    # params (JSON)\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    # totals\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    # Create only if missing; never rewrite an existing header.\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(header)\n",
    "\n",
    "ensure_csv(CSV_PATH, CSV_HEADER)\n",
    "\n",
    "# ------------------------ safe helpers ------------------------\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def _safe_int(x, default):\n",
    "    if x is None:\n",
    "        return default\n",
    "    try:\n",
    "        # catch \"nan\" float case\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return default\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _safe_float(x, default=np.nan):\n",
    "    if x is None:\n",
    "        return default\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def save_one_run(run):\n",
    "    global stage3_handles  # needed by stage3_val_values_det fallback\n",
    "\n",
    "    inst = int(run[\"instance_id\"])\n",
    "    L    = int(run[\"n_layers\"])\n",
    "    seed = int(run.get(\"seed\", inst))\n",
    "\n",
    "    # standardized filename includes arch + layers + instance, so no collisions\n",
    "    fname = std_instance_name(n_qubits, n_latent, n_trash, L, inst)\n",
    "\n",
    "    # Save all instances for this architecture in the same folder (no per-layer subfolders)\n",
    "    out_dir = subroot\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    # pull stage results (robust to missing keys / None)\n",
    "    s1 = run[\"stage1\"]\n",
    "    s3 = run[\"stage3\"]\n",
    "\n",
    "    # Stage-1 metrics\n",
    "    s1_hist_val = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val = _safe_float(s1.get(\"best_val\"), np.nan)\n",
    "    s1_final_val = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = _safe_int(s1.get(\"best_epoch\"), _safe_argmin(s1_hist_val))\n",
    "    s1_epochs     = _safe_int(s1.get(\"epochs\"), len(s1_hist_val))\n",
    "    s1_seconds    = _safe_float(s1.get(\"train_seconds\"), np.nan)\n",
    "\n",
    "    # Stage-3 series\n",
    "    s3_hist_val   = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "\n",
    "    s3_best_val   = _safe_float(s3.get(\"best_val\"), np.nan)\n",
    "    s3_final_val  = _safe_last(s3_hist_val)\n",
    "    s3_best_epoch = _safe_int(s3.get(\"best_epoch\"), _safe_argmin(s3_hist_val))\n",
    "    s3_epochs     = _safe_int(s3.get(\"epochs\"), len(s3_hist_val))\n",
    "    s3_seconds    = _safe_float(s3.get(\"train_seconds\"), np.nan)\n",
    "\n",
    "    # --- compute metrics with FALLBACKS if curves are missing ---\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = (float(np.nanmax(s3_hist_delta)) if (len(s3_hist_delta) and np.isfinite(np.nanmax(s3_hist_delta)))\n",
    "                      else np.nan)\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    need_fallback = (not len(s3_hist_noisy)) or (not np.isfinite(noisy_baseline)) or (not np.isfinite(final_delta))\n",
    "\n",
    "    if need_fallback:\n",
    "        # Rebuild the QNodes for this (L, phi) so we can evaluate psi on X_val\n",
    "        phi_for_L = np.array(s1.get(\"phi\", []))\n",
    "        stage3_handles = stage3_qnodes(L, phi_for_L)  # sets the fixed decoder from φ\n",
    "        psi_params = np.array(s3.get(\"psi\", []))\n",
    "        # Deterministic validation at σ = EVAL_SIGMA\n",
    "        mN, mD, d_pct = stage3_val_values_det(psi_params, X_val, sigma=EVAL_SIGMA)\n",
    "        noisy_baseline = float(mN)\n",
    "        final_delta    = float(d_pct)\n",
    "        if not np.isfinite(best_delta):  # if we don't have a curve, use final as best\n",
    "            best_delta = final_delta\n",
    "\n",
    "    # bundle JSON (parameters + training curves)\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"id\": data_folder,\n",
    "            \"scale_low\":  float(info[\"scale_low\"]),\n",
    "            \"scale_high\": float(info[\"scale_high\"]),\n",
    "            \"window_size\": int(n_qubits),\n",
    "            \"window_stride\": int(WINDOW_STRIDE),\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(EVAL_SIGMA),\n",
    "            \"sigma_eval\":  float(EVAL_SIGMA),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(n_qubits),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(n_latent),\n",
    "            \"n_trash\":  int(n_trash),\n",
    "            \"latent_wires\": list(range(n_latent)),\n",
    "            \"trash_wires\":  list(range(n_latent, n_qubits)),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"patience\": S3_PATIENCE, \"plateau_steps\": S3_PLATEAU_STEPS, \"plateau_factor\": S3_PLATEAU_FACT,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val,\n",
    "                \"noisy_curve\": s3.get(\"hist_noisy\", []), \"delta_curve\": s3_hist_delta,\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # assemble CSV row\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        data_folder,\n",
    "        inst, seed,\n",
    "        int(n_qubits), int(n_latent), int(n_trash), int(L),\n",
    "        f\"{EVAL_SIGMA:.3f}\", f\"{EVAL_SIGMA:.3f}\", int(WINDOW_STRIDE),\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS), int(S3_PATIENCE), int(S3_PLATEAU_STEPS), f\"{S3_PLATEAU_FACT:.3f}\",\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert row into CSV (by unique filename)\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(CSV_PATH).exists():\n",
    "        df_old = pd.read_csv(CSV_PATH)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(CSV_PATH, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Upserted CSV row  → {CSV_PATH}\")\n",
    "\n",
    "# ---- save all runs from Cell 7 ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved and recorded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "137c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae_2L4T/all_training_instances_v5.csv\n",
      "Saved per-layer summary → ./runs_halfqae_2L4T/summary_by_layers_v5.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6q_2l_4t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>45.063395</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>15.386458</td>\n",
       "      <td>14.893963</td>\n",
       "      <td>[1.1953313406396242e-05, -1.9386720918741835, ...</td>\n",
       "      <td>[-0.0034292122594210975, -1.9377256154978648, ...</td>\n",
       "      <td>116.919855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6q_2l_4t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>77.722276</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>18.312435</td>\n",
       "      <td>15.479385</td>\n",
       "      <td>[5.998694984280951e-06, -1.9177094851112595, 0...</td>\n",
       "      <td>[0.007757789476135215, -1.960299593341035, -0....</td>\n",
       "      <td>149.765730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6q_2l_4t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>61.311070</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>19.585907</td>\n",
       "      <td>17.557150</td>\n",
       "      <td>[6.2247027028887795e-06, 1.1698281688570384, 0...</td>\n",
       "      <td>[0.0025118106333826594, 1.1632349898883352, 0....</td>\n",
       "      <td>131.091355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6q_2l_4t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>83.175510</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>16.046416</td>\n",
       "      <td>15.708703</td>\n",
       "      <td>[0.0008426983327898159, -1.938211349456881, -0...</td>\n",
       "      <td>[-0.001547487907518119, -1.969858523696229, 0....</td>\n",
       "      <td>152.618318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6q_2l_4t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>61.173647</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>19.050344</td>\n",
       "      <td>18.622338</td>\n",
       "      <td>[1.9639351994848866e-05, -1.9725538965320986, ...</td>\n",
       "      <td>[-0.00514503448999216, -2.000050624335022, 0.3...</td>\n",
       "      <td>132.017926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6q_2l_4t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>137.788396</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>26.923059</td>\n",
       "      <td>18.342597</td>\n",
       "      <td>[1.8061457272615866, -1.6877912127944268, 1.31...</td>\n",
       "      <td>[2.359331222105529, -2.313642184503458, 0.6172...</td>\n",
       "      <td>321.862607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6q_2l_4t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>96.703910</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>16.266519</td>\n",
       "      <td>15.837423</td>\n",
       "      <td>[-0.016808627648464025, -0.1576437568966197, -...</td>\n",
       "      <td>[0.005006020328862956, -0.1215657194936346, -1...</td>\n",
       "      <td>279.048704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6q_2l_4t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>59.903825</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>4.983086</td>\n",
       "      <td>-11.437768</td>\n",
       "      <td>[1.41980459393824, -0.024631712374098003, 1.41...</td>\n",
       "      <td>[1.3777239146996079, -0.011025114402657315, 1....</td>\n",
       "      <td>240.099909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6q_2l_4t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>124.109225</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>20.781404</td>\n",
       "      <td>20.128560</td>\n",
       "      <td>[-1.590450979468099, -1.5914962442281613, 0.16...</td>\n",
       "      <td>[-1.6361208238498184, -1.601449807742929, 0.07...</td>\n",
       "      <td>311.792627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6q_2l_4t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>156.161839</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>22.092780</td>\n",
       "      <td>20.808540</td>\n",
       "      <td>[-1.3809726190280383, -0.05057157887741369, 0....</td>\n",
       "      <td>[-1.457859401290905, -0.029123654395085696, 0....</td>\n",
       "      <td>339.292391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag     dataset_folder  instance_id  rng_seed  \\\n",
       "0  6q_2l_4t_1ls_01.json  inst1_L1  mackey_glass_n100            1         1   \n",
       "1  6q_2l_4t_1ls_02.json  inst2_L1  mackey_glass_n100            2         2   \n",
       "2  6q_2l_4t_1ls_03.json  inst3_L1  mackey_glass_n100            3         3   \n",
       "3  6q_2l_4t_1ls_04.json  inst4_L1  mackey_glass_n100            4         4   \n",
       "4  6q_2l_4t_1ls_05.json  inst5_L1  mackey_glass_n100            5         5   \n",
       "5  6q_2l_4t_3ls_01.json  inst1_L3  mackey_glass_n100            1         1   \n",
       "6  6q_2l_4t_3ls_02.json  inst2_L3  mackey_glass_n100            2         2   \n",
       "7  6q_2l_4t_3ls_03.json  inst3_L3  mackey_glass_n100            3         3   \n",
       "8  6q_2l_4t_3ls_04.json  inst4_L3  mackey_glass_n100            4         4   \n",
       "9  6q_2l_4t_3ls_05.json  inst5_L3  mackey_glass_n100            5         5   \n",
       "\n",
       "   n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  s3_final_val_mse  \\\n",
       "0         6         2        4         1          0.1  ...          0.002957   \n",
       "1         6         2        4         1          0.1  ...          0.002937   \n",
       "2         6         2        4         1          0.1  ...          0.002865   \n",
       "3         6         2        4         1          0.1  ...          0.002929   \n",
       "4         6         2        4         1          0.1  ...          0.002828   \n",
       "5         6         2        4         3          0.1  ...          0.002837   \n",
       "6         6         2        4         3          0.1  ...          0.002924   \n",
       "7         6         2        4         3          0.1  ...          0.003872   \n",
       "8         6         2        4         3          0.1  ...          0.002775   \n",
       "9         6         2        4         3          0.1  ...          0.002752   \n",
       "\n",
       "   s3_best_epoch  s3_epochs  s3_train_seconds  s3_noisy_baseline_mse  \\\n",
       "0              9         20         45.063395               0.003475   \n",
       "1             23         34         77.722276               0.003475   \n",
       "2             17         28         61.311070               0.003475   \n",
       "3             27         38         83.175510               0.003475   \n",
       "4             17         28         61.173647               0.003475   \n",
       "5             14         25        137.788396               0.003475   \n",
       "6              7         18         96.703910               0.003475   \n",
       "7              0         11         59.903825               0.003475   \n",
       "8             11         22        124.109225               0.003475   \n",
       "9             18         29        156.161839               0.003475   \n",
       "\n",
       "   s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0          15.386458           14.893963   \n",
       "1          18.312435           15.479385   \n",
       "2          19.585907           17.557150   \n",
       "3          16.046416           15.708703   \n",
       "4          19.050344           18.622338   \n",
       "5          26.923059           18.342597   \n",
       "6          16.266519           15.837423   \n",
       "7           4.983086          -11.437768   \n",
       "8          20.781404           20.128560   \n",
       "9          22.092780           20.808540   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [1.1953313406396242e-05, -1.9386720918741835, ...   \n",
       "1  [5.998694984280951e-06, -1.9177094851112595, 0...   \n",
       "2  [6.2247027028887795e-06, 1.1698281688570384, 0...   \n",
       "3  [0.0008426983327898159, -1.938211349456881, -0...   \n",
       "4  [1.9639351994848866e-05, -1.9725538965320986, ...   \n",
       "5  [1.8061457272615866, -1.6877912127944268, 1.31...   \n",
       "6  [-0.016808627648464025, -0.1576437568966197, -...   \n",
       "7  [1.41980459393824, -0.024631712374098003, 1.41...   \n",
       "8  [-1.590450979468099, -1.5914962442281613, 0.16...   \n",
       "9  [-1.3809726190280383, -0.05057157887741369, 0....   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [-0.0034292122594210975, -1.9377256154978648, ...           116.919855  \n",
       "1  [0.007757789476135215, -1.960299593341035, -0....           149.765730  \n",
       "2  [0.0025118106333826594, 1.1632349898883352, 0....           131.091355  \n",
       "3  [-0.001547487907518119, -1.969858523696229, 0....           152.618318  \n",
       "4  [-0.00514503448999216, -2.000050624335022, 0.3...           132.017926  \n",
       "5  [2.359331222105529, -2.313642184503458, 0.6172...           321.862607  \n",
       "6  [0.005006020328862956, -0.1215657194936346, -1...           279.048704  \n",
       "7  [1.3777239146996079, -0.011025114402657315, 1....           240.099909  \n",
       "8  [-1.6361208238498184, -1.601449807742929, 0.07...           311.792627  \n",
       "9  [-1.457859401290905, -0.029123654395085696, 0....           339.292391  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003475 ± 0.000000</td>\n",
       "      <td>0.002860 ± 0.000058</td>\n",
       "      <td>0.002903 ± 0.000049</td>\n",
       "      <td>17.676312 ± 1.663680</td>\n",
       "      <td>16.452308 ± 1.404114</td>\n",
       "      <td>0.235555 ± 0.003976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003475 ± 0.000000</td>\n",
       "      <td>0.002842 ± 0.000258</td>\n",
       "      <td>0.003032 ± 0.000424</td>\n",
       "      <td>18.209370 ± 7.434163</td>\n",
       "      <td>12.735871 ± 12.208519</td>\n",
       "      <td>0.145439 ± 0.034227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.003475 ± 0.000000     0.002860 ± 0.000058   \n",
       "3            5           0.003475 ± 0.000000     0.002842 ± 0.000258   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.002903 ± 0.000049      17.676312 ± 1.663680   \n",
       "3             0.003032 ± 0.000424      18.209370 ± 7.434163   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1              16.452308 ± 1.404114    0.235555 ± 0.003976  \n",
       "3             12.735871 ± 12.208519    0.145439 ± 0.034227  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results table\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(CSV_PATH).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Run Cell 8 first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Drop duplicate runs; keep the newest copy (with baseline/delta)\n",
    "if \"filename\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "# Typical numeric casts (safe)\n",
    "for col in [\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\n",
    "    \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "    \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\",\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\"instance_id\",\"rng_seed\",\n",
    "    \"window_stride\"\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "\n",
    "clean_path = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "Path(OUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved training-only table → {clean_path}\")\n",
    "\n",
    "# A compact per-layer summary (mean±std); guards against all-NaN\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "metrics = [\n",
    "    (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "    (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "    (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "    (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "    (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "    (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "]\n",
    "\n",
    "grp = df.groupby(\"n_layers\", dropna=False)\n",
    "summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "for col, label in metrics:\n",
    "    if col in df.columns and np.isfinite(df[col]).any():\n",
    "        summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "summary_path = f\"{OUT_BASE}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "summary.to_csv(summary_path, index=True)\n",
    "print(f\"Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c34ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
