{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e979dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# We'll run 5 instances and two depths: 1 and 3 layers.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # used in filenames as ..._ls_01.json, ..._ls_02.json, ...\n",
    "LAYER_OPTIONS  = [1, 3]            # train 1-layer first, then 3-layers\n",
    "EVAL_SIGMA     = 0.10              # fixed noise everywhere (train & eval)\n",
    "\n",
    "# where to save artifacts (JSON bundles, instance records, CSV summary)\n",
    "# tip: new folder so these runs don't mix with your 2L/2T ones\n",
    "OUT_BASE = \"./runs_halfqae_4L2T\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458cfa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils import OK\n",
      "Seed/filename utils ready.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Cell 1 — Imports, utils, reproducibility (fixed seed)\n",
    "# =====================================================\n",
    "import os, sys, json, math, random, time, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- repo utils (your existing readers) -----\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "parent_dir = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'Jacob' else current_dir\n",
    "sys.path.insert(0, parent_dir); sys.path.insert(0, '../')\n",
    "try:\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Utils import OK\")\n",
    "except Exception as e:\n",
    "    print(\"Import error:\", e)\n",
    "    qae_utils_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(''))), 'qae_utils')\n",
    "    sys.path.insert(0, os.path.dirname(qae_utils_path))\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Absolute path fallback OK\")\n",
    "\n",
    "assert callable(read_ts_file) and callable(read_json_file)\n",
    "\n",
    "# ----- plotting defaults -----\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4)\n",
    "\n",
    "# ----- reproducibility -----\n",
    "def set_global_seed(instance_id: int):\n",
    "    \"\"\"\n",
    "    Derive all RNGs from a simple instance ID (1..5).\n",
    "    Keep the mapping stable across notebooks.\n",
    "    \"\"\"\n",
    "    base = 10_000 + int(instance_id)  # simple, memorable\n",
    "    random.seed(base + 11)\n",
    "    np.random.seed(base + 22)\n",
    "    try:\n",
    "        pnp.random.seed(base + 33)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Pennylane's default.qubit is deterministic given params; no device seeding needed.\n",
    "    return dict(global_seed=base, numpy_seed=base+22, pnp_seed=base+33)\n",
    "\n",
    "def std_instance_name(nq, n_latent, n_trash, n_layers, instance_id):\n",
    "    \"\"\"\n",
    "    Standardized filename pattern used across the project.\n",
    "    Example: 4q_2l_2t_3ls_01.json\n",
    "    \"\"\"\n",
    "    return f\"{int(nq)}q_{int(n_latent)}l_{int(n_trash)}t_{int(n_layers)}ls_{int(instance_id):02d}.json\"\n",
    "\n",
    "def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "print(\"Seed/filename utils ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a103b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: mackey_glass_n100\n",
      "Loaded 100 samples; scale [0.200,0.800]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 2 — Data loading (deterministic)\n",
    "# =========================================\n",
    "DATA_PATH = '../jacobs_examples/aintern/data'\n",
    "DATA_NAME = 'mackey_glass'  # your folder prefix\n",
    "\n",
    "# fixed split across all instances/layers (so results are comparable)\n",
    "SPLIT_RANDOM_STATE = 42\n",
    "WINDOW_STRIDE = 1\n",
    "\n",
    "# ---- pick most recent MG folder starting with mackey_glass* ----\n",
    "data_folders = [f for f in os.listdir(DATA_PATH) if f.startswith(DATA_NAME)]\n",
    "if not data_folders:\n",
    "    raise FileNotFoundError(\"No Mackey-Glass data found. Generate it first.\")\n",
    "data_folders.sort()\n",
    "data_folder = data_folders[-1]     # take the last one if multiple\n",
    "print(f\"Using data folder: {data_folder}\")\n",
    "\n",
    "# ---- load series + scaling info ----\n",
    "X_idx = read_ts_file(f'{DATA_PATH}/{data_folder}/x_org.arr')   # indices for plotting, not used\n",
    "y_all = read_ts_file(f'{DATA_PATH}/{data_folder}/y_org.arr')   # values\n",
    "info  = read_json_file(f'{DATA_PATH}/{data_folder}/info.json')\n",
    "\n",
    "print(f\"Loaded {len(y_all)} samples; scale [{info['scale_low']:.3f},{info['scale_high']:.3f}]\")\n",
    "\n",
    "# ---- helper: uniform embed wrapper (works with/without explicit info param)\n",
    "def embed_input(x, info_=None):\n",
    "    \"\"\"\n",
    "    Map value-domain window x (in [lo,hi]) to RY(π·v01).\n",
    "    Accepts optional info to match Stage-3 call signatures.\n",
    "    \"\"\"\n",
    "    if info_ is None:\n",
    "        info_ = info\n",
    "    lo, hi = info_['scale_low'], info_['scale_high']\n",
    "    xn = (pnp.array(x) - lo) / max(hi - lo, 1e-12)   # -> [0,1]\n",
    "    for i, v in enumerate(xn):\n",
    "        qml.RY(v * pnp.pi, wires=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2caea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture set: 6q (4 latent, 2 trash).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — Architecture (do NOT change your brick/entanglers)\n",
    "# ============================================================\n",
    "# This notebook = 6 qubits, 4 latent, 2 trash.\n",
    "n_qubits = 6\n",
    "n_latent = 4\n",
    "n_trash  = n_qubits - n_latent          # 2\n",
    "\n",
    "latent_wires = list(range(n_latent))              # [0,1,2,3]\n",
    "trash_wires  = list(range(n_latent, n_qubits))    # [4,5]\n",
    "signal_wires = list(range(n_qubits))              # [0..5] for diagnostics\n",
    "\n",
    "assert n_latent + n_trash == n_qubits\n",
    "\n",
    "# --- device factory (simple; default.qubit) ---\n",
    "def make_device(nq):\n",
    "    return qml.device('default.qubit', wires=nq)\n",
    "\n",
    "# --- Stage-1 encoder template (unchanged architecture) ---\n",
    "def encoder_template(params, n_layers):\n",
    "    \"\"\"RX/RY/RZ per qubit + ring CNOT per layer.\"\"\"\n",
    "    assert len(params) == n_layers * n_qubits * 3\n",
    "    for l in range(n_layers):\n",
    "        # local rotations\n",
    "        for q in range(n_qubits):\n",
    "            idx = l * n_qubits * 3 + q * 3\n",
    "            qml.RX(params[idx + 0], wires=q)\n",
    "            qml.RY(params[idx + 1], wires=q)\n",
    "            qml.RZ(params[idx + 2], wires=q)\n",
    "        # ring entanglers\n",
    "        for q in range(n_qubits - 1):\n",
    "            qml.CNOT(wires=[q, q + 1])\n",
    "        qml.CNOT(wires=[n_qubits - 1, 0])\n",
    "\n",
    "print(\"Architecture set: 6q (4 latent, 2 trash).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "539c5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Cell 4 — Stage-1 QNodes, loss, and training (seeded)\n",
    "# ====================================================\n",
    "def stage1_qnodes(n_layers):\n",
    "    dev = make_device(n_qubits)\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def trash_expectations(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in trash_wires]\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def recon_EdagE(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        qml.adjoint(encoder_template)(phi, n_layers)   # E†\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return trash_expectations, recon_EdagE\n",
    "\n",
    "def stage1_batch_loss(trash_expectations, phi, clean_batch):\n",
    "    # L = mean_{batch,trash} P(|1>) = (1 - Z)/2\n",
    "    zs = []\n",
    "    for c in clean_batch:\n",
    "        z = pnp.array(trash_expectations(phi, c))  # shape (n_trash,)\n",
    "        zs.append(z)\n",
    "    zs = pnp.stack(zs, axis=0)\n",
    "    prob_one = (1.0 - zs) * 0.5\n",
    "    return pnp.mean(prob_one)\n",
    "\n",
    "def train_stage1(X_train, X_val, n_layers, instance_id, \n",
    "                 n_epochs=120, batch_size=32, lr_init=0.010,\n",
    "                 patience=10, lr_patience=8, min_delta=1e-6):\n",
    "    set_global_seed(instance_id)\n",
    "    # init\n",
    "    enc_shape = n_layers * n_qubits * 3\n",
    "    phi = pnp.array(np.random.normal(0, 0.5, enc_shape), requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr_init)\n",
    "    lr = lr_init\n",
    "\n",
    "    trash_expectations, recon_EdagE = stage1_qnodes(n_layers)\n",
    "\n",
    "    # helper\n",
    "    def minibatches(N, B, rng_seed=123456):\n",
    "        rng = np.random.default_rng(rng_seed)  # fixed per-epoch seed below\n",
    "        idx = rng.permutation(N)\n",
    "        for i in range(0, N, B):\n",
    "            yield idx[i:i+B]\n",
    "\n",
    "    train_hist, val_hist, lr_hist = [], [], []\n",
    "    best_phi, best_val = None, float(\"inf\")\n",
    "    no_improve = 0\n",
    "    for ep in range(n_epochs):\n",
    "        # batch order deterministic per-epoch per-instance\n",
    "        seed_ep = 77_000 + 100*instance_id + ep\n",
    "        acc = 0.0; nb = 0\n",
    "        for ix in minibatches(len(X_train), batch_size, rng_seed=seed_ep):\n",
    "            clean_batch = X_train[ix]\n",
    "            def loss_fn(p): return stage1_batch_loss(trash_expectations, p, clean_batch)\n",
    "            phi, cost = opt.step_and_cost(loss_fn, phi)\n",
    "            acc += float(cost); nb += 1\n",
    "        train_cost = acc / max(nb, 1)\n",
    "\n",
    "        # validation\n",
    "        v_costs = []\n",
    "        for c in X_val:\n",
    "            v_costs.append(stage1_batch_loss(trash_expectations, phi, pnp.array([c])))\n",
    "        val_cost = float(pnp.mean(pnp.stack(v_costs)))\n",
    "\n",
    "        train_hist.append(train_cost); val_hist.append(val_cost); lr_hist.append(lr)\n",
    "\n",
    "        if val_cost + min_delta < best_val:\n",
    "            best_val, best_phi = val_cost, pnp.array(phi, requires_grad=False); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % lr_patience) == 0:\n",
    "                lr = max(lr * 0.5, 1e-4)\n",
    "                opt = qml.AdamOptimizer(stepsize=lr)\n",
    "                print(f\"[Stage1] ↓ LR → {lr:.5f}\")\n",
    "            if no_improve >= patience:\n",
    "                print(\"[Stage1] Early stopping.\"); break\n",
    "\n",
    "        print(f\"[Stage1] L={n_layers} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {lr:.5f}\")\n",
    "\n",
    "    phi_best = best_phi if best_phi is not None else phi\n",
    "    return dict(\n",
    "        phi=phi_best, best_val=float(best_val),\n",
    "        hist_train=list(map(float, train_hist)),\n",
    "        hist_val=list(map(float, val_hist)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        recon_EdagE=recon_EdagE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3943eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Cell 5 — Stage-3 (ψ) with fixed decoder = adjoint(Stage-1 encoder φ)\n",
    "#          (records hist_noisy, hist_delta, best_epoch, epochs, seconds)\n",
    "# ======================================================================\n",
    "import time\n",
    "\n",
    "def stage3_qnodes(n_layers, phi_stage1):\n",
    "    dev3 = make_device(n_qubits)\n",
    "    theta_fixed = pnp.array(phi_stage1, requires_grad=False).reshape((n_layers, n_qubits, 3))\n",
    "\n",
    "    def encoder_fixed_body(theta):\n",
    "        for l in range(n_layers):\n",
    "            for q in range(n_qubits):\n",
    "                qml.RX(theta[l, q, 0], wires=q)\n",
    "                qml.RY(theta[l, q, 1], wires=q)\n",
    "                qml.RZ(theta[l, q, 2], wires=q)\n",
    "            for q in range(n_qubits-1):\n",
    "                qml.CNOT(wires=[q, q+1])\n",
    "            qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "    def decoder_fixed():\n",
    "        qml.adjoint(encoder_fixed_body)(theta_fixed)\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def encoder_only_expZ_all(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def teacher_code_latents(x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_fixed_body(theta_fixed)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def student_code_latents(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def denoiser_qnode_all(flat_params, x_noisy):\n",
    "        embed_input(x_noisy)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        decoder_fixed()\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return dict(\n",
    "        theta_fixed=theta_fixed,\n",
    "        encoder_only_expZ_all=encoder_only_expZ_all,\n",
    "        teacher_code_latents=teacher_code_latents,\n",
    "        student_code_latents=student_code_latents,\n",
    "        denoiser_qnode_all=denoiser_qnode_all\n",
    "    )\n",
    "\n",
    "# ----- value readout helpers (unchanged)\n",
    "def Z_to_values_autograd(z_all):\n",
    "    z_all = pnp.clip(pnp.asarray(z_all), -0.999999, 0.999999)\n",
    "    v01 = pnp.arccos(z_all) / pnp.pi\n",
    "    return v01 * (info[\"scale_high\"] - info[\"scale_low\"]) + info[\"scale_low\"]\n",
    "\n",
    "def first_diff(x): \n",
    "    x = pnp.array(x); return x[1:] - x[:-1]\n",
    "\n",
    "def p1_from_expZ(z): \n",
    "    return (1 - pnp.asarray(z)) * 0.5\n",
    "\n",
    "# ----- deterministic noisy window (shared with eval)\n",
    "def ts_add_noise_window_det(x, sigma, seed):\n",
    "    low, high = float(info[\"scale_low\"]), float(info[\"scale_high\"])\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    noise = rng.normal(0.0, sigma * (high - low), size=np.asarray(x).shape)\n",
    "    return np.clip(np.asarray(x) + noise, low, high)\n",
    "\n",
    "# ----- validation with fixed per-window seeds (unchanged)\n",
    "VAL_BASE_SEED = 12345\n",
    "\n",
    "def stage3_val_values_det(psi, X_clean, sigma=EVAL_SIGMA):\n",
    "    ms_noisy, ms_deno = [], []\n",
    "    for i, c in enumerate(X_clean):\n",
    "        n = ts_add_noise_window_det(c, sigma, seed=VAL_BASE_SEED + i)\n",
    "        zD = np.array(stage3_handles[\"denoiser_qnode_all\"](psi, n))\n",
    "        v_hat = np.array(Z_to_values_autograd(zD))\n",
    "        ms_noisy.append(np.mean((np.asarray(c) - np.asarray(n))**2))\n",
    "        ms_deno.append(np.mean((np.asarray(c) - v_hat)**2))\n",
    "    mN, mD = float(np.mean(ms_noisy)), float(np.mean(ms_deno))\n",
    "    d_pct = 100.0 * (1.0 - mD / max(mN, 1e-12))\n",
    "    return mN, mD, d_pct\n",
    "\n",
    "# ----- small Huber\n",
    "def huber(residual, delta):\n",
    "    r = pnp.abs(residual)\n",
    "    return pnp.where(r <= delta, 0.5*r**2, delta*(r - 0.5*delta))\n",
    "\n",
    "\n",
    "def train_stage3(X_train, X_val, phi_stage1, n_layers, instance_id,\n",
    "                 TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16, \n",
    "                 LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "                 CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99):\n",
    "\n",
    "    # ---- seeds: varied but reproducible across (instance, layers, epoch, window)\n",
    "    def make_train_seed(instance_id, layers, ep, k, view=0):\n",
    "        return (1_000_003 * (instance_id * 10 + layers) + 97 * ep + 31 * int(k) + view) % 2_147_483_647\n",
    "\n",
    "    set_global_seed(instance_id)\n",
    "\n",
    "    global stage3_handles\n",
    "    stage3_handles = stage3_qnodes(n_layers, phi_stage1)\n",
    "    enc_all = stage3_handles[\"encoder_only_expZ_all\"]\n",
    "    teacher_lat = stage3_handles[\"teacher_code_latents\"]\n",
    "    denoise_all = stage3_handles[\"denoiser_qnode_all\"]\n",
    "\n",
    "    # ---- init ψ near φ\n",
    "    phi_flat = pnp.array(phi_stage1, requires_grad=False)\n",
    "    psi = pnp.array(np.array(phi_flat) + 0.05*np.random.randn(len(phi_flat)), requires_grad=True)\n",
    "\n",
    "    # ---- loss weights\n",
    "    ALPHA_REC, BETA_TF, GAMMA_TRASH, L_TV, L_ANCH = 1.0, 0.05, 0.5, 0.05, 2e-4\n",
    "    DELTA_TV, DELTA_Z = 0.02, 0.25\n",
    "\n",
    "    # loss on a single window with a specific noise seed\n",
    "    def loss_on_window_seeded(params, clean_values, seed):\n",
    "        v_noisy = pnp.array(ts_add_noise_window_det(clean_values, TARGET_NOISE, seed=seed))\n",
    "        z_all = pnp.array(enc_all(params, v_noisy))\n",
    "        z_sig, z_tr = z_all[:n_latent], z_all[n_latent:]\n",
    "        zD = pnp.array(denoise_all(params, v_noisy))\n",
    "        v_hat = Z_to_values_autograd(zD)\n",
    "\n",
    "        L_rec = pnp.mean((pnp.array(clean_values) - v_hat)**2)\n",
    "        z_t_sig = pnp.array(teacher_lat(clean_values))\n",
    "        L_tf = pnp.mean(huber(z_t_sig - z_sig, DELTA_Z))\n",
    "        L_tr = pnp.mean(p1_from_expZ(z_tr))\n",
    "        L_tv = pnp.mean(huber(first_diff(clean_values) - first_diff(v_hat), DELTA_TV))\n",
    "        L_anchor = pnp.mean((params - phi_flat)**2)\n",
    "        return (ALPHA_REC*L_rec + BETA_TF*L_tf + GAMMA_TRASH*L_tr + L_TV*L_tv + L_ANCH*L_anchor)\n",
    "\n",
    "    # manual Adam\n",
    "    m = pnp.zeros_like(psi); v = pnp.zeros_like(psi)\n",
    "    b1, b2, eps = 0.9, 0.999, 1e-8\n",
    "    t = 0\n",
    "    def adam_step(params, grad, lr):\n",
    "        nonlocal m, v, t\n",
    "        t += 1\n",
    "        m = b1*m + (1-b1)*grad\n",
    "        v = b2*v + (1-b2)*(grad*grad)\n",
    "        mhat = m/(1-b1**t); vhat = v/(1-b2**t)\n",
    "        return params - lr * (mhat/(pnp.sqrt(vhat)+eps))\n",
    "\n",
    "    # batches deterministic per-epoch\n",
    "    def batch_indices(N, B, ep_seed):\n",
    "        rng = np.random.default_rng(ep_seed)\n",
    "        idx = rng.permutation(N)\n",
    "        for s in range(0, N, B):\n",
    "            yield idx[s:s+B]\n",
    "\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve, lr = 0, LR_START\n",
    "    ema = pnp.array(psi, requires_grad=False) if USE_EMA else None\n",
    "\n",
    "    # history buffers (for CSV/reporting)\n",
    "    hist_train, hist_val = [], []\n",
    "    hist_noisy, hist_delta = [], []\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(MAX_EPOCHS):\n",
    "        seed_ep = 88_000 + 100*instance_id + ep  # reproducible shuffling\n",
    "        acc, nb = 0.0, 0\n",
    "        for ix in batch_indices(len(X_train), BATCH, ep_seed=seed_ep):\n",
    "            for k in ix:                                   # k = absolute index in X_train\n",
    "                c = X_train[k]\n",
    "                seed = make_train_seed(instance_id, n_layers, ep, int(k))\n",
    "                L = loss_on_window_seeded(psi, c, seed)\n",
    "                if not pnp.isfinite(L): \n",
    "                    continue\n",
    "                g = qml.grad(lambda p: loss_on_window_seeded(p, c, seed))(psi)\n",
    "                if not pnp.all(pnp.isfinite(g)): \n",
    "                    continue\n",
    "                # clip\n",
    "                gnorm = pnp.linalg.norm(g) + 1e-12\n",
    "                if gnorm > CLIP_NORM:\n",
    "                    g = g * (CLIP_NORM / gnorm)\n",
    "                psi = adam_step(psi, g, lr)\n",
    "                if USE_EMA: \n",
    "                    ema = EMA_DECAY*ema + (1-EMA_DECAY)*psi\n",
    "                acc += float(L); nb += 1\n",
    "\n",
    "        train_loss = acc / max(nb, 1)\n",
    "        eval_params = ema if USE_EMA else psi\n",
    "\n",
    "        # strict value-domain validation at σ=EVAL_SIGMA (deterministic per window)\n",
    "        mN, mD, dV = stage3_val_values_det(eval_params, X_val, sigma=EVAL_SIGMA)\n",
    "        hist_train.append(train_loss); hist_val.append(mD)\n",
    "        hist_noisy.append(mN);        hist_delta.append(dV)\n",
    "\n",
    "        if mD < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = mD, pnp.array(eval_params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % PLATEAU_STEPS) == 0:\n",
    "                lr *= PLATEAU_FACTOR\n",
    "                print(f\"[Stage3] Plateau → LR {lr:.5f}\")\n",
    "\n",
    "        norm_diff = float(pnp.linalg.norm((eval_params - phi_flat)))\n",
    "        print(f\"[Stage3] L={n_layers} ep {ep:03d} | train {train_loss:.5f} | \"\n",
    "              f\"val {mD:.5f} | noisy {mN:.5f} | Δ {dV:+.1f}% | LR {lr:.5f} | ||ψ-φ|| {norm_diff:.3f}\")\n",
    "\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"[Stage3] Early stopping.\"); break\n",
    "\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    epochs_run = len(hist_val)\n",
    "\n",
    "    psi_best = best_params if best_params is not None else (ema if USE_EMA else psi)\n",
    "\n",
    "    return dict(\n",
    "        psi=psi_best, \n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=int(epochs_run),\n",
    "        hist_train=list(map(float, hist_train)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50129aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows built: 95 (W=6, step=1)\n",
      "Split sizes → train=57, val=19, test=19\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# Cell 6 — Build windows & deterministic train/val/test\n",
    "# ===================================================\n",
    "window_size = n_qubits\n",
    "stride = WINDOW_STRIDE\n",
    "\n",
    "X_windows = np.array([y_all[i:i+window_size] for i in range(0, len(y_all)-window_size+1, stride)], dtype=float)\n",
    "print(f\"Total windows built: {len(X_windows)} (W={window_size}, step={stride})\")\n",
    "\n",
    "# 60/20/20 split (deterministic)\n",
    "X_temp, X_test = train_test_split(X_windows, test_size=0.20, random_state=SPLIT_RANDOM_STATE)\n",
    "X_train, X_val = train_test_split(X_temp,   test_size=0.25, random_state=SPLIT_RANDOM_STATE)  # 0.25 of 0.8 = 0.2\n",
    "print(f\"Split sizes → train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6711326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.498918 | val 0.500289 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.498117 | val 0.499305 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.497213 | val 0.498332 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.495962 | val 0.497362 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.494200 | val 0.496385 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.494121 | val 0.495371 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.493031 | val 0.494329 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.492270 | val 0.493239 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.491296 | val 0.492109 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.489756 | val 0.490930 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.487280 | val 0.489714 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.487728 | val 0.488425 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.486436 | val 0.487088 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.483746 | val 0.485687 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.483011 | val 0.484181 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.481104 | val 0.482575 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.480783 | val 0.480851 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.478358 | val 0.479022 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.476689 | val 0.477105 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.475557 | val 0.475046 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.476105 | val 0.472883 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.472200 | val 0.470642 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.470162 | val 0.468353 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.468333 | val 0.465937 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.465331 | val 0.463464 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.463276 | val 0.460913 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.462099 | val 0.458298 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.459799 | val 0.455765 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.458918 | val 0.453185 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.456712 | val 0.450542 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.453438 | val 0.447961 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.451274 | val 0.445442 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.449754 | val 0.442908 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.445195 | val 0.440502 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.444048 | val 0.438140 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.438512 | val 0.435930 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.439404 | val 0.433763 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.439137 | val 0.431751 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.435902 | val 0.429901 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.434782 | val 0.428239 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.430791 | val 0.426616 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.429361 | val 0.425088 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.429439 | val 0.423584 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.424112 | val 0.422097 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.424268 | val 0.420711 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.424433 | val 0.419390 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.423124 | val 0.418078 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.416274 | val 0.416843 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.417409 | val 0.415752 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.415410 | val 0.414667 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.414005 | val 0.413600 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.411223 | val 0.412611 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.415251 | val 0.411757 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.408068 | val 0.410886 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.404688 | val 0.410103 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.402894 | val 0.409324 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.403447 | val 0.408563 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.400891 | val 0.407891 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.404260 | val 0.407236 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.399474 | val 0.406621 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.393514 | val 0.406027 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.400005 | val 0.405456 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.396700 | val 0.404879 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.392262 | val 0.404298 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.394608 | val 0.403737 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.393746 | val 0.403169 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.395255 | val 0.402597 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.389820 | val 0.402034 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.385174 | val 0.401445 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.387315 | val 0.400889 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.387281 | val 0.400293 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.380649 | val 0.399646 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.385875 | val 0.398921 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.384809 | val 0.398168 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.379279 | val 0.397378 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.378164 | val 0.396491 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.373862 | val 0.395502 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.378406 | val 0.394462 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.376297 | val 0.393292 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.373370 | val 0.392117 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.374965 | val 0.390777 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.370101 | val 0.389279 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.367789 | val 0.387613 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.371704 | val 0.385904 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.370261 | val 0.384082 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.369873 | val 0.382126 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.361385 | val 0.379927 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.361281 | val 0.377539 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.359476 | val 0.375083 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.358526 | val 0.372298 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.357351 | val 0.369455 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.358665 | val 0.366503 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.356751 | val 0.363483 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.352119 | val 0.360095 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.351669 | val 0.356530 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.352961 | val 0.352823 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.347228 | val 0.348933 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.348759 | val 0.345057 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.347108 | val 0.340932 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.342826 | val 0.336527 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.344206 | val 0.332265 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.334050 | val 0.328155 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.339309 | val 0.323785 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.337106 | val 0.319443 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.332776 | val 0.315091 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.330884 | val 0.310654 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.334816 | val 0.306350 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.330570 | val 0.302280 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.329823 | val 0.298219 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.325841 | val 0.294256 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.327802 | val 0.290470 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.325672 | val 0.286775 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.322896 | val 0.283335 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.319620 | val 0.279904 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.320812 | val 0.276773 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.323288 | val 0.273717 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.317049 | val 0.270890 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.319931 | val 0.268265 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.318039 | val 0.265954 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.322324 | val 0.263867 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.17831 | val 0.00338 | noisy 0.00347 | Δ +2.8% | LR 0.00300 | ||ψ-φ|| 0.205\n",
      "[Stage3] L=1 ep 001 | train 0.18546 | val 0.00336 | noisy 0.00347 | Δ +3.4% | LR 0.00300 | ||ψ-φ|| 0.258\n",
      "[Stage3] L=1 ep 002 | train 0.18832 | val 0.00332 | noisy 0.00347 | Δ +4.5% | LR 0.00300 | ||ψ-φ|| 0.353\n",
      "[Stage3] L=1 ep 003 | train 0.18913 | val 0.00324 | noisy 0.00347 | Δ +6.8% | LR 0.00300 | ||ψ-φ|| 0.460\n",
      "[Stage3] L=1 ep 004 | train 0.18307 | val 0.00318 | noisy 0.00347 | Δ +8.4% | LR 0.00300 | ||ψ-φ|| 0.565\n",
      "[Stage3] L=1 ep 005 | train 0.18330 | val 0.00314 | noisy 0.00347 | Δ +9.7% | LR 0.00300 | ||ψ-φ|| 0.674\n",
      "[Stage3] L=1 ep 006 | train 0.18581 | val 0.00309 | noisy 0.00347 | Δ +11.2% | LR 0.00300 | ||ψ-φ|| 0.778\n",
      "[Stage3] L=1 ep 007 | train 0.18608 | val 0.00305 | noisy 0.00347 | Δ +12.2% | LR 0.00300 | ||ψ-φ|| 0.860\n",
      "[Stage3] L=1 ep 008 | train 0.18651 | val 0.00303 | noisy 0.00347 | Δ +12.7% | LR 0.00300 | ||ψ-φ|| 0.925\n",
      "[Stage3] L=1 ep 009 | train 0.17815 | val 0.00303 | noisy 0.00347 | Δ +12.9% | LR 0.00300 | ||ψ-φ|| 0.981\n",
      "[Stage3] L=1 ep 010 | train 0.18159 | val 0.00305 | noisy 0.00347 | Δ +12.1% | LR 0.00300 | ||ψ-φ|| 1.029\n",
      "[Stage3] L=1 ep 011 | train 0.17337 | val 0.00307 | noisy 0.00347 | Δ +11.7% | LR 0.00300 | ||ψ-φ|| 1.059\n",
      "[Stage3] L=1 ep 012 | train 0.18708 | val 0.00309 | noisy 0.00347 | Δ +11.1% | LR 0.00300 | ||ψ-φ|| 1.089\n",
      "[Stage3] L=1 ep 013 | train 0.18778 | val 0.00312 | noisy 0.00347 | Δ +10.3% | LR 0.00300 | ||ψ-φ|| 1.121\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 014 | train 0.17928 | val 0.00312 | noisy 0.00347 | Δ +10.1% | LR 0.00150 | ||ψ-φ|| 1.159\n",
      "[Stage3] L=1 ep 015 | train 0.18331 | val 0.00313 | noisy 0.00347 | Δ +9.8% | LR 0.00150 | ||ψ-φ|| 1.186\n",
      "[Stage3] L=1 ep 016 | train 0.18076 | val 0.00312 | noisy 0.00347 | Δ +10.1% | LR 0.00150 | ||ψ-φ|| 1.205\n",
      "[Stage3] L=1 ep 017 | train 0.18362 | val 0.00312 | noisy 0.00347 | Δ +10.3% | LR 0.00150 | ||ψ-φ|| 1.215\n",
      "[Stage3] L=1 ep 018 | train 0.18341 | val 0.00309 | noisy 0.00347 | Δ +11.1% | LR 0.00150 | ||ψ-φ|| 1.222\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 019 | train 0.18048 | val 0.00308 | noisy 0.00347 | Δ +11.3% | LR 0.00075 | ||ψ-φ|| 1.225\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.495418 | val 0.497049 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.494796 | val 0.496757 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.494936 | val 0.496427 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.493011 | val 0.496080 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.492148 | val 0.495702 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.492909 | val 0.495276 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.490815 | val 0.494827 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.490370 | val 0.494307 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.488049 | val 0.493737 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.488524 | val 0.493084 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.486130 | val 0.492370 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.485922 | val 0.491556 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.483933 | val 0.490632 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.482392 | val 0.489599 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.480936 | val 0.488438 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.479418 | val 0.487122 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.477862 | val 0.485647 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.478062 | val 0.483994 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.474297 | val 0.482186 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.472284 | val 0.480218 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.470844 | val 0.478083 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.469720 | val 0.475814 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.469797 | val 0.473417 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.465933 | val 0.470936 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.460528 | val 0.468346 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.458552 | val 0.465717 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.461686 | val 0.463070 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.458788 | val 0.460434 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.453613 | val 0.457755 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.448837 | val 0.455123 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.446551 | val 0.452493 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.445482 | val 0.449874 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.444153 | val 0.447321 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.444900 | val 0.444764 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.437013 | val 0.442304 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.438881 | val 0.439878 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.435773 | val 0.437465 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.428023 | val 0.435226 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.431317 | val 0.433087 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.429474 | val 0.431040 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.427439 | val 0.429056 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.426034 | val 0.427143 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.421171 | val 0.425256 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.423139 | val 0.423443 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.421812 | val 0.421722 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.417176 | val 0.420125 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.421542 | val 0.418585 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.415452 | val 0.417139 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.411587 | val 0.415774 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.414723 | val 0.414463 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.408708 | val 0.413236 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.406666 | val 0.412077 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.409359 | val 0.410959 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.406735 | val 0.409976 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.403110 | val 0.408992 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.402207 | val 0.408125 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.394644 | val 0.407306 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.401260 | val 0.406524 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.397250 | val 0.405779 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.391690 | val 0.405076 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.394162 | val 0.404454 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.392773 | val 0.403822 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.393390 | val 0.403195 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.387407 | val 0.402570 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.393076 | val 0.401942 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.388460 | val 0.401297 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.388158 | val 0.400631 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.387615 | val 0.399943 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.384434 | val 0.399222 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.381644 | val 0.398430 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.379279 | val 0.397580 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.379110 | val 0.396653 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.377465 | val 0.395608 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.380865 | val 0.394504 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.375665 | val 0.393316 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.374144 | val 0.391968 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.374650 | val 0.390469 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.369378 | val 0.388816 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.375036 | val 0.387058 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.369582 | val 0.385199 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.368352 | val 0.383116 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.363020 | val 0.380770 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.363539 | val 0.378254 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.364181 | val 0.375648 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.358875 | val 0.372732 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.356803 | val 0.369775 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.356326 | val 0.366669 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.354902 | val 0.363294 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.352322 | val 0.359728 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.351835 | val 0.355913 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.349474 | val 0.351837 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.350630 | val 0.347845 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.346063 | val 0.343743 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.347741 | val 0.339532 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.344796 | val 0.335249 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.340316 | val 0.330843 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.340408 | val 0.326198 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.342429 | val 0.321658 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.333915 | val 0.316861 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.335743 | val 0.312481 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.332111 | val 0.307946 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.332822 | val 0.303457 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.330675 | val 0.299144 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.326436 | val 0.295061 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.325745 | val 0.291050 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.322314 | val 0.287248 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.327729 | val 0.283514 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.327325 | val 0.279991 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.321864 | val 0.276820 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.321173 | val 0.273869 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.319650 | val 0.271210 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.321795 | val 0.268638 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.316091 | val 0.266236 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.321561 | val 0.263954 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.319679 | val 0.261982 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.318823 | val 0.260121 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.315591 | val 0.258510 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.319049 | val 0.257068 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.322544 | val 0.255694 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.320378 | val 0.254600 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.17706 | val 0.00355 | noisy 0.00347 | Δ -2.2% | LR 0.00300 | ||ψ-φ|| 0.199\n",
      "[Stage3] L=1 ep 001 | train 0.18352 | val 0.00351 | noisy 0.00347 | Δ -1.0% | LR 0.00300 | ||ψ-φ|| 0.236\n",
      "[Stage3] L=1 ep 002 | train 0.18761 | val 0.00343 | noisy 0.00347 | Δ +1.3% | LR 0.00300 | ||ψ-φ|| 0.314\n",
      "[Stage3] L=1 ep 003 | train 0.18168 | val 0.00331 | noisy 0.00347 | Δ +4.6% | LR 0.00300 | ||ψ-φ|| 0.427\n",
      "[Stage3] L=1 ep 004 | train 0.18154 | val 0.00320 | noisy 0.00347 | Δ +8.0% | LR 0.00300 | ||ψ-φ|| 0.542\n",
      "[Stage3] L=1 ep 005 | train 0.18302 | val 0.00311 | noisy 0.00347 | Δ +10.6% | LR 0.00300 | ||ψ-φ|| 0.654\n",
      "[Stage3] L=1 ep 006 | train 0.17997 | val 0.00304 | noisy 0.00347 | Δ +12.4% | LR 0.00300 | ||ψ-φ|| 0.745\n",
      "[Stage3] L=1 ep 007 | train 0.18333 | val 0.00301 | noisy 0.00347 | Δ +13.4% | LR 0.00300 | ||ψ-φ|| 0.828\n",
      "[Stage3] L=1 ep 008 | train 0.18232 | val 0.00297 | noisy 0.00347 | Δ +14.6% | LR 0.00300 | ||ψ-φ|| 0.888\n",
      "[Stage3] L=1 ep 009 | train 0.17851 | val 0.00291 | noisy 0.00347 | Δ +16.1% | LR 0.00300 | ||ψ-φ|| 0.936\n",
      "[Stage3] L=1 ep 010 | train 0.19191 | val 0.00287 | noisy 0.00347 | Δ +17.4% | LR 0.00300 | ||ψ-φ|| 0.981\n",
      "[Stage3] L=1 ep 011 | train 0.18754 | val 0.00283 | noisy 0.00347 | Δ +18.7% | LR 0.00300 | ||ψ-φ|| 1.033\n",
      "[Stage3] L=1 ep 012 | train 0.18153 | val 0.00282 | noisy 0.00347 | Δ +18.7% | LR 0.00300 | ||ψ-φ|| 1.062\n",
      "[Stage3] L=1 ep 013 | train 0.18337 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00300 | ||ψ-φ|| 1.086\n",
      "[Stage3] L=1 ep 014 | train 0.18136 | val 0.00286 | noisy 0.00347 | Δ +17.8% | LR 0.00300 | ||ψ-φ|| 1.111\n",
      "[Stage3] L=1 ep 015 | train 0.17909 | val 0.00286 | noisy 0.00347 | Δ +17.7% | LR 0.00300 | ||ψ-φ|| 1.118\n",
      "[Stage3] L=1 ep 016 | train 0.18256 | val 0.00285 | noisy 0.00347 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 1.126\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 017 | train 0.18237 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00150 | ||ψ-φ|| 1.130\n",
      "[Stage3] L=1 ep 018 | train 0.17621 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00150 | ||ψ-φ|| 1.136\n",
      "[Stage3] L=1 ep 019 | train 0.18382 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00150 | ||ψ-φ|| 1.142\n",
      "[Stage3] L=1 ep 020 | train 0.17690 | val 0.00283 | noisy 0.00347 | Δ +18.7% | LR 0.00150 | ||ψ-φ|| 1.149\n",
      "[Stage3] L=1 ep 021 | train 0.17664 | val 0.00282 | noisy 0.00347 | Δ +18.7% | LR 0.00150 | ||ψ-φ|| 1.148\n",
      "[Stage3] L=1 ep 022 | train 0.18229 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00150 | ||ψ-φ|| 1.150\n",
      "[Stage3] L=1 ep 023 | train 0.17939 | val 0.00282 | noisy 0.00347 | Δ +18.9% | LR 0.00150 | ||ψ-φ|| 1.148\n",
      "[Stage3] L=1 ep 024 | train 0.18635 | val 0.00283 | noisy 0.00347 | Δ +18.7% | LR 0.00150 | ||ψ-φ|| 1.149\n",
      "[Stage3] L=1 ep 025 | train 0.18049 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00150 | ||ψ-φ|| 1.156\n",
      "[Stage3] L=1 ep 026 | train 0.17736 | val 0.00284 | noisy 0.00347 | Δ +18.2% | LR 0.00150 | ||ψ-φ|| 1.158\n",
      "[Stage3] L=1 ep 027 | train 0.18455 | val 0.00285 | noisy 0.00347 | Δ +17.9% | LR 0.00150 | ||ψ-φ|| 1.161\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 028 | train 0.17489 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00075 | ||ψ-φ|| 1.164\n",
      "[Stage3] L=1 ep 029 | train 0.18532 | val 0.00289 | noisy 0.00347 | Δ +16.9% | LR 0.00075 | ||ψ-φ|| 1.166\n",
      "[Stage3] L=1 ep 030 | train 0.17760 | val 0.00290 | noisy 0.00347 | Δ +16.6% | LR 0.00075 | ||ψ-φ|| 1.169\n",
      "[Stage3] L=1 ep 031 | train 0.17973 | val 0.00290 | noisy 0.00347 | Δ +16.4% | LR 0.00075 | ||ψ-φ|| 1.173\n",
      "[Stage3] L=1 ep 032 | train 0.18411 | val 0.00291 | noisy 0.00347 | Δ +16.1% | LR 0.00075 | ||ψ-φ|| 1.176\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=1 ep 033 | train 0.17432 | val 0.00292 | noisy 0.00347 | Δ +16.0% | LR 0.00038 | ||ψ-φ|| 1.178\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.492188 | val 0.500133 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.491235 | val 0.499832 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.490163 | val 0.499526 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.490620 | val 0.499220 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.487835 | val 0.498915 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.487365 | val 0.498596 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.484848 | val 0.498258 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.484603 | val 0.497886 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.484290 | val 0.497474 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.482399 | val 0.497012 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.480100 | val 0.496493 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.480911 | val 0.495858 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.478277 | val 0.495132 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.476923 | val 0.494255 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.473969 | val 0.493235 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.472542 | val 0.492015 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.472197 | val 0.490559 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.468274 | val 0.488881 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.465680 | val 0.486930 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.463767 | val 0.484698 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.460401 | val 0.482193 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.457484 | val 0.479356 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.454123 | val 0.476200 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.448741 | val 0.472767 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.449871 | val 0.469002 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.441034 | val 0.464954 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.438766 | val 0.460637 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.433319 | val 0.456033 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.430517 | val 0.451183 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.428571 | val 0.446044 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.417879 | val 0.440741 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.416534 | val 0.435270 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.411280 | val 0.429617 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.406048 | val 0.423863 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.403468 | val 0.417963 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.398249 | val 0.412006 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.397287 | val 0.406042 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.390948 | val 0.400277 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.387175 | val 0.394570 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.381920 | val 0.389018 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.377794 | val 0.383617 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.371952 | val 0.378474 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.372034 | val 0.373517 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.364895 | val 0.368836 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.364846 | val 0.364418 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.366603 | val 0.360368 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.360981 | val 0.356365 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.357213 | val 0.352475 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.358787 | val 0.348725 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.358843 | val 0.345098 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.352549 | val 0.341660 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.353445 | val 0.338290 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.347172 | val 0.334882 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.349878 | val 0.331551 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.343195 | val 0.328285 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.347089 | val 0.324902 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.346619 | val 0.321578 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.340310 | val 0.318385 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.344820 | val 0.315205 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.341414 | val 0.312036 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.337266 | val 0.309079 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.343883 | val 0.306211 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.335879 | val 0.303381 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.333053 | val 0.300612 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.331330 | val 0.297820 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.332288 | val 0.294944 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.329978 | val 0.292150 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.333597 | val 0.289650 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.329792 | val 0.286994 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.329754 | val 0.284492 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.329746 | val 0.281810 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.323516 | val 0.279407 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.329815 | val 0.276992 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.326677 | val 0.274598 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.331068 | val 0.272262 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.326836 | val 0.270264 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.329315 | val 0.268347 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.326979 | val 0.266396 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.323215 | val 0.264725 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.315704 | val 0.263177 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.321136 | val 0.261589 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.318707 | val 0.260243 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.321779 | val 0.259098 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.320902 | val 0.258045 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.323546 | val 0.257014 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.316860 | val 0.256137 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.317085 | val 0.255249 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.317476 | val 0.254432 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.318675 | val 0.253678 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.316868 | val 0.253025 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.322413 | val 0.252505 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.313630 | val 0.251932 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.317363 | val 0.251377 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.316470 | val 0.250900 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.320043 | val 0.250547 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.320070 | val 0.250341 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.313517 | val 0.250108 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.316408 | val 0.249805 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.315427 | val 0.249623 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.320286 | val 0.249392 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.315422 | val 0.249222 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.314884 | val 0.249036 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.311372 | val 0.248925 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.316141 | val 0.248788 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.317176 | val 0.248690 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.315888 | val 0.248577 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.316487 | val 0.248482 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.312247 | val 0.248413 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.316605 | val 0.248274 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.316588 | val 0.248162 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.310450 | val 0.248086 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.317002 | val 0.247910 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.316770 | val 0.247791 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.314343 | val 0.247662 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.317870 | val 0.247649 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.313531 | val 0.247647 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.316332 | val 0.247637 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.313857 | val 0.247668 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.317552 | val 0.247736 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.315183 | val 0.247816 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.18840 | val 0.00326 | noisy 0.00347 | Δ +6.0% | LR 0.00300 | ||ψ-φ|| 0.213\n",
      "[Stage3] L=1 ep 001 | train 0.18050 | val 0.00323 | noisy 0.00347 | Δ +6.9% | LR 0.00300 | ||ψ-φ|| 0.250\n",
      "[Stage3] L=1 ep 002 | train 0.18700 | val 0.00318 | noisy 0.00347 | Δ +8.6% | LR 0.00300 | ||ψ-φ|| 0.327\n",
      "[Stage3] L=1 ep 003 | train 0.18503 | val 0.00311 | noisy 0.00347 | Δ +10.6% | LR 0.00300 | ||ψ-φ|| 0.436\n",
      "[Stage3] L=1 ep 004 | train 0.18560 | val 0.00303 | noisy 0.00347 | Δ +12.7% | LR 0.00300 | ||ψ-φ|| 0.553\n",
      "[Stage3] L=1 ep 005 | train 0.18782 | val 0.00298 | noisy 0.00347 | Δ +14.1% | LR 0.00300 | ||ψ-φ|| 0.648\n",
      "[Stage3] L=1 ep 006 | train 0.18425 | val 0.00292 | noisy 0.00347 | Δ +15.9% | LR 0.00300 | ||ψ-φ|| 0.740\n",
      "[Stage3] L=1 ep 007 | train 0.17647 | val 0.00290 | noisy 0.00347 | Δ +16.5% | LR 0.00300 | ||ψ-φ|| 0.799\n",
      "[Stage3] L=1 ep 008 | train 0.18029 | val 0.00286 | noisy 0.00347 | Δ +17.6% | LR 0.00300 | ||ψ-φ|| 0.853\n",
      "[Stage3] L=1 ep 009 | train 0.18247 | val 0.00284 | noisy 0.00347 | Δ +18.4% | LR 0.00300 | ||ψ-φ|| 0.904\n",
      "[Stage3] L=1 ep 010 | train 0.18590 | val 0.00283 | noisy 0.00347 | Δ +18.6% | LR 0.00300 | ||ψ-φ|| 0.956\n",
      "[Stage3] L=1 ep 011 | train 0.18208 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00300 | ||ψ-φ|| 0.999\n",
      "[Stage3] L=1 ep 012 | train 0.18137 | val 0.00283 | noisy 0.00347 | Δ +18.6% | LR 0.00300 | ||ψ-φ|| 1.046\n",
      "[Stage3] L=1 ep 013 | train 0.18322 | val 0.00283 | noisy 0.00347 | Δ +18.7% | LR 0.00300 | ||ψ-φ|| 1.088\n",
      "[Stage3] L=1 ep 014 | train 0.18217 | val 0.00283 | noisy 0.00347 | Δ +18.7% | LR 0.00300 | ||ψ-φ|| 1.115\n",
      "[Stage3] L=1 ep 015 | train 0.17941 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00300 | ||ψ-φ|| 1.117\n",
      "[Stage3] L=1 ep 016 | train 0.17288 | val 0.00281 | noisy 0.00347 | Δ +19.2% | LR 0.00300 | ||ψ-φ|| 1.125\n",
      "[Stage3] L=1 ep 017 | train 0.18367 | val 0.00281 | noisy 0.00347 | Δ +19.2% | LR 0.00300 | ||ψ-φ|| 1.127\n",
      "[Stage3] L=1 ep 018 | train 0.17704 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00300 | ||ψ-φ|| 1.123\n",
      "[Stage3] L=1 ep 019 | train 0.17818 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00300 | ||ψ-φ|| 1.129\n",
      "[Stage3] L=1 ep 020 | train 0.17965 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00300 | ||ψ-φ|| 1.131\n",
      "[Stage3] L=1 ep 021 | train 0.18656 | val 0.00285 | noisy 0.00347 | Δ +17.9% | LR 0.00300 | ||ψ-φ|| 1.134\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 022 | train 0.18611 | val 0.00286 | noisy 0.00347 | Δ +17.7% | LR 0.00150 | ||ψ-φ|| 1.136\n",
      "[Stage3] L=1 ep 023 | train 0.19078 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00150 | ||ψ-φ|| 1.143\n",
      "[Stage3] L=1 ep 024 | train 0.17512 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00150 | ||ψ-φ|| 1.147\n",
      "[Stage3] L=1 ep 025 | train 0.17531 | val 0.00286 | noisy 0.00347 | Δ +17.7% | LR 0.00150 | ||ψ-φ|| 1.147\n",
      "[Stage3] L=1 ep 026 | train 0.18288 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00150 | ||ψ-φ|| 1.141\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 027 | train 0.18394 | val 0.00287 | noisy 0.00347 | Δ +17.5% | LR 0.00075 | ||ψ-φ|| 1.145\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.495280 | val 0.496285 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.494705 | val 0.495805 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.493918 | val 0.495300 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.493782 | val 0.494769 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.492396 | val 0.494219 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.491880 | val 0.493632 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.491112 | val 0.492999 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.489559 | val 0.492338 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.489527 | val 0.491614 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.488021 | val 0.490850 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.487673 | val 0.490022 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.486692 | val 0.489152 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.486121 | val 0.488209 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.485980 | val 0.487178 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.484066 | val 0.486084 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.484924 | val 0.484885 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.482822 | val 0.483603 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.481151 | val 0.482214 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.480459 | val 0.480745 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.479452 | val 0.479131 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.476712 | val 0.477399 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.475623 | val 0.475561 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.476038 | val 0.473605 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.474797 | val 0.471556 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.470303 | val 0.469458 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.469024 | val 0.467264 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.466214 | val 0.465017 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.465700 | val 0.462706 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.464849 | val 0.460413 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.462930 | val 0.458113 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.456726 | val 0.455922 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.455733 | val 0.453755 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.452762 | val 0.451579 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.450646 | val 0.449517 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.449075 | val 0.447536 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.447618 | val 0.445570 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.443817 | val 0.443704 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.441588 | val 0.441836 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.435962 | val 0.440121 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.435161 | val 0.438405 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.439319 | val 0.436678 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.434436 | val 0.435024 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.429695 | val 0.433381 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.434726 | val 0.431657 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.427560 | val 0.429947 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.426087 | val 0.428385 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.423308 | val 0.426811 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.418401 | val 0.425358 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.423406 | val 0.423886 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.415259 | val 0.422417 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.415530 | val 0.421043 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.419551 | val 0.419777 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.416145 | val 0.418532 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.412677 | val 0.417386 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.411780 | val 0.416259 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.411573 | val 0.415221 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.408991 | val 0.414219 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.409297 | val 0.413180 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.405709 | val 0.412192 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.403431 | val 0.411259 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.401242 | val 0.410342 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.402697 | val 0.409463 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.403721 | val 0.408625 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.399677 | val 0.407823 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.400625 | val 0.407070 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.399894 | val 0.406330 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.396721 | val 0.405634 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.392383 | val 0.404949 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.393831 | val 0.404258 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.390321 | val 0.403565 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.386283 | val 0.402895 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.391458 | val 0.402208 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.387749 | val 0.401518 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.388781 | val 0.400782 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.382031 | val 0.400012 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.383589 | val 0.399168 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.381311 | val 0.398251 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.378140 | val 0.397302 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.378416 | val 0.396215 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.379675 | val 0.395149 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.380476 | val 0.393919 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.372287 | val 0.392547 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.374984 | val 0.391092 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.370978 | val 0.389493 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.371814 | val 0.387799 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.371130 | val 0.386003 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.365070 | val 0.383980 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.368217 | val 0.381826 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.361992 | val 0.379541 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.361631 | val 0.376997 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.360827 | val 0.374495 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.360976 | val 0.371720 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.359864 | val 0.368656 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.356067 | val 0.365521 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.353535 | val 0.362096 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.350274 | val 0.358546 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.351051 | val 0.354721 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.348904 | val 0.350766 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.349927 | val 0.346773 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.346770 | val 0.342651 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.342913 | val 0.338402 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.339511 | val 0.334276 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.342570 | val 0.329977 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.334500 | val 0.325636 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.337139 | val 0.321129 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.338268 | val 0.316686 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.334926 | val 0.312174 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.333227 | val 0.307600 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.335102 | val 0.303263 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.331605 | val 0.299009 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.330900 | val 0.294965 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.323497 | val 0.291161 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.325818 | val 0.287304 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.328805 | val 0.283677 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.324826 | val 0.280196 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.323490 | val 0.277035 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.317896 | val 0.274095 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.318497 | val 0.271296 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.324634 | val 0.268786 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.317205 | val 0.266507 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.18584 | val 0.00350 | noisy 0.00347 | Δ -0.9% | LR 0.00300 | ||ψ-φ|| 0.211\n",
      "[Stage3] L=1 ep 001 | train 0.18409 | val 0.00346 | noisy 0.00347 | Δ +0.6% | LR 0.00300 | ||ψ-φ|| 0.259\n",
      "[Stage3] L=1 ep 002 | train 0.17867 | val 0.00337 | noisy 0.00347 | Δ +2.9% | LR 0.00300 | ||ψ-φ|| 0.334\n",
      "[Stage3] L=1 ep 003 | train 0.18934 | val 0.00327 | noisy 0.00347 | Δ +5.9% | LR 0.00300 | ||ψ-φ|| 0.433\n",
      "[Stage3] L=1 ep 004 | train 0.18407 | val 0.00320 | noisy 0.00347 | Δ +8.0% | LR 0.00300 | ||ψ-φ|| 0.532\n",
      "[Stage3] L=1 ep 005 | train 0.17277 | val 0.00314 | noisy 0.00347 | Δ +9.6% | LR 0.00300 | ||ψ-φ|| 0.616\n",
      "[Stage3] L=1 ep 006 | train 0.17731 | val 0.00310 | noisy 0.00347 | Δ +10.8% | LR 0.00300 | ||ψ-φ|| 0.694\n",
      "[Stage3] L=1 ep 007 | train 0.17852 | val 0.00307 | noisy 0.00347 | Δ +11.7% | LR 0.00300 | ||ψ-φ|| 0.767\n",
      "[Stage3] L=1 ep 008 | train 0.18397 | val 0.00307 | noisy 0.00347 | Δ +11.7% | LR 0.00300 | ||ψ-φ|| 0.833\n",
      "[Stage3] L=1 ep 009 | train 0.18064 | val 0.00309 | noisy 0.00347 | Δ +11.2% | LR 0.00300 | ||ψ-φ|| 0.897\n",
      "[Stage3] L=1 ep 010 | train 0.17909 | val 0.00310 | noisy 0.00347 | Δ +10.9% | LR 0.00300 | ||ψ-φ|| 0.965\n",
      "[Stage3] L=1 ep 011 | train 0.17978 | val 0.00313 | noisy 0.00347 | Δ +9.9% | LR 0.00300 | ||ψ-φ|| 1.018\n",
      "[Stage3] L=1 ep 012 | train 0.17656 | val 0.00316 | noisy 0.00347 | Δ +9.0% | LR 0.00300 | ||ψ-φ|| 1.051\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 013 | train 0.17849 | val 0.00319 | noisy 0.00347 | Δ +8.3% | LR 0.00150 | ||ψ-φ|| 1.074\n",
      "[Stage3] L=1 ep 014 | train 0.17826 | val 0.00319 | noisy 0.00347 | Δ +8.2% | LR 0.00150 | ||ψ-φ|| 1.103\n",
      "[Stage3] L=1 ep 015 | train 0.17930 | val 0.00318 | noisy 0.00347 | Δ +8.4% | LR 0.00150 | ||ψ-φ|| 1.127\n",
      "[Stage3] L=1 ep 016 | train 0.18480 | val 0.00317 | noisy 0.00347 | Δ +8.7% | LR 0.00150 | ||ψ-φ|| 1.150\n",
      "[Stage3] L=1 ep 017 | train 0.17822 | val 0.00317 | noisy 0.00347 | Δ +8.8% | LR 0.00150 | ||ψ-φ|| 1.166\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 018 | train 0.18096 | val 0.00315 | noisy 0.00347 | Δ +9.5% | LR 0.00075 | ||ψ-φ|| 1.183\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.489687 | val 0.494156 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.489663 | val 0.493118 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.487385 | val 0.492029 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.487195 | val 0.490820 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.484592 | val 0.489513 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.482515 | val 0.488081 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.482054 | val 0.486549 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.480076 | val 0.484973 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.476437 | val 0.483296 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.476195 | val 0.481488 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.473987 | val 0.479627 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.471019 | val 0.477661 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.469908 | val 0.475593 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.465527 | val 0.473474 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.464733 | val 0.471252 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.463281 | val 0.468961 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.462816 | val 0.466577 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.458268 | val 0.464166 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.452572 | val 0.461774 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.454009 | val 0.459311 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.449741 | val 0.456830 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.449955 | val 0.454409 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.444676 | val 0.451878 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.446081 | val 0.449403 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.442480 | val 0.447018 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.442003 | val 0.444563 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.438116 | val 0.442114 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.439046 | val 0.439703 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.434234 | val 0.437315 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.429637 | val 0.434968 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.428794 | val 0.432656 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.430600 | val 0.430424 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.425474 | val 0.428256 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.423838 | val 0.426204 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.419436 | val 0.424218 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.416959 | val 0.422304 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.417869 | val 0.420493 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.416160 | val 0.418764 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.413742 | val 0.417121 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.413841 | val 0.415620 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.411682 | val 0.414276 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.409666 | val 0.413036 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.409902 | val 0.411843 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.403130 | val 0.410759 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.403754 | val 0.409696 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.403959 | val 0.408705 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.399565 | val 0.407792 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.397271 | val 0.406949 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.397497 | val 0.406168 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.397480 | val 0.405411 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.391240 | val 0.404700 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.395470 | val 0.403983 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.389034 | val 0.403275 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.390372 | val 0.402581 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.391779 | val 0.401869 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.387347 | val 0.401177 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.387605 | val 0.400452 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.383434 | val 0.399682 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.382564 | val 0.398904 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.384839 | val 0.398047 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.385179 | val 0.397133 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.377164 | val 0.396196 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.378358 | val 0.395113 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.373692 | val 0.393942 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.375411 | val 0.392635 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.377090 | val 0.391269 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.375750 | val 0.389704 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.372286 | val 0.387970 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.368298 | val 0.386057 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.368061 | val 0.384025 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.365694 | val 0.381794 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.368409 | val 0.379383 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.365392 | val 0.376760 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.360055 | val 0.373914 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.361725 | val 0.371004 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.356909 | val 0.367880 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.357708 | val 0.364467 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.354612 | val 0.360922 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.354778 | val 0.357086 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.351643 | val 0.353105 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.348000 | val 0.348895 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.347479 | val 0.344649 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.345761 | val 0.340365 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.345008 | val 0.335866 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.341276 | val 0.331354 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.336830 | val 0.326641 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.337132 | val 0.321848 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.336851 | val 0.316996 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.335446 | val 0.312375 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.330728 | val 0.307632 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.329210 | val 0.303076 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.328120 | val 0.298670 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.327280 | val 0.294430 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.330565 | val 0.290343 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.326725 | val 0.286325 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.323977 | val 0.282506 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.323566 | val 0.278986 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.321130 | val 0.275833 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.320122 | val 0.272833 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.316825 | val 0.270054 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.313833 | val 0.267662 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.316942 | val 0.265269 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.317920 | val 0.263203 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.315579 | val 0.261159 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.316690 | val 0.259348 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.321091 | val 0.257683 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.319321 | val 0.256334 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.317986 | val 0.255020 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.316650 | val 0.253930 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.314950 | val 0.253256 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.317316 | val 0.252513 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.314571 | val 0.251921 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.312063 | val 0.251470 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.312978 | val 0.251109 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.313916 | val 0.250706 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.316025 | val 0.250309 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.313220 | val 0.249916 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.316951 | val 0.249605 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.313285 | val 0.249236 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.319273 | val 0.248880 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.18061 | val 0.00341 | noisy 0.00347 | Δ +1.8% | LR 0.00300 | ||ψ-φ|| 0.171\n",
      "[Stage3] L=1 ep 001 | train 0.18176 | val 0.00338 | noisy 0.00347 | Δ +2.8% | LR 0.00300 | ||ψ-φ|| 0.169\n",
      "[Stage3] L=1 ep 002 | train 0.17455 | val 0.00332 | noisy 0.00347 | Δ +4.3% | LR 0.00300 | ||ψ-φ|| 0.233\n",
      "[Stage3] L=1 ep 003 | train 0.18911 | val 0.00327 | noisy 0.00347 | Δ +6.0% | LR 0.00300 | ||ψ-φ|| 0.325\n",
      "[Stage3] L=1 ep 004 | train 0.18321 | val 0.00319 | noisy 0.00347 | Δ +8.1% | LR 0.00300 | ||ψ-φ|| 0.436\n",
      "[Stage3] L=1 ep 005 | train 0.18582 | val 0.00310 | noisy 0.00347 | Δ +10.8% | LR 0.00300 | ||ψ-φ|| 0.554\n",
      "[Stage3] L=1 ep 006 | train 0.18872 | val 0.00303 | noisy 0.00347 | Δ +12.8% | LR 0.00300 | ||ψ-φ|| 0.660\n",
      "[Stage3] L=1 ep 007 | train 0.17852 | val 0.00298 | noisy 0.00347 | Δ +14.2% | LR 0.00300 | ||ψ-φ|| 0.747\n",
      "[Stage3] L=1 ep 008 | train 0.18219 | val 0.00295 | noisy 0.00347 | Δ +15.2% | LR 0.00300 | ||ψ-φ|| 0.820\n",
      "[Stage3] L=1 ep 009 | train 0.18593 | val 0.00292 | noisy 0.00347 | Δ +16.0% | LR 0.00300 | ||ψ-φ|| 0.894\n",
      "[Stage3] L=1 ep 010 | train 0.17917 | val 0.00289 | noisy 0.00347 | Δ +16.9% | LR 0.00300 | ||ψ-φ|| 0.949\n",
      "[Stage3] L=1 ep 011 | train 0.17836 | val 0.00287 | noisy 0.00347 | Δ +17.4% | LR 0.00300 | ||ψ-φ|| 0.982\n",
      "[Stage3] L=1 ep 012 | train 0.17914 | val 0.00287 | noisy 0.00347 | Δ +17.4% | LR 0.00300 | ||ψ-φ|| 0.992\n",
      "[Stage3] L=1 ep 013 | train 0.17813 | val 0.00287 | noisy 0.00347 | Δ +17.3% | LR 0.00300 | ||ψ-φ|| 1.009\n",
      "[Stage3] L=1 ep 014 | train 0.17656 | val 0.00285 | noisy 0.00347 | Δ +17.9% | LR 0.00300 | ||ψ-φ|| 1.032\n",
      "[Stage3] L=1 ep 015 | train 0.17598 | val 0.00284 | noisy 0.00347 | Δ +18.2% | LR 0.00300 | ||ψ-φ|| 1.046\n",
      "[Stage3] L=1 ep 016 | train 0.18222 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00300 | ||ψ-φ|| 1.060\n",
      "[Stage3] L=1 ep 017 | train 0.18064 | val 0.00283 | noisy 0.00347 | Δ +18.6% | LR 0.00300 | ||ψ-φ|| 1.075\n",
      "[Stage3] L=1 ep 018 | train 0.17878 | val 0.00283 | noisy 0.00347 | Δ +18.4% | LR 0.00300 | ||ψ-φ|| 1.084\n",
      "[Stage3] L=1 ep 019 | train 0.17976 | val 0.00284 | noisy 0.00347 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 1.076\n",
      "[Stage3] L=1 ep 020 | train 0.18425 | val 0.00284 | noisy 0.00347 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 1.069\n",
      "[Stage3] L=1 ep 021 | train 0.18789 | val 0.00284 | noisy 0.00347 | Δ +18.2% | LR 0.00300 | ||ψ-φ|| 1.065\n",
      "[Stage3] L=1 ep 022 | train 0.18598 | val 0.00282 | noisy 0.00347 | Δ +18.7% | LR 0.00300 | ||ψ-φ|| 1.086\n",
      "[Stage3] L=1 ep 023 | train 0.18331 | val 0.00283 | noisy 0.00347 | Δ +18.4% | LR 0.00300 | ||ψ-φ|| 1.090\n",
      "[Stage3] L=1 ep 024 | train 0.17804 | val 0.00284 | noisy 0.00347 | Δ +18.3% | LR 0.00300 | ||ψ-φ|| 1.086\n",
      "[Stage3] L=1 ep 025 | train 0.18534 | val 0.00285 | noisy 0.00347 | Δ +17.9% | LR 0.00300 | ||ψ-φ|| 1.088\n",
      "[Stage3] L=1 ep 026 | train 0.18262 | val 0.00285 | noisy 0.00347 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 1.094\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 027 | train 0.18378 | val 0.00283 | noisy 0.00347 | Δ +18.4% | LR 0.00150 | ||ψ-φ|| 1.092\n",
      "[Stage3] L=1 ep 028 | train 0.17646 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00150 | ||ψ-φ|| 1.085\n",
      "[Stage3] L=1 ep 029 | train 0.18051 | val 0.00282 | noisy 0.00347 | Δ +19.0% | LR 0.00150 | ||ψ-φ|| 1.085\n",
      "[Stage3] L=1 ep 030 | train 0.18006 | val 0.00281 | noisy 0.00347 | Δ +19.2% | LR 0.00150 | ||ψ-φ|| 1.084\n",
      "[Stage3] L=1 ep 031 | train 0.17663 | val 0.00280 | noisy 0.00347 | Δ +19.4% | LR 0.00150 | ||ψ-φ|| 1.086\n",
      "[Stage3] L=1 ep 032 | train 0.17930 | val 0.00279 | noisy 0.00347 | Δ +19.6% | LR 0.00150 | ||ψ-φ|| 1.086\n",
      "[Stage3] L=1 ep 033 | train 0.18977 | val 0.00279 | noisy 0.00347 | Δ +19.8% | LR 0.00150 | ||ψ-φ|| 1.093\n",
      "[Stage3] L=1 ep 034 | train 0.18106 | val 0.00278 | noisy 0.00347 | Δ +20.0% | LR 0.00150 | ||ψ-φ|| 1.102\n",
      "[Stage3] L=1 ep 035 | train 0.17631 | val 0.00278 | noisy 0.00347 | Δ +20.1% | LR 0.00150 | ||ψ-φ|| 1.114\n",
      "[Stage3] L=1 ep 036 | train 0.17945 | val 0.00278 | noisy 0.00347 | Δ +20.0% | LR 0.00150 | ||ψ-φ|| 1.112\n",
      "[Stage3] L=1 ep 037 | train 0.18086 | val 0.00278 | noisy 0.00347 | Δ +20.1% | LR 0.00150 | ||ψ-φ|| 1.113\n",
      "[Stage3] L=1 ep 038 | train 0.18021 | val 0.00278 | noisy 0.00347 | Δ +20.1% | LR 0.00150 | ||ψ-φ|| 1.118\n",
      "[Stage3] L=1 ep 039 | train 0.18038 | val 0.00278 | noisy 0.00347 | Δ +20.0% | LR 0.00150 | ||ψ-φ|| 1.124\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 040 | train 0.18907 | val 0.00278 | noisy 0.00347 | Δ +19.9% | LR 0.00075 | ||ψ-φ|| 1.129\n",
      "[Stage3] L=1 ep 041 | train 0.18410 | val 0.00279 | noisy 0.00347 | Δ +19.7% | LR 0.00075 | ||ψ-φ|| 1.134\n",
      "[Stage3] L=1 ep 042 | train 0.17788 | val 0.00279 | noisy 0.00347 | Δ +19.6% | LR 0.00075 | ||ψ-φ|| 1.135\n",
      "[Stage3] L=1 ep 043 | train 0.18197 | val 0.00280 | noisy 0.00347 | Δ +19.4% | LR 0.00075 | ||ψ-φ|| 1.135\n",
      "[Stage3] L=1 ep 044 | train 0.17819 | val 0.00281 | noisy 0.00347 | Δ +19.2% | LR 0.00075 | ||ψ-φ|| 1.134\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=1 ep 045 | train 0.17716 | val 0.00281 | noisy 0.00347 | Δ +19.0% | LR 0.00038 | ||ψ-φ|| 1.136\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 1 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.508255 | val 0.499296 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.498970 | val 0.489332 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.490240 | val 0.479654 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.481279 | val 0.470376 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.472863 | val 0.461845 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.465289 | val 0.454077 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.459321 | val 0.447099 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.452344 | val 0.440860 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.446728 | val 0.435054 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.441661 | val 0.429557 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.436350 | val 0.424099 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.431365 | val 0.418694 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.426319 | val 0.413298 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.421779 | val 0.407844 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.416018 | val 0.402485 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.410499 | val 0.397067 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.406145 | val 0.391582 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.401054 | val 0.386132 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.395859 | val 0.380824 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.390985 | val 0.375684 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.386442 | val 0.370664 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.381441 | val 0.365772 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.376336 | val 0.361094 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.371915 | val 0.356469 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.367259 | val 0.351837 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.361863 | val 0.347235 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.357716 | val 0.342462 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.352559 | val 0.337627 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.346468 | val 0.332677 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.342295 | val 0.327621 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.336762 | val 0.322463 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.330366 | val 0.317320 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.326739 | val 0.312125 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.321952 | val 0.306921 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.317206 | val 0.301727 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.311461 | val 0.296530 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.307113 | val 0.291229 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.303906 | val 0.285903 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.299015 | val 0.280464 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.292302 | val 0.274966 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.287498 | val 0.269346 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.282972 | val 0.263507 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.277340 | val 0.257517 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.271249 | val 0.251137 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.263950 | val 0.244470 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.259155 | val 0.237442 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.252356 | val 0.230095 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.247412 | val 0.222271 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.239526 | val 0.214490 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.233560 | val 0.206847 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.228604 | val 0.199364 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.223372 | val 0.192328 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.215982 | val 0.185986 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.213296 | val 0.180304 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.209833 | val 0.175364 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.205331 | val 0.171130 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.201823 | val 0.167632 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.199102 | val 0.164507 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.198333 | val 0.161776 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.192383 | val 0.159316 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.191262 | val 0.157051 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.188831 | val 0.154935 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.184889 | val 0.152936 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.184872 | val 0.150960 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.180338 | val 0.149011 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.180085 | val 0.147032 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.177806 | val 0.145012 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.175005 | val 0.142897 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.173683 | val 0.140729 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.166162 | val 0.138550 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.163447 | val 0.136258 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.161728 | val 0.133807 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.161493 | val 0.131315 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.156472 | val 0.128755 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.151345 | val 0.126115 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.148115 | val 0.123371 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.148495 | val 0.120550 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.143224 | val 0.117862 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.140802 | val 0.115122 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.134899 | val 0.112538 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.133188 | val 0.109953 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.130544 | val 0.107412 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.128490 | val 0.104956 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.125960 | val 0.102635 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.121906 | val 0.100440 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.118432 | val 0.098323 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.116008 | val 0.096218 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.115077 | val 0.094191 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.110958 | val 0.092383 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.110045 | val 0.090592 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.108097 | val 0.088959 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.105409 | val 0.087438 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.103758 | val 0.085990 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.101794 | val 0.084582 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.100928 | val 0.083281 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.101061 | val 0.082054 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.098057 | val 0.080932 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.096338 | val 0.079910 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.096373 | val 0.078950 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.093713 | val 0.078077 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.094175 | val 0.077261 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.090694 | val 0.076540 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.091201 | val 0.075821 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.089824 | val 0.075125 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.088209 | val 0.074469 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.086739 | val 0.073852 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.088554 | val 0.073239 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.086433 | val 0.072688 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.086476 | val 0.072149 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.084511 | val 0.071640 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.084412 | val 0.071128 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.084220 | val 0.070611 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.082556 | val 0.070110 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.081692 | val 0.069617 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.082667 | val 0.069127 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.082362 | val 0.068639 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.080853 | val 0.068171 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.080421 | val 0.067719 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.080486 | val 0.067249 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.080715 | val 0.066823 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.05581 | val 0.00329 | noisy 0.00347 | Δ +5.4% | LR 0.00300 | ||ψ-φ|| 0.312\n",
      "[Stage3] L=3 ep 001 | train 0.05803 | val 0.00336 | noisy 0.00347 | Δ +3.3% | LR 0.00300 | ||ψ-φ|| 0.291\n",
      "[Stage3] L=3 ep 002 | train 0.06152 | val 0.00333 | noisy 0.00347 | Δ +4.2% | LR 0.00300 | ||ψ-φ|| 0.333\n",
      "[Stage3] L=3 ep 003 | train 0.05647 | val 0.00323 | noisy 0.00347 | Δ +7.0% | LR 0.00300 | ||ψ-φ|| 0.414\n",
      "[Stage3] L=3 ep 004 | train 0.05027 | val 0.00318 | noisy 0.00347 | Δ +8.6% | LR 0.00300 | ||ψ-φ|| 0.506\n",
      "[Stage3] L=3 ep 005 | train 0.05491 | val 0.00309 | noisy 0.00347 | Δ +11.1% | LR 0.00300 | ||ψ-φ|| 0.601\n",
      "[Stage3] L=3 ep 006 | train 0.05416 | val 0.00301 | noisy 0.00347 | Δ +13.4% | LR 0.00300 | ||ψ-φ|| 0.698\n",
      "[Stage3] L=3 ep 007 | train 0.05041 | val 0.00296 | noisy 0.00347 | Δ +14.8% | LR 0.00300 | ||ψ-φ|| 0.780\n",
      "[Stage3] L=3 ep 008 | train 0.05185 | val 0.00291 | noisy 0.00347 | Δ +16.1% | LR 0.00300 | ||ψ-φ|| 0.849\n",
      "[Stage3] L=3 ep 009 | train 0.05130 | val 0.00286 | noisy 0.00347 | Δ +17.5% | LR 0.00300 | ||ψ-φ|| 0.918\n",
      "[Stage3] L=3 ep 010 | train 0.05290 | val 0.00283 | noisy 0.00347 | Δ +18.5% | LR 0.00300 | ||ψ-φ|| 0.987\n",
      "[Stage3] L=3 ep 011 | train 0.04426 | val 0.00287 | noisy 0.00347 | Δ +17.3% | LR 0.00300 | ||ψ-φ|| 1.035\n",
      "[Stage3] L=3 ep 012 | train 0.04765 | val 0.00290 | noisy 0.00347 | Δ +16.7% | LR 0.00300 | ||ψ-φ|| 1.083\n",
      "[Stage3] L=3 ep 013 | train 0.04988 | val 0.00293 | noisy 0.00347 | Δ +15.7% | LR 0.00300 | ||ψ-φ|| 1.124\n",
      "[Stage3] L=3 ep 014 | train 0.04703 | val 0.00294 | noisy 0.00347 | Δ +15.5% | LR 0.00300 | ||ψ-φ|| 1.167\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 015 | train 0.04942 | val 0.00296 | noisy 0.00347 | Δ +14.9% | LR 0.00150 | ||ψ-φ|| 1.208\n",
      "[Stage3] L=3 ep 016 | train 0.04458 | val 0.00298 | noisy 0.00347 | Δ +14.1% | LR 0.00150 | ||ψ-φ|| 1.247\n",
      "[Stage3] L=3 ep 017 | train 0.04709 | val 0.00299 | noisy 0.00347 | Δ +14.0% | LR 0.00150 | ||ψ-φ|| 1.278\n",
      "[Stage3] L=3 ep 018 | train 0.04974 | val 0.00298 | noisy 0.00347 | Δ +14.1% | LR 0.00150 | ||ψ-φ|| 1.299\n",
      "[Stage3] L=3 ep 019 | train 0.04600 | val 0.00298 | noisy 0.00347 | Δ +14.1% | LR 0.00150 | ||ψ-φ|| 1.312\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 020 | train 0.04642 | val 0.00298 | noisy 0.00347 | Δ +14.2% | LR 0.00075 | ||ψ-φ|| 1.326\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.495396 | val 0.482398 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.482678 | val 0.470160 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.471442 | val 0.458008 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.460056 | val 0.446871 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.450462 | val 0.437127 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.442241 | val 0.428850 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.436301 | val 0.421818 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.428638 | val 0.415734 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.423828 | val 0.410166 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.418170 | val 0.404861 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.413883 | val 0.399638 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.409932 | val 0.394581 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.403520 | val 0.389723 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.398656 | val 0.385171 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.395720 | val 0.380982 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.392593 | val 0.377165 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.387130 | val 0.373742 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.383952 | val 0.370565 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.381416 | val 0.367648 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.379668 | val 0.364807 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.374729 | val 0.361948 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.371499 | val 0.359148 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.368733 | val 0.356269 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.365907 | val 0.353493 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.362998 | val 0.350815 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.360034 | val 0.348252 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.356468 | val 0.345719 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.354180 | val 0.343260 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.352212 | val 0.340711 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.350713 | val 0.338155 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.348501 | val 0.335557 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.345676 | val 0.332888 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.342872 | val 0.330052 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.337667 | val 0.327105 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.336931 | val 0.324002 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.333904 | val 0.320863 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.330623 | val 0.317756 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.329134 | val 0.314525 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.323569 | val 0.311262 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.320872 | val 0.307893 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.317982 | val 0.304548 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.314952 | val 0.301116 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.312337 | val 0.297677 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.307385 | val 0.294172 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.303940 | val 0.290558 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.300927 | val 0.286821 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.294514 | val 0.283088 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.293530 | val 0.279104 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.288153 | val 0.275069 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.284436 | val 0.270890 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.281662 | val 0.266575 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.275805 | val 0.262164 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.272832 | val 0.257619 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.267572 | val 0.252785 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.262588 | val 0.247766 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.257637 | val 0.242455 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.252997 | val 0.236777 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.249101 | val 0.230963 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.243748 | val 0.224771 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.240591 | val 0.218162 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.231482 | val 0.211321 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.226655 | val 0.204104 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.221339 | val 0.196601 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.215101 | val 0.188935 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.207174 | val 0.181493 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.200871 | val 0.174044 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.195545 | val 0.166841 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.187485 | val 0.160077 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.181900 | val 0.153657 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.176353 | val 0.147940 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.170954 | val 0.142790 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.164325 | val 0.138291 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.162634 | val 0.134164 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.158676 | val 0.130479 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.152324 | val 0.127206 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.149865 | val 0.124073 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.148217 | val 0.121064 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.143964 | val 0.118354 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.142408 | val 0.115947 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.137346 | val 0.113852 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.134383 | val 0.112006 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.131026 | val 0.110309 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.130570 | val 0.108705 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.129066 | val 0.107283 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.127080 | val 0.105953 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.125361 | val 0.104833 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.123401 | val 0.103854 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.122757 | val 0.102911 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.121276 | val 0.102091 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.120550 | val 0.101320 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.120540 | val 0.100597 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.121614 | val 0.099997 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.118692 | val 0.099653 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.117190 | val 0.099298 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.117830 | val 0.098900 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.116936 | val 0.098547 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.116017 | val 0.098110 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.117676 | val 0.097730 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.114351 | val 0.097403 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.115062 | val 0.097111 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.115982 | val 0.096828 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.115899 | val 0.096558 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.114876 | val 0.096434 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.112879 | val 0.096428 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.114328 | val 0.096272 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.113909 | val 0.096101 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.115099 | val 0.095824 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.115940 | val 0.095655 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.113186 | val 0.095661 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.113418 | val 0.095564 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.114430 | val 0.095450 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.114144 | val 0.095301 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.111922 | val 0.095195 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.113603 | val 0.094962 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.113215 | val 0.094760 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.111876 | val 0.094667 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.111878 | val 0.094595 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.112596 | val 0.094479 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.113459 | val 0.094307 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.113561 | val 0.094243 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.07592 | val 0.00341 | noisy 0.00347 | Δ +1.9% | LR 0.00300 | ||ψ-φ|| 0.290\n",
      "[Stage3] L=3 ep 001 | train 0.07485 | val 0.00338 | noisy 0.00347 | Δ +2.8% | LR 0.00300 | ||ψ-φ|| 0.292\n",
      "[Stage3] L=3 ep 002 | train 0.07843 | val 0.00331 | noisy 0.00347 | Δ +4.8% | LR 0.00300 | ||ψ-φ|| 0.372\n",
      "[Stage3] L=3 ep 003 | train 0.08173 | val 0.00322 | noisy 0.00347 | Δ +7.4% | LR 0.00300 | ||ψ-φ|| 0.480\n",
      "[Stage3] L=3 ep 004 | train 0.07390 | val 0.00319 | noisy 0.00347 | Δ +8.1% | LR 0.00300 | ||ψ-φ|| 0.562\n",
      "[Stage3] L=3 ep 005 | train 0.07584 | val 0.00311 | noisy 0.00347 | Δ +10.5% | LR 0.00300 | ||ψ-φ|| 0.642\n",
      "[Stage3] L=3 ep 006 | train 0.07297 | val 0.00302 | noisy 0.00347 | Δ +13.1% | LR 0.00300 | ||ψ-φ|| 0.695\n",
      "[Stage3] L=3 ep 007 | train 0.07051 | val 0.00295 | noisy 0.00347 | Δ +15.1% | LR 0.00300 | ||ψ-φ|| 0.754\n",
      "[Stage3] L=3 ep 008 | train 0.07551 | val 0.00293 | noisy 0.00347 | Δ +15.5% | LR 0.00300 | ||ψ-φ|| 0.804\n",
      "[Stage3] L=3 ep 009 | train 0.07383 | val 0.00292 | noisy 0.00347 | Δ +16.0% | LR 0.00300 | ||ψ-φ|| 0.838\n",
      "[Stage3] L=3 ep 010 | train 0.07000 | val 0.00292 | noisy 0.00347 | Δ +16.0% | LR 0.00300 | ||ψ-φ|| 0.857\n",
      "[Stage3] L=3 ep 011 | train 0.07021 | val 0.00289 | noisy 0.00347 | Δ +16.8% | LR 0.00300 | ||ψ-φ|| 0.889\n",
      "[Stage3] L=3 ep 012 | train 0.06497 | val 0.00287 | noisy 0.00347 | Δ +17.3% | LR 0.00300 | ||ψ-φ|| 0.924\n",
      "[Stage3] L=3 ep 013 | train 0.07419 | val 0.00285 | noisy 0.00347 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 0.955\n",
      "[Stage3] L=3 ep 014 | train 0.07197 | val 0.00286 | noisy 0.00347 | Δ +17.8% | LR 0.00300 | ||ψ-φ|| 0.957\n",
      "[Stage3] L=3 ep 015 | train 0.07048 | val 0.00285 | noisy 0.00347 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 0.967\n",
      "[Stage3] L=3 ep 016 | train 0.07238 | val 0.00280 | noisy 0.00347 | Δ +19.4% | LR 0.00300 | ||ψ-φ|| 0.989\n",
      "[Stage3] L=3 ep 017 | train 0.07362 | val 0.00278 | noisy 0.00347 | Δ +20.1% | LR 0.00300 | ||ψ-φ|| 1.002\n",
      "[Stage3] L=3 ep 018 | train 0.06818 | val 0.00278 | noisy 0.00347 | Δ +20.1% | LR 0.00300 | ||ψ-φ|| 1.011\n",
      "[Stage3] L=3 ep 019 | train 0.06913 | val 0.00279 | noisy 0.00347 | Δ +19.7% | LR 0.00300 | ||ψ-φ|| 1.023\n",
      "[Stage3] L=3 ep 020 | train 0.07834 | val 0.00279 | noisy 0.00347 | Δ +19.6% | LR 0.00300 | ||ψ-φ|| 1.031\n",
      "[Stage3] L=3 ep 021 | train 0.06581 | val 0.00282 | noisy 0.00347 | Δ +18.9% | LR 0.00300 | ||ψ-φ|| 1.037\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 022 | train 0.06570 | val 0.00282 | noisy 0.00347 | Δ +18.7% | LR 0.00150 | ||ψ-φ|| 1.046\n",
      "[Stage3] L=3 ep 023 | train 0.06944 | val 0.00282 | noisy 0.00347 | Δ +18.9% | LR 0.00150 | ||ψ-φ|| 1.059\n",
      "[Stage3] L=3 ep 024 | train 0.06867 | val 0.00280 | noisy 0.00347 | Δ +19.4% | LR 0.00150 | ||ψ-φ|| 1.072\n",
      "[Stage3] L=3 ep 025 | train 0.06817 | val 0.00280 | noisy 0.00347 | Δ +19.4% | LR 0.00150 | ||ψ-φ|| 1.083\n",
      "[Stage3] L=3 ep 026 | train 0.07379 | val 0.00280 | noisy 0.00347 | Δ +19.4% | LR 0.00150 | ||ψ-φ|| 1.095\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 027 | train 0.06781 | val 0.00279 | noisy 0.00347 | Δ +19.8% | LR 0.00075 | ||ψ-φ|| 1.109\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.478290 | val 0.452750 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.464933 | val 0.437541 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.453261 | val 0.423445 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.436687 | val 0.410418 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.427369 | val 0.398062 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.414116 | val 0.386234 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.407223 | val 0.374445 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.395431 | val 0.362443 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.381195 | val 0.350141 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.371413 | val 0.337633 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.359045 | val 0.325418 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.348086 | val 0.313767 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.336945 | val 0.303228 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.330867 | val 0.293917 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.322366 | val 0.285898 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.315155 | val 0.278815 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.307502 | val 0.272316 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.303678 | val 0.265953 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.300969 | val 0.259546 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.294219 | val 0.253182 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.287377 | val 0.246530 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.282519 | val 0.239600 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.277750 | val 0.232365 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.269883 | val 0.224848 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.259510 | val 0.217266 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.257253 | val 0.209546 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.246444 | val 0.201957 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.241614 | val 0.194473 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.234974 | val 0.187248 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.229319 | val 0.180389 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.221443 | val 0.173950 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.214790 | val 0.167988 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.211387 | val 0.162542 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.203141 | val 0.157671 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.201879 | val 0.153254 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.195812 | val 0.149370 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.197915 | val 0.145792 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.189103 | val 0.142700 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.188051 | val 0.139879 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.182761 | val 0.137348 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.180902 | val 0.135023 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.174135 | val 0.132986 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.176664 | val 0.131075 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.173565 | val 0.129364 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.170683 | val 0.127890 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.167046 | val 0.126622 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.165353 | val 0.125373 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.163801 | val 0.124238 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.163974 | val 0.123150 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.165559 | val 0.122088 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.158704 | val 0.121239 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.158423 | val 0.120368 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.156987 | val 0.119487 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.156407 | val 0.118686 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.154185 | val 0.117922 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.156290 | val 0.117114 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.155842 | val 0.116354 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.151444 | val 0.115718 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.154121 | val 0.115071 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.152316 | val 0.114366 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.149495 | val 0.113663 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.151660 | val 0.112937 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.148163 | val 0.112237 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.145048 | val 0.111542 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.144669 | val 0.110781 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.144935 | val 0.109968 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.142866 | val 0.109140 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.143295 | val 0.108356 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.141724 | val 0.107530 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.141010 | val 0.106707 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.140117 | val 0.105853 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.135885 | val 0.105107 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.138242 | val 0.104226 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.136426 | val 0.103383 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.137520 | val 0.102450 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.134288 | val 0.101550 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.134928 | val 0.100679 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.132345 | val 0.099778 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.129597 | val 0.098952 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.125682 | val 0.098113 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.126688 | val 0.097096 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.123953 | val 0.096084 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.123686 | val 0.095088 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.121936 | val 0.094124 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.121940 | val 0.093189 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.118537 | val 0.092361 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.117179 | val 0.091562 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.115430 | val 0.090832 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.114694 | val 0.090142 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.112938 | val 0.089467 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.113114 | val 0.088770 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.110082 | val 0.088295 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.109440 | val 0.087770 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.108257 | val 0.087252 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.108319 | val 0.086725 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.106908 | val 0.086286 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.105460 | val 0.085994 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.105112 | val 0.085611 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.104178 | val 0.085261 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.104464 | val 0.084914 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.103016 | val 0.084658 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.101702 | val 0.084403 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.099960 | val 0.084236 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.100489 | val 0.084029 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.100494 | val 0.083809 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.099775 | val 0.083650 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.100147 | val 0.083499 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.098557 | val 0.083411 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.098979 | val 0.083342 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.099187 | val 0.083288 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.097536 | val 0.083284 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.098409 | val 0.083288 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.097797 | val 0.083281 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.097587 | val 0.083286 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.098034 | val 0.083246 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.097202 | val 0.083256 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.097548 | val 0.083228 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.096935 | val 0.083237 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.097415 | val 0.083241 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.097421 | val 0.083286 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.06705 | val 0.00293 | noisy 0.00347 | Δ +15.5% | LR 0.00300 | ||ψ-φ|| 0.357\n",
      "[Stage3] L=3 ep 001 | train 0.06791 | val 0.00294 | noisy 0.00347 | Δ +15.4% | LR 0.00300 | ||ψ-φ|| 0.317\n",
      "[Stage3] L=3 ep 002 | train 0.06588 | val 0.00290 | noisy 0.00347 | Δ +16.5% | LR 0.00300 | ||ψ-φ|| 0.349\n",
      "[Stage3] L=3 ep 003 | train 0.06858 | val 0.00284 | noisy 0.00347 | Δ +18.2% | LR 0.00300 | ||ψ-φ|| 0.431\n",
      "[Stage3] L=3 ep 004 | train 0.06584 | val 0.00280 | noisy 0.00347 | Δ +19.3% | LR 0.00300 | ||ψ-φ|| 0.521\n",
      "[Stage3] L=3 ep 005 | train 0.06789 | val 0.00279 | noisy 0.00347 | Δ +19.7% | LR 0.00300 | ||ψ-φ|| 0.589\n",
      "[Stage3] L=3 ep 006 | train 0.07300 | val 0.00275 | noisy 0.00347 | Δ +20.9% | LR 0.00300 | ||ψ-φ|| 0.670\n",
      "[Stage3] L=3 ep 007 | train 0.06040 | val 0.00276 | noisy 0.00347 | Δ +20.5% | LR 0.00300 | ||ψ-φ|| 0.704\n",
      "[Stage3] L=3 ep 008 | train 0.06512 | val 0.00274 | noisy 0.00347 | Δ +21.3% | LR 0.00300 | ||ψ-φ|| 0.722\n",
      "[Stage3] L=3 ep 009 | train 0.06471 | val 0.00270 | noisy 0.00347 | Δ +22.4% | LR 0.00300 | ||ψ-φ|| 0.744\n",
      "[Stage3] L=3 ep 010 | train 0.06460 | val 0.00266 | noisy 0.00347 | Δ +23.5% | LR 0.00300 | ||ψ-φ|| 0.779\n",
      "[Stage3] L=3 ep 011 | train 0.06535 | val 0.00267 | noisy 0.00347 | Δ +23.2% | LR 0.00300 | ||ψ-φ|| 0.814\n",
      "[Stage3] L=3 ep 012 | train 0.06428 | val 0.00267 | noisy 0.00347 | Δ +23.2% | LR 0.00300 | ||ψ-φ|| 0.860\n",
      "[Stage3] L=3 ep 013 | train 0.06763 | val 0.00266 | noisy 0.00347 | Δ +23.3% | LR 0.00300 | ||ψ-φ|| 0.907\n",
      "[Stage3] L=3 ep 014 | train 0.06617 | val 0.00266 | noisy 0.00347 | Δ +23.5% | LR 0.00300 | ||ψ-φ|| 0.949\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 015 | train 0.07327 | val 0.00267 | noisy 0.00347 | Δ +23.1% | LR 0.00150 | ||ψ-φ|| 0.982\n",
      "[Stage3] L=3 ep 016 | train 0.06644 | val 0.00267 | noisy 0.00347 | Δ +23.0% | LR 0.00150 | ||ψ-φ|| 1.018\n",
      "[Stage3] L=3 ep 017 | train 0.06441 | val 0.00268 | noisy 0.00347 | Δ +23.0% | LR 0.00150 | ||ψ-φ|| 1.049\n",
      "[Stage3] L=3 ep 018 | train 0.07109 | val 0.00269 | noisy 0.00347 | Δ +22.7% | LR 0.00150 | ||ψ-φ|| 1.077\n",
      "[Stage3] L=3 ep 019 | train 0.06286 | val 0.00269 | noisy 0.00347 | Δ +22.5% | LR 0.00150 | ||ψ-φ|| 1.097\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 020 | train 0.07648 | val 0.00269 | noisy 0.00347 | Δ +22.6% | LR 0.00075 | ||ψ-φ|| 1.119\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.511380 | val 0.509395 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.498140 | val 0.494522 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.485583 | val 0.480123 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.473623 | val 0.465990 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.462072 | val 0.452112 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.450820 | val 0.438230 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.440325 | val 0.424423 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.427612 | val 0.410996 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.416185 | val 0.398194 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.406105 | val 0.386386 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.396051 | val 0.375777 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.386097 | val 0.366193 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.377035 | val 0.357511 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.369458 | val 0.349490 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.363303 | val 0.341807 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.354672 | val 0.334353 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.348738 | val 0.326981 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.342308 | val 0.319709 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.335653 | val 0.312580 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.328888 | val 0.305671 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.324768 | val 0.299060 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.317893 | val 0.292745 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.311591 | val 0.286802 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.305561 | val 0.281160 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.300872 | val 0.275745 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.296280 | val 0.270460 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.294982 | val 0.265169 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.287487 | val 0.259926 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.283461 | val 0.254551 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.278330 | val 0.249107 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.276072 | val 0.243399 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.269022 | val 0.237590 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.267201 | val 0.231712 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.261957 | val 0.225855 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.256608 | val 0.220018 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.250554 | val 0.214343 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.246992 | val 0.208703 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.242245 | val 0.203271 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.239457 | val 0.197993 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.232429 | val 0.193008 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.224537 | val 0.188342 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.223026 | val 0.183808 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.214978 | val 0.179532 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.209785 | val 0.175522 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.207705 | val 0.171617 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.201497 | val 0.167825 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.200817 | val 0.164184 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.199042 | val 0.160741 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.193817 | val 0.157528 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.192004 | val 0.154378 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.187152 | val 0.151368 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.179953 | val 0.148536 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.178521 | val 0.145749 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.173946 | val 0.142986 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.173833 | val 0.140374 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.167107 | val 0.137827 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.165096 | val 0.135327 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.163789 | val 0.132955 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.159675 | val 0.130586 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.156328 | val 0.128305 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.155173 | val 0.126080 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.151899 | val 0.123928 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.147938 | val 0.121840 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.145531 | val 0.119772 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.144292 | val 0.117808 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.140483 | val 0.115854 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.136449 | val 0.113968 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.137048 | val 0.112151 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.134374 | val 0.110434 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.131109 | val 0.108841 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.130475 | val 0.107318 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.128695 | val 0.105884 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.125407 | val 0.104537 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.124498 | val 0.103253 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.124147 | val 0.101998 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.122013 | val 0.100855 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.119228 | val 0.099823 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.117296 | val 0.098867 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.117873 | val 0.097997 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.114664 | val 0.097150 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.116961 | val 0.096388 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.114394 | val 0.095678 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.114573 | val 0.095045 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.113372 | val 0.094492 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.112479 | val 0.094005 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.109250 | val 0.093558 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.110739 | val 0.093110 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.111623 | val 0.092696 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.109107 | val 0.092378 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.111144 | val 0.092087 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.108446 | val 0.091836 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.108100 | val 0.091624 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.107011 | val 0.091432 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.107950 | val 0.091276 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.106237 | val 0.091154 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.105328 | val 0.090959 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.105884 | val 0.090789 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.106943 | val 0.090627 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.104653 | val 0.090485 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.105237 | val 0.090345 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.104599 | val 0.090222 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.104901 | val 0.090093 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.104194 | val 0.089988 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.102208 | val 0.089922 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.103804 | val 0.089752 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.103372 | val 0.089593 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.104555 | val 0.089471 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.102712 | val 0.089392 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.102709 | val 0.089298 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.103953 | val 0.089179 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.102760 | val 0.089081 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.100415 | val 0.089048 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.101952 | val 0.088973 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.103473 | val 0.088864 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.102770 | val 0.088806 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.101271 | val 0.088742 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.099703 | val 0.088709 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.100356 | val 0.088592 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.102273 | val 0.088477 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.101016 | val 0.088401 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.06950 | val 0.00328 | noisy 0.00347 | Δ +5.5% | LR 0.00300 | ||ψ-φ|| 0.351\n",
      "[Stage3] L=3 ep 001 | train 0.06923 | val 0.00324 | noisy 0.00347 | Δ +6.7% | LR 0.00300 | ||ψ-φ|| 0.361\n",
      "[Stage3] L=3 ep 002 | train 0.06762 | val 0.00317 | noisy 0.00347 | Δ +8.8% | LR 0.00300 | ||ψ-φ|| 0.394\n",
      "[Stage3] L=3 ep 003 | train 0.06714 | val 0.00310 | noisy 0.00347 | Δ +10.7% | LR 0.00300 | ||ψ-φ|| 0.434\n",
      "[Stage3] L=3 ep 004 | train 0.06534 | val 0.00310 | noisy 0.00347 | Δ +10.9% | LR 0.00300 | ||ψ-φ|| 0.473\n",
      "[Stage3] L=3 ep 005 | train 0.07040 | val 0.00311 | noisy 0.00347 | Δ +10.5% | LR 0.00300 | ||ψ-φ|| 0.521\n",
      "[Stage3] L=3 ep 006 | train 0.07021 | val 0.00307 | noisy 0.00347 | Δ +11.6% | LR 0.00300 | ||ψ-φ|| 0.565\n",
      "[Stage3] L=3 ep 007 | train 0.06925 | val 0.00307 | noisy 0.00347 | Δ +11.5% | LR 0.00300 | ||ψ-φ|| 0.605\n",
      "[Stage3] L=3 ep 008 | train 0.06927 | val 0.00308 | noisy 0.00347 | Δ +11.3% | LR 0.00300 | ||ψ-φ|| 0.642\n",
      "[Stage3] L=3 ep 009 | train 0.06737 | val 0.00308 | noisy 0.00347 | Δ +11.4% | LR 0.00300 | ||ψ-φ|| 0.674\n",
      "[Stage3] L=3 ep 010 | train 0.06363 | val 0.00305 | noisy 0.00347 | Δ +12.2% | LR 0.00300 | ||ψ-φ|| 0.707\n",
      "[Stage3] L=3 ep 011 | train 0.06339 | val 0.00303 | noisy 0.00347 | Δ +12.7% | LR 0.00300 | ||ψ-φ|| 0.749\n",
      "[Stage3] L=3 ep 012 | train 0.06539 | val 0.00308 | noisy 0.00347 | Δ +11.4% | LR 0.00300 | ||ψ-φ|| 0.778\n",
      "[Stage3] L=3 ep 013 | train 0.06727 | val 0.00318 | noisy 0.00347 | Δ +8.4% | LR 0.00300 | ||ψ-φ|| 0.812\n",
      "[Stage3] L=3 ep 014 | train 0.06055 | val 0.00321 | noisy 0.00347 | Δ +7.7% | LR 0.00300 | ||ψ-φ|| 0.853\n",
      "[Stage3] L=3 ep 015 | train 0.07389 | val 0.00319 | noisy 0.00347 | Δ +8.1% | LR 0.00300 | ||ψ-φ|| 0.885\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 016 | train 0.07111 | val 0.00319 | noisy 0.00347 | Δ +8.1% | LR 0.00150 | ||ψ-φ|| 0.929\n",
      "[Stage3] L=3 ep 017 | train 0.06289 | val 0.00322 | noisy 0.00347 | Δ +7.5% | LR 0.00150 | ||ψ-φ|| 0.969\n",
      "[Stage3] L=3 ep 018 | train 0.06140 | val 0.00321 | noisy 0.00347 | Δ +7.6% | LR 0.00150 | ||ψ-φ|| 1.002\n",
      "[Stage3] L=3 ep 019 | train 0.06528 | val 0.00320 | noisy 0.00347 | Δ +7.9% | LR 0.00150 | ||ψ-φ|| 1.032\n",
      "[Stage3] L=3 ep 020 | train 0.06585 | val 0.00321 | noisy 0.00347 | Δ +7.6% | LR 0.00150 | ||ψ-φ|| 1.059\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 021 | train 0.06124 | val 0.00320 | noisy 0.00347 | Δ +7.8% | LR 0.00075 | ||ψ-φ|| 1.089\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.481522 | val 0.474125 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.473210 | val 0.463983 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.462505 | val 0.453410 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.453591 | val 0.442746 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.444545 | val 0.432148 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.435615 | val 0.421795 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.426071 | val 0.411809 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.418399 | val 0.402123 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.409884 | val 0.392776 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.402493 | val 0.383865 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.394177 | val 0.375387 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.385877 | val 0.367383 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.378802 | val 0.359800 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.372441 | val 0.352705 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.365790 | val 0.346180 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.359333 | val 0.340195 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.352791 | val 0.334760 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.348843 | val 0.329751 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.346610 | val 0.325116 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.339727 | val 0.320875 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.336978 | val 0.316873 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.332869 | val 0.313137 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.330464 | val 0.309667 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.325010 | val 0.306464 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.322509 | val 0.303435 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.320754 | val 0.300569 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.317237 | val 0.297781 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.313384 | val 0.295103 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.312726 | val 0.292391 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.311655 | val 0.289651 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.309581 | val 0.286883 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.305351 | val 0.284037 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.304427 | val 0.281084 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.299684 | val 0.278036 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.298438 | val 0.274879 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.295644 | val 0.271612 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.290360 | val 0.268280 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.287820 | val 0.264792 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.286886 | val 0.261159 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.283292 | val 0.257450 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.277902 | val 0.253671 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.273800 | val 0.249797 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.271497 | val 0.245802 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.267334 | val 0.241773 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.263050 | val 0.237699 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.258867 | val 0.233649 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.256387 | val 0.229680 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.254337 | val 0.225845 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.248860 | val 0.222168 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.246067 | val 0.218617 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.242077 | val 0.215166 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.238277 | val 0.211838 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.236118 | val 0.208587 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.232629 | val 0.205375 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.230430 | val 0.202213 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.224131 | val 0.198982 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.221445 | val 0.195733 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.219545 | val 0.192348 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.214851 | val 0.188866 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.212289 | val 0.185276 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.209081 | val 0.181640 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.202144 | val 0.177845 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.200427 | val 0.173953 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.194364 | val 0.170022 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.192143 | val 0.166004 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.186277 | val 0.162046 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.183711 | val 0.158058 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.179562 | val 0.153993 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.176923 | val 0.149939 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.172700 | val 0.146050 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.168485 | val 0.142311 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.167334 | val 0.138737 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.162819 | val 0.135363 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.159527 | val 0.132216 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.156583 | val 0.129358 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.154176 | val 0.126746 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.152030 | val 0.124379 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.149781 | val 0.122236 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.149495 | val 0.120311 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.146661 | val 0.118617 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.145731 | val 0.117085 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.144292 | val 0.115739 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.143341 | val 0.114529 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.143788 | val 0.113440 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.141506 | val 0.112468 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.139840 | val 0.111569 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.139879 | val 0.110751 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.140289 | val 0.109979 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.139189 | val 0.109294 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.136922 | val 0.108644 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.135558 | val 0.108076 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.136318 | val 0.107514 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.134040 | val 0.107033 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.137044 | val 0.106540 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.134622 | val 0.106087 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.133439 | val 0.105655 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.132622 | val 0.105208 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.133185 | val 0.104782 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.132411 | val 0.104387 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.129958 | val 0.104001 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.129566 | val 0.103623 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.131077 | val 0.103225 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.130872 | val 0.102874 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.129975 | val 0.102518 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.130032 | val 0.102180 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.132319 | val 0.101846 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.130393 | val 0.101557 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.129964 | val 0.101288 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.128738 | val 0.101042 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.129840 | val 0.100830 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.129220 | val 0.100623 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.127760 | val 0.100467 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.126643 | val 0.100315 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.126203 | val 0.100168 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.126062 | val 0.100042 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.127340 | val 0.099886 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.126590 | val 0.099765 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.127597 | val 0.099632 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.125184 | val 0.099544 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.127919 | val 0.099433 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.07929 | val 0.00323 | noisy 0.00347 | Δ +6.9% | LR 0.00300 | ||ψ-φ|| 0.390\n",
      "[Stage3] L=3 ep 001 | train 0.07201 | val 0.00318 | noisy 0.00347 | Δ +8.4% | LR 0.00300 | ||ψ-φ|| 0.406\n",
      "[Stage3] L=3 ep 002 | train 0.07180 | val 0.00314 | noisy 0.00347 | Δ +9.5% | LR 0.00300 | ||ψ-φ|| 0.441\n",
      "[Stage3] L=3 ep 003 | train 0.07682 | val 0.00313 | noisy 0.00347 | Δ +10.0% | LR 0.00300 | ||ψ-φ|| 0.495\n",
      "[Stage3] L=3 ep 004 | train 0.07928 | val 0.00310 | noisy 0.00347 | Δ +10.7% | LR 0.00300 | ||ψ-φ|| 0.561\n",
      "[Stage3] L=3 ep 005 | train 0.06989 | val 0.00310 | noisy 0.00347 | Δ +10.9% | LR 0.00300 | ||ψ-φ|| 0.621\n",
      "[Stage3] L=3 ep 006 | train 0.07449 | val 0.00305 | noisy 0.00347 | Δ +12.2% | LR 0.00300 | ||ψ-φ|| 0.686\n",
      "[Stage3] L=3 ep 007 | train 0.06977 | val 0.00297 | noisy 0.00347 | Δ +14.4% | LR 0.00300 | ||ψ-φ|| 0.751\n",
      "[Stage3] L=3 ep 008 | train 0.07591 | val 0.00295 | noisy 0.00347 | Δ +15.2% | LR 0.00300 | ||ψ-φ|| 0.825\n",
      "[Stage3] L=3 ep 009 | train 0.07233 | val 0.00289 | noisy 0.00347 | Δ +16.7% | LR 0.00300 | ||ψ-φ|| 0.892\n",
      "[Stage3] L=3 ep 010 | train 0.07655 | val 0.00287 | noisy 0.00347 | Δ +17.4% | LR 0.00300 | ||ψ-φ|| 0.941\n",
      "[Stage3] L=3 ep 011 | train 0.07465 | val 0.00286 | noisy 0.00347 | Δ +17.6% | LR 0.00300 | ||ψ-φ|| 0.983\n",
      "[Stage3] L=3 ep 012 | train 0.07357 | val 0.00285 | noisy 0.00347 | Δ +17.9% | LR 0.00300 | ||ψ-φ|| 1.024\n",
      "[Stage3] L=3 ep 013 | train 0.07282 | val 0.00284 | noisy 0.00347 | Δ +18.2% | LR 0.00300 | ||ψ-φ|| 1.063\n",
      "[Stage3] L=3 ep 014 | train 0.07260 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00300 | ||ψ-φ|| 1.101\n",
      "[Stage3] L=3 ep 015 | train 0.07079 | val 0.00284 | noisy 0.00347 | Δ +18.2% | LR 0.00300 | ||ψ-φ|| 1.137\n",
      "[Stage3] L=3 ep 016 | train 0.07766 | val 0.00283 | noisy 0.00347 | Δ +18.4% | LR 0.00300 | ||ψ-φ|| 1.175\n",
      "[Stage3] L=3 ep 017 | train 0.08034 | val 0.00282 | noisy 0.00347 | Δ +18.9% | LR 0.00300 | ||ψ-φ|| 1.219\n",
      "[Stage3] L=3 ep 018 | train 0.07194 | val 0.00279 | noisy 0.00347 | Δ +19.7% | LR 0.00300 | ||ψ-φ|| 1.263\n",
      "[Stage3] L=3 ep 019 | train 0.07571 | val 0.00280 | noisy 0.00347 | Δ +19.4% | LR 0.00300 | ||ψ-φ|| 1.306\n",
      "[Stage3] L=3 ep 020 | train 0.07637 | val 0.00281 | noisy 0.00347 | Δ +19.3% | LR 0.00300 | ||ψ-φ|| 1.356\n",
      "[Stage3] L=3 ep 021 | train 0.07350 | val 0.00282 | noisy 0.00347 | Δ +18.9% | LR 0.00300 | ||ψ-φ|| 1.399\n",
      "[Stage3] L=3 ep 022 | train 0.07581 | val 0.00280 | noisy 0.00347 | Δ +19.3% | LR 0.00300 | ||ψ-φ|| 1.441\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 023 | train 0.07518 | val 0.00282 | noisy 0.00347 | Δ +19.0% | LR 0.00150 | ||ψ-φ|| 1.476\n",
      "[Stage3] L=3 ep 024 | train 0.07660 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00150 | ||ψ-φ|| 1.511\n",
      "[Stage3] L=3 ep 025 | train 0.07737 | val 0.00281 | noisy 0.00347 | Δ +19.2% | LR 0.00150 | ||ψ-φ|| 1.539\n",
      "[Stage3] L=3 ep 026 | train 0.07476 | val 0.00280 | noisy 0.00347 | Δ +19.5% | LR 0.00150 | ||ψ-φ|| 1.568\n",
      "[Stage3] L=3 ep 027 | train 0.07418 | val 0.00282 | noisy 0.00347 | Δ +18.8% | LR 0.00150 | ||ψ-φ|| 1.593\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 028 | train 0.07411 | val 0.00283 | noisy 0.00347 | Δ +18.6% | LR 0.00075 | ||ψ-φ|| 1.619\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "Completed 10 runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7 — Train runs (instances × layers)\n",
    "# ============================================\n",
    "RUNS = []  # we’ll save each run in the next cell\n",
    "\n",
    "for L in LAYER_OPTIONS:\n",
    "    for inst in INSTANCE_IDS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Instance {inst} | Layers {L}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        s1 = train_stage1(\n",
    "            X_train, X_val,\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=120, batch_size=32,\n",
    "            lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        s3 = train_stage3(\n",
    "            X_train, X_val,\n",
    "            phi_stage1=s1[\"phi\"],\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16,\n",
    "            LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "            CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        RUNS.append({\n",
    "            \"instance_id\": inst,\n",
    "            \"n_layers\": L,\n",
    "            \"stage1\": {\n",
    "                \"phi\": s1[\"phi\"],\n",
    "                \"best_val\": s1[\"best_val\"],\n",
    "                \"hist_train\": s1[\"hist_train\"],\n",
    "                \"hist_val\": s1[\"hist_val\"],\n",
    "                \"hist_lr\": s1[\"hist_lr\"],\n",
    "                \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                \"epochs\": s1.get(\"epochs\"),\n",
    "                \"train_seconds\": float(t1 - t0),\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"psi\": s3[\"psi\"],\n",
    "                \"best_val\": s3[\"best_val\"],\n",
    "                \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                \"epochs\": s3.get(\"epochs\"),\n",
    "                \"hist_train\": s3[\"hist_train\"],\n",
    "                \"hist_val\": s3[\"hist_val\"],\n",
    "                # NEW: capture these so Cell 8 has them\n",
    "                \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": float(t2 - t1),\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c039220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_1ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_1ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_1ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_1ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_1ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_3ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_3ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_3ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_3ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved bundle → ./runs_halfqae_4L2T/q6_l4t2/6q_4l_2t_3ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "\n",
      "All runs saved and recorded.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 8 — Save artifacts (JSON) and append a paper-ready CSV per run\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams logged (keep in sync with training cells) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.003\n",
    "S3_MAX_EPOCHS    = 60\n",
    "S3_PATIENCE      = 10\n",
    "S3_PLATEAU_STEPS = 5\n",
    "S3_PLATEAU_FACT  = 0.5\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v4\"  # keep same as earlier runs so we reuse the same CSV\n",
    "\n",
    "# --- ensure dirs ---\n",
    "ensure_dir(OUT_BASE)\n",
    "# one folder per architecture (e.g., runs_halfqae/q4_l3t1)\n",
    "subroot = ensure_dir(f\"{OUT_BASE}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "\n",
    "# --- CSV path (shared across ALL architectures/runs) ---\n",
    "CSV_PATH = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "\n",
    "# --- header for the full, paper-friendly table ---\n",
    "CSV_HEADER = [\n",
    "    # id / naming\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    # architecture\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    # noise & window\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    # stage-1 hyperparams + outcomes\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    # stage-3 hyperparams + outcomes\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\"s3_patience\",\"s3_plateau_steps\",\"s3_plateau_factor\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    # params (JSON)\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    # totals\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    # Create only if missing; never rewrite an existing header.\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(header)\n",
    "\n",
    "ensure_csv(CSV_PATH, CSV_HEADER)\n",
    "\n",
    "# ------------------------ safe helpers ------------------------\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def _safe_int(x, default):\n",
    "    if x is None:\n",
    "        return default\n",
    "    try:\n",
    "        # catch \"nan\" float case\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return default\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _safe_float(x, default=np.nan):\n",
    "    if x is None:\n",
    "        return default\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def save_one_run(run):\n",
    "    global stage3_handles  # needed by stage3_val_values_det fallback\n",
    "\n",
    "    inst = int(run[\"instance_id\"])\n",
    "    L    = int(run[\"n_layers\"])\n",
    "    seed = int(run.get(\"seed\", inst))\n",
    "\n",
    "    # standardized filename includes arch + layers + instance, so no collisions\n",
    "    fname = std_instance_name(n_qubits, n_latent, n_trash, L, inst)\n",
    "\n",
    "    # Save all instances for this architecture in the same folder (no per-layer subfolders)\n",
    "    out_dir = subroot\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    # pull stage results (robust to missing keys / None)\n",
    "    s1 = run[\"stage1\"]\n",
    "    s3 = run[\"stage3\"]\n",
    "\n",
    "    # Stage-1 metrics\n",
    "    s1_hist_val = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val = _safe_float(s1.get(\"best_val\"), np.nan)\n",
    "    s1_final_val = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = _safe_int(s1.get(\"best_epoch\"), _safe_argmin(s1_hist_val))\n",
    "    s1_epochs     = _safe_int(s1.get(\"epochs\"), len(s1_hist_val))\n",
    "    s1_seconds    = _safe_float(s1.get(\"train_seconds\"), np.nan)\n",
    "\n",
    "    # Stage-3 series\n",
    "    s3_hist_val   = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "\n",
    "    s3_best_val   = _safe_float(s3.get(\"best_val\"), np.nan)\n",
    "    s3_final_val  = _safe_last(s3_hist_val)\n",
    "    s3_best_epoch = _safe_int(s3.get(\"best_epoch\"), _safe_argmin(s3_hist_val))\n",
    "    s3_epochs     = _safe_int(s3.get(\"epochs\"), len(s3_hist_val))\n",
    "    s3_seconds    = _safe_float(s3.get(\"train_seconds\"), np.nan)\n",
    "\n",
    "    # --- compute metrics with FALLBACKS if curves are missing ---\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = (float(np.nanmax(s3_hist_delta)) if (len(s3_hist_delta) and np.isfinite(np.nanmax(s3_hist_delta)))\n",
    "                      else np.nan)\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    need_fallback = (not len(s3_hist_noisy)) or (not np.isfinite(noisy_baseline)) or (not np.isfinite(final_delta))\n",
    "\n",
    "    if need_fallback:\n",
    "        # Rebuild the QNodes for this (L, phi) so we can evaluate psi on X_val\n",
    "        phi_for_L = np.array(s1.get(\"phi\", []))\n",
    "        stage3_handles = stage3_qnodes(L, phi_for_L)  # sets the fixed decoder from φ\n",
    "        psi_params = np.array(s3.get(\"psi\", []))\n",
    "        # Deterministic validation at σ = EVAL_SIGMA\n",
    "        mN, mD, d_pct = stage3_val_values_det(psi_params, X_val, sigma=EVAL_SIGMA)\n",
    "        noisy_baseline = float(mN)\n",
    "        final_delta    = float(d_pct)\n",
    "        if not np.isfinite(best_delta):  # if we don't have a curve, use final as best\n",
    "            best_delta = final_delta\n",
    "\n",
    "    # bundle JSON (parameters + training curves)\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"id\": data_folder,\n",
    "            \"scale_low\":  float(info[\"scale_low\"]),\n",
    "            \"scale_high\": float(info[\"scale_high\"]),\n",
    "            \"window_size\": int(n_qubits),\n",
    "            \"window_stride\": int(WINDOW_STRIDE),\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(EVAL_SIGMA),\n",
    "            \"sigma_eval\":  float(EVAL_SIGMA),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(n_qubits),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(n_latent),\n",
    "            \"n_trash\":  int(n_trash),\n",
    "            \"latent_wires\": list(range(n_latent)),\n",
    "            \"trash_wires\":  list(range(n_latent, n_qubits)),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"patience\": S3_PATIENCE, \"plateau_steps\": S3_PLATEAU_STEPS, \"plateau_factor\": S3_PLATEAU_FACT,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val,\n",
    "                \"noisy_curve\": s3.get(\"hist_noisy\", []), \"delta_curve\": s3_hist_delta,\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # assemble CSV row\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        data_folder,\n",
    "        inst, seed,\n",
    "        int(n_qubits), int(n_latent), int(n_trash), int(L),\n",
    "        f\"{EVAL_SIGMA:.3f}\", f\"{EVAL_SIGMA:.3f}\", int(WINDOW_STRIDE),\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS), int(S3_PATIENCE), int(S3_PLATEAU_STEPS), f\"{S3_PLATEAU_FACT:.3f}\",\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert row into CSV (by unique filename)\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(CSV_PATH).exists():\n",
    "        df_old = pd.read_csv(CSV_PATH)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(CSV_PATH, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Upserted CSV row  → {CSV_PATH}\")\n",
    "\n",
    "# ---- save all runs from Cell 7 ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved and recorded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae_4L2T/all_training_instances_v4.csv\n",
      "Saved per-layer summary → ./runs_halfqae_4L2T/summary_by_layers_v4.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6q_4l_2t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>47.078807</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>12.941182</td>\n",
       "      <td>11.281768</td>\n",
       "      <td>[0.0005375356304854157, -1.8579223288307554, 0...</td>\n",
       "      <td>[-0.008994822722970534, -1.8689791854625175, -...</td>\n",
       "      <td>117.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6q_4l_2t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>78.013863</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>18.890562</td>\n",
       "      <td>15.981441</td>\n",
       "      <td>[-2.567257366777967e-06, -1.8817604408341266, ...</td>\n",
       "      <td>[0.006751557677767634, -1.9058217937723876, -0...</td>\n",
       "      <td>145.601760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6q_4l_2t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>62.339512</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>19.239168</td>\n",
       "      <td>17.501471</td>\n",
       "      <td>[-4.83237894571108e-06, 1.2182999262317835, 0....</td>\n",
       "      <td>[0.0025336851693088017, 1.2079906408714822, 0....</td>\n",
       "      <td>131.439336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6q_4l_2t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>44.182359</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>11.732284</td>\n",
       "      <td>9.458359</td>\n",
       "      <td>[0.0020109709523342934, -1.852544821308237, -0...</td>\n",
       "      <td>[0.029320779488134923, -1.8424198183351221, 0....</td>\n",
       "      <td>112.992497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6q_4l_2t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>35</td>\n",
       "      <td>46</td>\n",
       "      <td>102.902482</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>20.130484</td>\n",
       "      <td>19.047261</td>\n",
       "      <td>[-1.6016100352142232e-05, -1.919881566039838, ...</td>\n",
       "      <td>[0.006040674573675668, -1.9045764990434473, -0...</td>\n",
       "      <td>172.294678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6q_4l_2t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>114.379120</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>18.458633</td>\n",
       "      <td>14.192294</td>\n",
       "      <td>[-0.22636681010539517, -0.039108268662156345, ...</td>\n",
       "      <td>[-0.04200191536379389, -0.005377837694551939, ...</td>\n",
       "      <td>293.164213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6q_4l_2t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>153.594245</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>20.131725</td>\n",
       "      <td>19.780344</td>\n",
       "      <td>[1.1496546002736785, -1.0216942666292088, -1.0...</td>\n",
       "      <td>[1.1602289470190845, -1.3395185526029607, -1.1...</td>\n",
       "      <td>335.909292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6q_4l_2t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>1052.417701</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>23.549401</td>\n",
       "      <td>22.627437</td>\n",
       "      <td>[0.021942474133891873, -0.09905476947713128, 1...</td>\n",
       "      <td>[0.060424175763688866, -0.08064890692342684, 1...</td>\n",
       "      <td>1242.063308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6q_4l_2t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>121.982521</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>12.684094</td>\n",
       "      <td>7.762785</td>\n",
       "      <td>[-0.850349876060555, -1.4869761147778213, -0.8...</td>\n",
       "      <td>[-0.8161244194506448, -1.5152231655474144, -0....</td>\n",
       "      <td>298.071205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6q_4l_2t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>161.190543</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>19.676425</td>\n",
       "      <td>18.619542</td>\n",
       "      <td>[-1.807573563264794, -1.642203524277058, 0.429...</td>\n",
       "      <td>[-2.078484107236107, -1.6342425767598456, 0.60...</td>\n",
       "      <td>346.035676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag     dataset_folder  instance_id  rng_seed  \\\n",
       "0  6q_4l_2t_1ls_01.json  inst1_L1  mackey_glass_n100            1         1   \n",
       "1  6q_4l_2t_1ls_02.json  inst2_L1  mackey_glass_n100            2         2   \n",
       "2  6q_4l_2t_1ls_03.json  inst3_L1  mackey_glass_n100            3         3   \n",
       "3  6q_4l_2t_1ls_04.json  inst4_L1  mackey_glass_n100            4         4   \n",
       "4  6q_4l_2t_1ls_05.json  inst5_L1  mackey_glass_n100            5         5   \n",
       "5  6q_4l_2t_3ls_01.json  inst1_L3  mackey_glass_n100            1         1   \n",
       "6  6q_4l_2t_3ls_02.json  inst2_L3  mackey_glass_n100            2         2   \n",
       "7  6q_4l_2t_3ls_03.json  inst3_L3  mackey_glass_n100            3         3   \n",
       "8  6q_4l_2t_3ls_04.json  inst4_L3  mackey_glass_n100            4         4   \n",
       "9  6q_4l_2t_3ls_05.json  inst5_L3  mackey_glass_n100            5         5   \n",
       "\n",
       "   n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  s3_final_val_mse  \\\n",
       "0         6         4        2         1          0.1  ...          0.003083   \n",
       "1         6         4        2         1          0.1  ...          0.002919   \n",
       "2         6         4        2         1          0.1  ...          0.002867   \n",
       "3         6         4        2         1          0.1  ...          0.003146   \n",
       "4         6         4        2         1          0.1  ...          0.002813   \n",
       "5         6         4        2         3          0.1  ...          0.002982   \n",
       "6         6         4        2         3          0.1  ...          0.002787   \n",
       "7         6         4        2         3          0.1  ...          0.002688   \n",
       "8         6         4        2         3          0.1  ...          0.003205   \n",
       "9         6         4        2         3          0.1  ...          0.002828   \n",
       "\n",
       "   s3_best_epoch  s3_epochs  s3_train_seconds  s3_noisy_baseline_mse  \\\n",
       "0              9         20         47.078807               0.003475   \n",
       "1             23         34         78.013863               0.003475   \n",
       "2             17         28         62.339512               0.003475   \n",
       "3              8         19         44.182359               0.003475   \n",
       "4             35         46        102.902482               0.003475   \n",
       "5             10         21        114.379120               0.003475   \n",
       "6             17         28        153.594245               0.003475   \n",
       "7             10         21       1052.417701               0.003475   \n",
       "8             11         22        121.982521               0.003475   \n",
       "9             18         29        161.190543               0.003475   \n",
       "\n",
       "   s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0          12.941182           11.281768   \n",
       "1          18.890562           15.981441   \n",
       "2          19.239168           17.501471   \n",
       "3          11.732284            9.458359   \n",
       "4          20.130484           19.047261   \n",
       "5          18.458633           14.192294   \n",
       "6          20.131725           19.780344   \n",
       "7          23.549401           22.627437   \n",
       "8          12.684094            7.762785   \n",
       "9          19.676425           18.619542   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [0.0005375356304854157, -1.8579223288307554, 0...   \n",
       "1  [-2.567257366777967e-06, -1.8817604408341266, ...   \n",
       "2  [-4.83237894571108e-06, 1.2182999262317835, 0....   \n",
       "3  [0.0020109709523342934, -1.852544821308237, -0...   \n",
       "4  [-1.6016100352142232e-05, -1.919881566039838, ...   \n",
       "5  [-0.22636681010539517, -0.039108268662156345, ...   \n",
       "6  [1.1496546002736785, -1.0216942666292088, -1.0...   \n",
       "7  [0.021942474133891873, -0.09905476947713128, 1...   \n",
       "8  [-0.850349876060555, -1.4869761147778213, -0.8...   \n",
       "9  [-1.807573563264794, -1.642203524277058, 0.429...   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [-0.008994822722970534, -1.8689791854625175, -...           117.000458  \n",
       "1  [0.006751557677767634, -1.9058217937723876, -0...           145.601760  \n",
       "2  [0.0025336851693088017, 1.2079906408714822, 0....           131.439336  \n",
       "3  [0.029320779488134923, -1.8424198183351221, 0....           112.992497  \n",
       "4  [0.006040674573675668, -1.9045764990434473, -0...           172.294678  \n",
       "5  [-0.04200191536379389, -0.005377837694551939, ...           293.164213  \n",
       "6  [1.1602289470190845, -1.3395185526029607, -1.1...           335.909292  \n",
       "7  [0.060424175763688866, -0.08064890692342684, 1...          1242.063308  \n",
       "8  [-0.8161244194506448, -1.5152231655474144, -0....           298.071205  \n",
       "9  [-2.078484107236107, -1.6342425767598456, 0.60...           346.035676  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003475 ± 0.000000</td>\n",
       "      <td>0.002898 ± 0.000122</td>\n",
       "      <td>0.002966 ± 0.000128</td>\n",
       "      <td>16.586736 ± 3.514454</td>\n",
       "      <td>14.654060 ± 3.675257</td>\n",
       "      <td>0.256298 ± 0.007674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003475 ± 0.000000</td>\n",
       "      <td>0.002818 ± 0.000123</td>\n",
       "      <td>0.002898 ± 0.000180</td>\n",
       "      <td>18.900056 ± 3.537491</td>\n",
       "      <td>16.596480 ± 5.184911</td>\n",
       "      <td>0.086426 ± 0.011213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.003475 ± 0.000000     0.002898 ± 0.000122   \n",
       "3            5           0.003475 ± 0.000000     0.002818 ± 0.000123   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.002966 ± 0.000128      16.586736 ± 3.514454   \n",
       "3             0.002898 ± 0.000180      18.900056 ± 3.537491   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1              14.654060 ± 3.675257    0.256298 ± 0.007674  \n",
       "3              16.596480 ± 5.184911    0.086426 ± 0.011213  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results table\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(CSV_PATH).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Run Cell 8 first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Drop duplicate runs; keep the newest copy (with baseline/delta)\n",
    "if \"filename\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "# Typical numeric casts (safe)\n",
    "for col in [\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\n",
    "    \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "    \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\",\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\"instance_id\",\"rng_seed\",\n",
    "    \"window_stride\"\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "\n",
    "clean_path = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "Path(OUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved training-only table → {clean_path}\")\n",
    "\n",
    "# A compact per-layer summary (mean±std); guards against all-NaN\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "metrics = [\n",
    "    (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "    (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "    (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "    (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "    (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "    (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "]\n",
    "\n",
    "grp = df.groupby(\"n_layers\", dropna=False)\n",
    "summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "for col, label in metrics:\n",
    "    if col in df.columns and np.isfinite(df[col]).any():\n",
    "        summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "summary_path = f\"{OUT_BASE}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "summary.to_csv(summary_path, index=True)\n",
    "print(f\"Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c34ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
