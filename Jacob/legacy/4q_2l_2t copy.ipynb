{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e979dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# For THIS notebook: 4q / 2 latent / 2 trash\n",
    "# We run 2 layer depths (1,3) × 5 instances = 10 runs PER dataset.\n",
    "# With 2 datasets that’s 20 runs from this notebook.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # five instances per (dataset, depth)\n",
    "LAYER_OPTIONS  = [1, 3]\n",
    "\n",
    "# We use PRE-NOISED datasets for Stage-3 at sigma = 0.20\n",
    "NOISE_SIGMA    = 0.20\n",
    "EVAL_SIGMA     = NOISE_SIGMA\n",
    "\n",
    "# Two Mackey-Glass datasets\n",
    "DATASETS_TO_RUN = [\n",
    "    \"mackey_glass_tau17_n200\",\n",
    "    \"mackey_glass_tau30_n200\",\n",
    "]\n",
    "\n",
    "# Where to save (folder structure fixed below)\n",
    "OUT_BASE = \"./runs_qae\"   # <- per your requested name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "458cfa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils import OK\n",
      "Seed/filename utils ready.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Cell 1 — Imports, utils, reproducibility (fixed seed)\n",
    "# =====================================================\n",
    "import os, sys, json, math, random, time, hashlib, csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- repo utils (your existing readers) -----\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "parent_dir = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'Jacob' else current_dir\n",
    "sys.path.insert(0, parent_dir); sys.path.insert(0, '../')\n",
    "try:\n",
    "    from qae_utils.Files import read_ts_file, read_json_file, create_folder_if_needed\n",
    "    from qae_utils.Window import ts_wind_make, ts_wind_split, ts_wind_flatten_avg, ts_add_noise\n",
    "    print(\"Utils import OK\")\n",
    "except Exception as e:\n",
    "    print(\"Import error:\", e)\n",
    "    qae_utils_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(''))), 'qae_utils')\n",
    "    sys.path.insert(0, os.path.dirname(qae_utils_path))\n",
    "    from qae_utils.Files import read_ts_file, read_json_file, create_folder_if_needed\n",
    "    from qae_utils.Window import ts_wind_make, ts_wind_split, ts_wind_flatten_avg, ts_add_noise\n",
    "    print(\"Absolute path fallback OK\")\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4)\n",
    "\n",
    "def set_global_seed(instance_id: int):\n",
    "    base = 10_000 + int(instance_id)\n",
    "    random.seed(base + 11)\n",
    "    np.random.seed(base + 22)\n",
    "    try:\n",
    "        pnp.random.seed(base + 33)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return dict(global_seed=base, numpy_seed=base+22, pnp_seed=base+33)\n",
    "\n",
    "def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "print(\"Seed/filename utils ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a103b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 2 — Data loading (deterministic)\n",
    "# =========================================\n",
    "DATA_PATH = '../jacobs_examples/aintern/data'\n",
    "WINDOW_STRIDE = 1\n",
    "\n",
    "def load_dataset(folder_name: str):\n",
    "    \"\"\"Return (y_all, info, folder_name) for a dataset folder.\"\"\"\n",
    "    y = read_ts_file(f'{DATA_PATH}/{folder_name}/y_org.arr')   # values\n",
    "    info = read_json_file(f'{DATA_PATH}/{folder_name}/info.json')\n",
    "    return np.asarray(y, dtype=float), info, folder_name\n",
    "\n",
    "# ---- value→angle embedding (uses global `info` set in Cell 7 loop) ----\n",
    "def embed_input(x, info_=None):\n",
    "    \"\"\"\n",
    "    Map value-domain window x (in [lo,hi]) to RY(π·v01).\n",
    "    Accepts optional info to match Stage-3 call signatures.\n",
    "    \"\"\"\n",
    "    if info_ is None:\n",
    "        info_ = info\n",
    "    lo, hi = float(info_['scale_low']), float(info_['scale_high'])\n",
    "    xn = (pnp.array(x) - lo) / max(hi - lo, 1e-12)   # -> [0,1]\n",
    "    for i, v in enumerate(xn):\n",
    "        qml.RY(v * pnp.pi, wires=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2caea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture set: 4q (2 latent and 2 trash).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — Architecture (do NOT change your brick/entanglers)\n",
    "# ============================================================\n",
    "# This notebook = 4 qubits, 2 latent, 2 trash.\n",
    "n_qubits = 4\n",
    "n_latent = 2\n",
    "n_trash  = n_qubits - n_latent\n",
    "trash_wires = [2, 3]            # your original choice\n",
    "signal_wires = list(range(4))   # Stage-1 diagnostics use all wires\n",
    "\n",
    "# --- device factory (simple; default.qubit) ---\n",
    "def make_device(nq): \n",
    "    return qml.device('default.qubit', wires=nq)\n",
    "\n",
    "# --- Stage-1 encoder template (unchanged architecture) ---\n",
    "def encoder_template(params, n_layers):\n",
    "    \"\"\"RX/RY/RZ per qubit + ring CNOT per layer.\"\"\"\n",
    "    assert len(params) == n_layers * n_qubits * 3\n",
    "    for l in range(n_layers):\n",
    "        # local rotations\n",
    "        for q in range(n_qubits):\n",
    "            idx = l * n_qubits * 3 + q * 3\n",
    "            qml.RX(params[idx + 0], wires=q)\n",
    "            qml.RY(params[idx + 1], wires=q)\n",
    "            qml.RZ(params[idx + 2], wires=q)\n",
    "        # ring entanglers\n",
    "        for q in range(n_qubits-1):\n",
    "            qml.CNOT(wires=[q, q+1])\n",
    "        qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "print(\"Architecture set: 4q (2 latent and 2 trash).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "539c5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Cell 4 — Stage-1 QNodes, loss, and training (seeded)\n",
    "# ====================================================\n",
    "def stage1_qnodes(n_layers):\n",
    "    dev = make_device(n_qubits)\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def trash_expectations(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in trash_wires]\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def recon_EdagE(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        qml.adjoint(encoder_template)(phi, n_layers)   # E†\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return trash_expectations, recon_EdagE\n",
    "\n",
    "def stage1_batch_loss(trash_expectations, phi, clean_batch):\n",
    "    # L = mean_{batch,trash} P(|1>) = (1 - Z)/2\n",
    "    zs = []\n",
    "    for c in clean_batch:\n",
    "        z = pnp.array(trash_expectations(phi, c))  # shape (n_trash,)\n",
    "        zs.append(z)\n",
    "    zs = pnp.stack(zs, axis=0)\n",
    "    prob_one = (1.0 - zs) * 0.5\n",
    "    return pnp.mean(prob_one)\n",
    "\n",
    "def train_stage1(X_train, X_val, n_layers, instance_id, \n",
    "                 n_epochs=120, batch_size=32, lr_init=0.010,\n",
    "                 patience=10, lr_patience=8, min_delta=1e-6):\n",
    "    set_global_seed(instance_id)\n",
    "    # init\n",
    "    enc_shape = n_layers * n_qubits * 3\n",
    "    phi = pnp.array(np.random.normal(0, 0.5, enc_shape), requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr_init)\n",
    "    lr = lr_init\n",
    "\n",
    "    trash_expectations, recon_EdagE = stage1_qnodes(n_layers)\n",
    "\n",
    "    # helper\n",
    "    def minibatches(N, B, rng_seed=123456):\n",
    "        rng = np.random.default_rng(rng_seed)  # fixed per-epoch seed below\n",
    "        idx = rng.permutation(N)\n",
    "        for i in range(0, N, B):\n",
    "            yield idx[i:i+B]\n",
    "\n",
    "    train_hist, val_hist, lr_hist = [], [], []\n",
    "    best_phi, best_val = None, float(\"inf\")\n",
    "    no_improve = 0\n",
    "    for ep in range(n_epochs):\n",
    "        # batch order deterministic per-epoch per-instance\n",
    "        seed_ep = 77_000 + 100*instance_id + ep\n",
    "        acc = 0.0; nb = 0\n",
    "        for ix in minibatches(len(X_train), batch_size, rng_seed=seed_ep):\n",
    "            clean_batch = X_train[ix]\n",
    "            def loss_fn(p): return stage1_batch_loss(trash_expectations, p, clean_batch)\n",
    "            phi, cost = opt.step_and_cost(loss_fn, phi)\n",
    "            acc += float(cost); nb += 1\n",
    "        train_cost = acc / max(nb, 1)\n",
    "\n",
    "        # validation\n",
    "        v_costs = []\n",
    "        for c in X_val:\n",
    "            v_costs.append(stage1_batch_loss(trash_expectations, phi, pnp.array([c])))\n",
    "        val_cost = float(pnp.mean(pnp.stack(v_costs)))\n",
    "\n",
    "        train_hist.append(train_cost); val_hist.append(val_cost); lr_hist.append(lr)\n",
    "\n",
    "        if val_cost + min_delta < best_val:\n",
    "            best_val, best_phi = val_cost, pnp.array(phi, requires_grad=False); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % lr_patience) == 0:\n",
    "                lr = max(lr * 0.5, 1e-4)\n",
    "                opt = qml.AdamOptimizer(stepsize=lr)\n",
    "                print(f\"[Stage1] ↓ LR → {lr:.5f}\")\n",
    "            if no_improve >= patience:\n",
    "                print(\"[Stage1] Early stopping.\"); break\n",
    "\n",
    "        print(f\"[Stage1] L={n_layers} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {lr:.5f}\")\n",
    "\n",
    "    phi_best = best_phi if best_phi is not None else phi\n",
    "    return dict(\n",
    "        phi=phi_best, best_val=float(best_val),\n",
    "        hist_train=list(map(float, train_hist)),\n",
    "        hist_val=list(map(float, val_hist)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        recon_EdagE=recon_EdagE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3943eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Cell 5 — Stage-3 (ψ) with fixed decoder = adjoint(Stage-1 encoder φ)\n",
    "#          Uses pre-built noisy windows (σ = NOISE_SIGMA) — no injection\n",
    "# ======================================================================\n",
    "import time\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "def stage3_qnodes(n_layers, phi_stage1):\n",
    "    dev3 = make_device(n_qubits)\n",
    "    theta_fixed = pnp.array(phi_stage1, requires_grad=False).reshape((n_layers, n_qubits, 3))\n",
    "\n",
    "    def encoder_fixed_body(theta):\n",
    "        for l in range(n_layers):\n",
    "            for q in range(n_qubits):\n",
    "                qml.RX(theta[l, q, 0], wires=q)\n",
    "                qml.RY(theta[l, q, 1], wires=q)\n",
    "                qml.RZ(theta[l, q, 2], wires=q)\n",
    "            for q in range(n_qubits-1):\n",
    "                qml.CNOT(wires=[q, q+1])\n",
    "            qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "    def decoder_fixed():\n",
    "        qml.adjoint(encoder_fixed_body)(theta_fixed)\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def encoder_only_expZ_all(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def teacher_code_latents(x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_fixed_body(theta_fixed)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def student_code_latents(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def denoiser_qnode_all(flat_params, x_noisy):\n",
    "        embed_input(x_noisy)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        decoder_fixed()\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return dict(\n",
    "        theta_fixed=theta_fixed,\n",
    "        encoder_only_expZ_all=encoder_only_expZ_all,\n",
    "        teacher_code_latents=teacher_code_latents,\n",
    "        student_code_latents=student_code_latents,\n",
    "        denoiser_qnode_all=denoiser_qnode_all\n",
    "    )\n",
    "\n",
    "# ----- value readout helpers -----\n",
    "def Z_to_values_autograd(z_all):\n",
    "    z_all = pnp.clip(pnp.asarray(z_all), -0.999999, 0.999999)\n",
    "    v01 = pnp.arccos(z_all) / pnp.pi\n",
    "    return v01 * (info[\"scale_high\"] - info[\"scale_low\"]) + info[\"scale_low\"]\n",
    "\n",
    "def first_diff(x): \n",
    "    x = pnp.array(x); return x[1:] - x[:-1]\n",
    "\n",
    "def p1_from_expZ(z): \n",
    "    return (1 - pnp.asarray(z)) * 0.5\n",
    "\n",
    "# ----- validation using pre-noised windows (paired with clean) -----\n",
    "def stage3_val_from_pre_noised(psi, X_noisy, X_clean):\n",
    "    ms_noisy, ms_deno = [], []\n",
    "    for n, c in zip(X_noisy, X_clean):\n",
    "        zD = np.array(stage3_handles[\"denoiser_qnode_all\"](psi, n))\n",
    "        v_hat = np.array(Z_to_values_autograd(zD))\n",
    "        ms_noisy.append(float(np.mean((np.asarray(c) - np.asarray(n))**2)))\n",
    "        ms_deno.append(float(np.mean((np.asarray(c) - v_hat)**2)))\n",
    "    mN, mD = float(np.mean(ms_noisy)), float(np.mean(ms_deno))\n",
    "    d_pct = 100.0 * (1.0 - mD / max(mN, 1e-12))\n",
    "    return mN, mD, d_pct\n",
    "\n",
    "def huber(residual, delta):\n",
    "    r = pnp.abs(residual)\n",
    "    return pnp.where(r <= delta, 0.5*r**2, delta*(r - 0.5*delta))\n",
    "\n",
    "def train_stage3(X_train_clean, X_val_clean, X_train_noisy, X_val_noisy,\n",
    "                 phi_stage1, n_layers, instance_id,\n",
    "                 MAX_EPOCHS=60, BATCH=16, \n",
    "                 LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "                 CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99):\n",
    "\n",
    "    set_global_seed(instance_id)\n",
    "\n",
    "    global stage3_handles\n",
    "    stage3_handles = stage3_qnodes(n_layers, phi_stage1)\n",
    "    enc_all   = stage3_handles[\"encoder_only_expZ_all\"]\n",
    "    teacher_l = stage3_handles[\"teacher_code_latents\"]\n",
    "    denoise   = stage3_handles[\"denoiser_qnode_all\"]\n",
    "\n",
    "    # init ψ near φ\n",
    "    phi_flat = pnp.array(phi_stage1, requires_grad=False)\n",
    "    psi = pnp.array(np.array(phi_flat) + 0.05*np.random.randn(len(phi_flat)), requires_grad=True)\n",
    "\n",
    "    # loss weights (same as before)\n",
    "    ALPHA_REC, BETA_TF, GAMMA_TRASH, L_TV, L_ANCH = 1.0, 0.05, 0.5, 0.05, 2e-4\n",
    "    DELTA_TV, DELTA_Z = 0.02, 0.25\n",
    "\n",
    "    # single-window loss using pre-noised pair\n",
    "    def loss_on_window_pre(params, clean_values, noisy_values):\n",
    "        z_all = pnp.array(enc_all(params, noisy_values))\n",
    "        z_sig, z_tr = z_all[:n_latent], z_all[n_latent:]\n",
    "        zD = pnp.array(denoise(params, noisy_values))\n",
    "        v_hat = Z_to_values_autograd(zD)\n",
    "\n",
    "        L_rec = pnp.mean((pnp.array(clean_values) - v_hat)**2)\n",
    "        z_t_sig = pnp.array(teacher_l(clean_values))\n",
    "        L_tf = pnp.mean(huber(z_t_sig - z_sig, DELTA_Z))\n",
    "        L_tr = pnp.mean(p1_from_expZ(z_tr))\n",
    "        L_tv = pnp.mean(huber(first_diff(clean_values) - first_diff(v_hat), DELTA_TV))\n",
    "        L_anchor = pnp.mean((params - phi_flat)**2)\n",
    "        return (ALPHA_REC*L_rec + BETA_TF*L_tf + GAMMA_TRASH*L_tr + L_TV*L_tv + L_ANCH*L_anchor)\n",
    "\n",
    "    # manual Adam\n",
    "    m = pnp.zeros_like(psi); v = pnp.zeros_like(psi)\n",
    "    b1, b2, eps = 0.9, 0.999, 1e-8\n",
    "    t = 0\n",
    "    def adam_step(params, grad, lr):\n",
    "        nonlocal m, v, t\n",
    "        t += 1\n",
    "        m = b1*m + (1-b1)*grad\n",
    "        v = b2*v + (1-b2)*(grad*grad)\n",
    "        mhat = m/(1-b1**t); vhat = v/(1-b2**t)\n",
    "        return params - lr * (mhat/(pnp.sqrt(vhat)+eps))\n",
    "\n",
    "    # batches (deterministic per-epoch)\n",
    "    def batch_indices(N, B, ep_seed):\n",
    "        rng = np.random.default_rng(ep_seed)\n",
    "        idx = rng.permutation(N)\n",
    "        for s in range(0, N, B):\n",
    "            yield idx[s:s+B]\n",
    "\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve, lr = 0, LR_START\n",
    "    ema = pnp.array(psi, requires_grad=False) if USE_EMA else None\n",
    "\n",
    "    hist_train, hist_val, hist_noisy, hist_delta = [], [], [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(MAX_EPOCHS):\n",
    "        seed_ep = 88_000 + 100*instance_id + ep\n",
    "        acc, nb = 0.0, 0\n",
    "        for ix in batch_indices(len(X_train_clean), BATCH, ep_seed=seed_ep):\n",
    "            for k in ix:\n",
    "                c = X_train_clean[int(k)]\n",
    "                n = X_train_noisy[int(k)]\n",
    "                L = loss_on_window_pre(psi, c, n)\n",
    "                if not pnp.isfinite(L): \n",
    "                    continue\n",
    "                g = qml.grad(lambda p: loss_on_window_pre(p, c, n))(psi)\n",
    "                if not pnp.all(pnp.isfinite(g)): \n",
    "                    continue\n",
    "                # clip\n",
    "                gnorm = pnp.linalg.norm(g) + 1e-12\n",
    "                if gnorm > CLIP_NORM:\n",
    "                    g = g * (CLIP_NORM / gnorm)\n",
    "                psi = adam_step(psi, g, lr)\n",
    "                if USE_EMA:\n",
    "                    ema = EMA_DECAY*ema + (1-EMA_DECAY)*psi\n",
    "                acc += float(L); nb += 1\n",
    "\n",
    "        train_loss = acc / max(nb, 1)\n",
    "        eval_params = ema if USE_EMA else psi\n",
    "\n",
    "        # value-domain validation on pre-noised pairs\n",
    "        mN, mD, dV = stage3_val_from_pre_noised(eval_params, X_val_noisy, X_val_clean)\n",
    "        hist_train.append(train_loss); hist_val.append(mD)\n",
    "        hist_noisy.append(mN);        hist_delta.append(dV)\n",
    "\n",
    "        if mD < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = mD, pnp.array(eval_params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % PLATEAU_STEPS) == 0:\n",
    "                lr = max(lr * PLATEAU_FACTOR, 1e-5)\n",
    "                print(f\"[Stage3] Plateau → LR {lr:.5f}\")\n",
    "\n",
    "        norm_diff = float(pnp.linalg.norm((eval_params - phi_flat)))\n",
    "        print(f\"[Stage3] L={n_layers} ep {ep:03d} | train {train_loss:.5f} | \"\n",
    "              f\"val {mD:.5f} | noisy {mN:.5f} | Δ {dV:+.1f}% | LR {lr:.5f} | ||ψ-φ|| {norm_diff:.3f}\")\n",
    "\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"[Stage3] Early stopping.\"); break\n",
    "\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    epochs_run = len(hist_val)\n",
    "    psi_best = best_params if best_params is not None else (ema if USE_EMA else psi)\n",
    "\n",
    "    return dict(\n",
    "        psi=psi_best, \n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=int(epochs_run),\n",
    "        hist_train=list(map(float, hist_train)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50129aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 6 ready: helpers defined (build_windows, sequential split).\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# Cell 6 — Windowing helpers (sequential split)\n",
    "# ===================================================\n",
    "window_size = n_qubits\n",
    "stride = WINDOW_STRIDE\n",
    "\n",
    "def build_windows(y_vec, window_size, stride):\n",
    "    return np.array(\n",
    "        [y_vec[i:i+window_size] for i in range(0, len(y_vec)-window_size+1, stride)],\n",
    "        dtype=float\n",
    "    )\n",
    "\n",
    "def sequential_train_val_test_split(X_windows, test_frac=0.20, val_frac=0.20):\n",
    "    \"\"\"\n",
    "    Split by order to avoid leakage: first train, then val, last test.\n",
    "    \"\"\"\n",
    "    n_total = len(X_windows)\n",
    "    n_test  = max(1, int(round(test_frac * n_total)))\n",
    "    X_trainval = X_windows[:-n_test]\n",
    "    X_test     = X_windows[-n_test:]\n",
    "    n_val = max(1, int(round(val_frac * len(X_trainval))))\n",
    "    X_train = X_trainval[:-n_val]\n",
    "    X_val   = X_trainval[-n_val:]\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "print(\"Cell 6 ready: helpers defined (build_windows, sequential split).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6711326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mackey_glass_tau17_n200] windows: 197 (W=4, step=1)\n",
      "[mackey_glass_tau17_n200] split → train=126, val=32, test=39\n",
      "\n",
      "==============================\n",
      "[mackey_glass_tau17_n200] Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.492434 | val 0.486984 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.490016 | val 0.484140 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.487615 | val 0.481283 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.485058 | val 0.478396 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.482569 | val 0.475409 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.479388 | val 0.472485 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.476619 | val 0.469424 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.473303 | val 0.466353 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.470964 | val 0.463099 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.467580 | val 0.459785 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.464121 | val 0.456428 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.460391 | val 0.452986 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.457310 | val 0.449473 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.453577 | val 0.446030 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.449432 | val 0.442803 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.446951 | val 0.439537 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.443615 | val 0.436494 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.441161 | val 0.433473 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.437523 | val 0.430707 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.434450 | val 0.428030 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.431990 | val 0.425396 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.428341 | val 0.422881 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.426531 | val 0.420333 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.423561 | val 0.417718 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.420276 | val 0.415154 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.418105 | val 0.412423 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.414952 | val 0.409565 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.410961 | val 0.406661 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.407701 | val 0.403498 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.404118 | val 0.400296 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.399818 | val 0.396911 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.396037 | val 0.393442 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.391313 | val 0.389859 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.386940 | val 0.386219 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.381958 | val 0.382577 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.377901 | val 0.378989 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.372941 | val 0.375339 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.368719 | val 0.371909 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.363215 | val 0.368645 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.359516 | val 0.365508 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.355477 | val 0.362511 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.351616 | val 0.359732 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.347733 | val 0.357375 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.344668 | val 0.355299 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.341770 | val 0.353440 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.339084 | val 0.351893 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.337130 | val 0.350518 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.334822 | val 0.349310 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.333660 | val 0.348239 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.331994 | val 0.347296 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.331062 | val 0.346382 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.329025 | val 0.345471 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.328700 | val 0.344602 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.327561 | val 0.343526 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.325834 | val 0.342436 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.324456 | val 0.341182 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.323392 | val 0.339895 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.322076 | val 0.338599 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.320972 | val 0.337279 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.319721 | val 0.335907 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.317853 | val 0.334641 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.316421 | val 0.333406 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.314969 | val 0.332235 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.314093 | val 0.331095 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.312970 | val 0.330139 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.311076 | val 0.329273 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.310772 | val 0.328486 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.310886 | val 0.327784 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.309293 | val 0.327153 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.309370 | val 0.326571 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.308279 | val 0.326164 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.307309 | val 0.325880 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.306873 | val 0.325539 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.306242 | val 0.325295 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.305819 | val 0.325101 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.306500 | val 0.324876 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.306543 | val 0.324726 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.306791 | val 0.324620 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.306456 | val 0.324555 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.305889 | val 0.324493 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.305876 | val 0.324472 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.306100 | val 0.324431 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.305356 | val 0.324434 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.305858 | val 0.324406 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.305628 | val 0.324401 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.305193 | val 0.324402 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.305877 | val 0.324397 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.305375 | val 0.324422 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.305059 | val 0.324417 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.305472 | val 0.324425 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.305139 | val 0.324434 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.305082 | val 0.324450 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.305329 | val 0.324473 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.305697 | val 0.324496 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 094 | train 0.305448 | val 0.324489 | LR 0.00500\n",
      "[Stage1] L=1 ep 095 | train 0.305395 | val 0.324596 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.22310 | val 0.01169 | noisy 0.01813 | Δ +35.5% | LR 0.00300 | ||ψ-φ|| 0.276\n",
      "[Stage3] L=1 ep 001 | train 0.22128 | val 0.01080 | noisy 0.01813 | Δ +40.4% | LR 0.00300 | ||ψ-φ|| 0.538\n",
      "[Stage3] L=1 ep 002 | train 0.22014 | val 0.01026 | noisy 0.01813 | Δ +43.4% | LR 0.00300 | ||ψ-φ|| 0.784\n",
      "[Stage3] L=1 ep 003 | train 0.21933 | val 0.00992 | noisy 0.01813 | Δ +45.3% | LR 0.00300 | ||ψ-φ|| 0.990\n",
      "[Stage3] L=1 ep 004 | train 0.21860 | val 0.00982 | noisy 0.01813 | Δ +45.8% | LR 0.00300 | ||ψ-φ|| 1.137\n",
      "[Stage3] L=1 ep 005 | train 0.21813 | val 0.00980 | noisy 0.01813 | Δ +45.9% | LR 0.00300 | ||ψ-φ|| 1.257\n",
      "[Stage3] L=1 ep 006 | train 0.21765 | val 0.00971 | noisy 0.01813 | Δ +46.4% | LR 0.00300 | ||ψ-φ|| 1.382\n",
      "[Stage3] L=1 ep 007 | train 0.21735 | val 0.00974 | noisy 0.01813 | Δ +46.3% | LR 0.00300 | ||ψ-φ|| 1.460\n",
      "[Stage3] L=1 ep 008 | train 0.21709 | val 0.00969 | noisy 0.01813 | Δ +46.5% | LR 0.00300 | ||ψ-φ|| 1.557\n",
      "[Stage3] L=1 ep 009 | train 0.21678 | val 0.00968 | noisy 0.01813 | Δ +46.6% | LR 0.00300 | ||ψ-φ|| 1.620\n",
      "[Stage3] L=1 ep 010 | train 0.21670 | val 0.00969 | noisy 0.01813 | Δ +46.5% | LR 0.00300 | ||ψ-φ|| 1.681\n",
      "[Stage3] L=1 ep 011 | train 0.21668 | val 0.00972 | noisy 0.01813 | Δ +46.4% | LR 0.00300 | ||ψ-φ|| 1.730\n",
      "[Stage3] L=1 ep 012 | train 0.21649 | val 0.00969 | noisy 0.01813 | Δ +46.5% | LR 0.00300 | ||ψ-φ|| 1.782\n",
      "[Stage3] L=1 ep 013 | train 0.21639 | val 0.00967 | noisy 0.01813 | Δ +46.6% | LR 0.00300 | ||ψ-φ|| 1.807\n",
      "[Stage3] L=1 ep 014 | train 0.21633 | val 0.00974 | noisy 0.01813 | Δ +46.3% | LR 0.00300 | ||ψ-φ|| 1.840\n",
      "[Stage3] L=1 ep 015 | train 0.21630 | val 0.00971 | noisy 0.01813 | Δ +46.4% | LR 0.00300 | ||ψ-φ|| 1.876\n",
      "[Stage3] L=1 ep 016 | train 0.21627 | val 0.00968 | noisy 0.01813 | Δ +46.6% | LR 0.00300 | ||ψ-φ|| 1.890\n",
      "[Stage3] L=1 ep 017 | train 0.21625 | val 0.00978 | noisy 0.01813 | Δ +46.1% | LR 0.00300 | ||ψ-φ|| 1.890\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 018 | train 0.21615 | val 0.00978 | noisy 0.01813 | Δ +46.0% | LR 0.00150 | ||ψ-φ|| 1.912\n",
      "[Stage3] L=1 ep 019 | train 0.21597 | val 0.00977 | noisy 0.01813 | Δ +46.1% | LR 0.00150 | ||ψ-φ|| 1.933\n",
      "[Stage3] L=1 ep 020 | train 0.21598 | val 0.00976 | noisy 0.01813 | Δ +46.1% | LR 0.00150 | ||ψ-φ|| 1.938\n",
      "[Stage3] L=1 ep 021 | train 0.21602 | val 0.00974 | noisy 0.01813 | Δ +46.3% | LR 0.00150 | ||ψ-φ|| 1.955\n",
      "[Stage3] L=1 ep 022 | train 0.21594 | val 0.00972 | noisy 0.01813 | Δ +46.4% | LR 0.00150 | ||ψ-φ|| 1.957\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 023 | train 0.21599 | val 0.00979 | noisy 0.01813 | Δ +46.0% | LR 0.00075 | ||ψ-φ|| 1.955\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "[mackey_glass_tau17_n200] Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.526793 | val 0.519779 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.520536 | val 0.513786 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.515059 | val 0.508400 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.509869 | val 0.503669 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.505478 | val 0.499608 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.501886 | val 0.496093 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.498670 | val 0.493141 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.495967 | val 0.490615 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.493737 | val 0.488276 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.491769 | val 0.486022 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.489656 | val 0.483823 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.487648 | val 0.481476 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.485647 | val 0.478807 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.483379 | val 0.475790 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.480346 | val 0.472361 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.477119 | val 0.468386 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.473148 | val 0.463960 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.469282 | val 0.458954 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.464080 | val 0.453666 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.459286 | val 0.448016 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.453646 | val 0.442263 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.447480 | val 0.436751 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.442398 | val 0.431246 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.437347 | val 0.426119 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.431766 | val 0.421542 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.428292 | val 0.417424 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.424190 | val 0.413960 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.420616 | val 0.411115 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.416859 | val 0.408752 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.414536 | val 0.406740 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.411397 | val 0.405091 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.411270 | val 0.403657 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.408637 | val 0.402430 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.407421 | val 0.401276 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.405641 | val 0.399931 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.403466 | val 0.398639 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.402288 | val 0.397275 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.400685 | val 0.395825 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.399271 | val 0.394170 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.396327 | val 0.392371 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.394613 | val 0.390384 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.391155 | val 0.388263 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.388008 | val 0.386019 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.385591 | val 0.383594 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.382812 | val 0.381165 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.379319 | val 0.378596 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.376054 | val 0.375939 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.371732 | val 0.373318 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.368611 | val 0.370774 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.364447 | val 0.368249 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.360716 | val 0.365635 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.357126 | val 0.362953 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.353391 | val 0.360254 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.349373 | val 0.357619 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.345102 | val 0.355021 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.341928 | val 0.352300 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.338462 | val 0.349553 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.335461 | val 0.346835 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.332031 | val 0.344320 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.328173 | val 0.341986 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.324866 | val 0.339892 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.322551 | val 0.337874 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.320096 | val 0.336079 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.317612 | val 0.334507 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.316263 | val 0.333094 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.314107 | val 0.331759 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.313069 | val 0.330637 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.311746 | val 0.329572 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.310729 | val 0.328778 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.308984 | val 0.328086 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.308543 | val 0.327391 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.308508 | val 0.326804 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.307185 | val 0.326311 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.306589 | val 0.325934 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.306875 | val 0.325597 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.306232 | val 0.325423 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.305439 | val 0.325254 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.305052 | val 0.325094 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.306070 | val 0.324987 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.304998 | val 0.324873 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.306008 | val 0.324761 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.305308 | val 0.324655 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.305569 | val 0.324594 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.306013 | val 0.324549 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.305477 | val 0.324535 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.305948 | val 0.324493 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.305357 | val 0.324503 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.305114 | val 0.324517 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.306515 | val 0.324495 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.304804 | val 0.324476 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.305275 | val 0.324513 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.304968 | val 0.324506 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.305610 | val 0.324510 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.305450 | val 0.324510 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.305061 | val 0.324522 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.304962 | val 0.324492 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.304922 | val 0.324458 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.304781 | val 0.324459 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.305200 | val 0.324418 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.305583 | val 0.324436 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.305447 | val 0.324394 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.305897 | val 0.324431 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.305217 | val 0.324423 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.305195 | val 0.324461 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.305163 | val 0.324447 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.305475 | val 0.324477 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.304578 | val 0.324473 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.306032 | val 0.324453 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 108 | train 0.305249 | val 0.324460 | LR 0.00500\n",
      "[Stage1] L=1 ep 109 | train 0.305881 | val 0.324454 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.22264 | val 0.01164 | noisy 0.01813 | Δ +35.8% | LR 0.00300 | ||ψ-φ|| 0.298\n",
      "[Stage3] L=1 ep 001 | train 0.22111 | val 0.01067 | noisy 0.01813 | Δ +41.1% | LR 0.00300 | ||ψ-φ|| 0.571\n",
      "[Stage3] L=1 ep 002 | train 0.21986 | val 0.01024 | noisy 0.01813 | Δ +43.5% | LR 0.00300 | ||ψ-φ|| 0.799\n",
      "[Stage3] L=1 ep 003 | train 0.21921 | val 0.01005 | noisy 0.01813 | Δ +44.5% | LR 0.00300 | ||ψ-φ|| 0.985\n",
      "[Stage3] L=1 ep 004 | train 0.21835 | val 0.00982 | noisy 0.01813 | Δ +45.8% | LR 0.00300 | ||ψ-φ|| 1.156\n",
      "[Stage3] L=1 ep 005 | train 0.21802 | val 0.00979 | noisy 0.01813 | Δ +46.0% | LR 0.00300 | ||ψ-φ|| 1.283\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7 — Train runs (instances × layers × datasets) — sequential split\n",
    "# ============================================\n",
    "import time\n",
    "import numpy as np\n",
    "from qae_utils.Window import ts_add_noise  # ensure imported in Cell 1\n",
    "\n",
    "RUNS = []\n",
    "\n",
    "NOISE_GLOBAL_SEED = 4242  # same corruption for all instances within a dataset\n",
    "\n",
    "def build_windows(y_vec, window_size, stride):\n",
    "    return np.array(\n",
    "        [y_vec[i:i+window_size] for i in range(0, len(y_vec)-window_size+1, stride)],\n",
    "        dtype=float\n",
    "    )\n",
    "\n",
    "def sequential_train_val_test_split(X_windows, test_frac=0.20, val_frac=0.20):\n",
    "    \"\"\"\n",
    "    Split by order to avoid leakage: first train, then val, last test.\n",
    "    \"\"\"\n",
    "    n_total = len(X_windows)\n",
    "    n_test  = max(1, int(round(test_frac * n_total)))\n",
    "    X_trainval = X_windows[:-n_test]\n",
    "    X_test     = X_windows[-n_test:]\n",
    "    n_val = max(1, int(round(val_frac * len(X_trainval))))\n",
    "    X_train = X_trainval[:-n_val]\n",
    "    X_val   = X_trainval[-n_val:]\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "window_size = n_qubits\n",
    "stride = WINDOW_STRIDE\n",
    "\n",
    "for dataset_folder in DATASETS_TO_RUN:\n",
    "    # ---- load clean series + info (used by embed & value scaling)\n",
    "    y_all, info_dset, _ = load_dataset(dataset_folder)\n",
    "    info = info_dset  # set global used by QNodes/encoders\n",
    "\n",
    "    # ---- create pre-noised series (σ = 0.20) once per dataset\n",
    "    y_noisy_all = ts_add_noise(\n",
    "        y_all,\n",
    "        noise=NOISE_SIGMA,            # 0.20 defined in Cell 0\n",
    "        noise_type='normal',\n",
    "        clip=False,\n",
    "        range_low=info['scale_low'],\n",
    "        range_high=info['scale_high'],\n",
    "        seed=NOISE_GLOBAL_SEED\n",
    "    )\n",
    "\n",
    "    # ---- build windows (clean & noisy) with identical alignment\n",
    "    X_clean_all = build_windows(y_all,       window_size, stride)\n",
    "    X_noisy_all = build_windows(y_noisy_all, window_size, stride)\n",
    "    assert len(X_clean_all) == len(X_noisy_all), \"Clean/noisy window counts must match.\"\n",
    "\n",
    "    # ---- sequential split (apply same scheme to both)\n",
    "    X_train_clean, X_val_clean, X_test_clean = sequential_train_val_test_split(\n",
    "        X_clean_all, test_frac=0.20, val_frac=0.20\n",
    "    )\n",
    "    X_train_noisy, X_val_noisy, X_test_noisy = sequential_train_val_test_split(\n",
    "        X_noisy_all, test_frac=0.20, val_frac=0.20\n",
    "    )\n",
    "\n",
    "    print(f\"[{dataset_folder}] windows: {len(X_clean_all)} (W={window_size}, step={stride})\")\n",
    "    print(f\"[{dataset_folder}] split → train={len(X_train_clean)}, val={len(X_val_clean)}, test={len(X_test_clean)}\")\n",
    "\n",
    "    # ---- run both depths & instances for THIS dataset\n",
    "    for L in LAYER_OPTIONS:\n",
    "        for inst in INSTANCE_IDS:\n",
    "            print(f\"\\n==============================\")\n",
    "            print(f\"[{dataset_folder}] Instance {inst} | Layers {L}\")\n",
    "            print(f\"==============================\")\n",
    "\n",
    "            # Stage 1 — train on CLEAN windows\n",
    "            t0 = time.time()\n",
    "            s1 = train_stage1(\n",
    "                X_train_clean, X_val_clean,\n",
    "                n_layers=L,\n",
    "                instance_id=inst,\n",
    "                n_epochs=120, batch_size=32,\n",
    "                lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6\n",
    "            )\n",
    "            t1 = time.time()\n",
    "\n",
    "            # Stage 3 — train on PRE-NOISED windows (σ=0.20)\n",
    "            s3 = train_stage3(\n",
    "                X_train_clean, X_val_clean,\n",
    "                X_train_noisy, X_val_noisy,\n",
    "                phi_stage1=s1[\"phi\"],\n",
    "                n_layers=L,\n",
    "                instance_id=inst,\n",
    "                MAX_EPOCHS=60, BATCH=16,\n",
    "                LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "                CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99\n",
    "            )\n",
    "            t2 = time.time()\n",
    "\n",
    "            RUNS.append({\n",
    "                \"dataset_folder\": dataset_folder,\n",
    "                \"dataset_info\": {\"scale_low\": float(info[\"scale_low\"]), \"scale_high\": float(info[\"scale_high\"])},\n",
    "                \"instance_id\": inst,\n",
    "                \"n_layers\": L,\n",
    "                \"stage1\": {\n",
    "                    \"phi\": s1[\"phi\"],\n",
    "                    \"best_val\": s1[\"best_val\"],\n",
    "                    \"hist_train\": s1[\"hist_train\"],\n",
    "                    \"hist_val\": s1[\"hist_val\"],\n",
    "                    \"hist_lr\": s1[\"hist_lr\"],\n",
    "                    \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                    \"epochs\": s1.get(\"epochs\"),\n",
    "                    \"train_seconds\": float(t1 - t0),\n",
    "                },\n",
    "                \"stage3\": {\n",
    "                    \"psi\": s3[\"psi\"],\n",
    "                    \"best_val\": s3[\"best_val\"],\n",
    "                    \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                    \"epochs\": s3.get(\"epochs\"),\n",
    "                    \"hist_train\": s3[\"hist_train\"],\n",
    "                    \"hist_val\": s3[\"hist_val\"],\n",
    "                    \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                    \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                    \"train_seconds\": float(t2 - t1),\n",
    "                }\n",
    "            })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs across {len(DATASETS_TO_RUN)} datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c039220",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 221\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# ---- save all runs from Cell 7 ----\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m RUNS:\n\u001b[0;32m--> 221\u001b[0m     save_one_run(run)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll runs saved and recorded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 95\u001b[0m, in \u001b[0;36msave_one_run\u001b[0;34m(run)\u001b[0m\n\u001b[1;32m     93\u001b[0m s1_best_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(s1\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_val\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan))\n\u001b[1;32m     94\u001b[0m s1_final_val \u001b[38;5;241m=\u001b[39m _safe_last(s1_hist_val)\n\u001b[0;32m---> 95\u001b[0m s1_best_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(s1\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, _safe_argmin(s1_hist_val)))\n\u001b[1;32m     96\u001b[0m s1_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(s1\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(s1_hist_val)))\n\u001b[1;32m     97\u001b[0m s1_seconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(s1\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan))\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 8 — Save artifacts (dataset-specific JSON bundles + CSV)\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams (for logging) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.003\n",
    "S3_MAX_EPOCHS    = 60\n",
    "S3_PATIENCE      = 10\n",
    "S3_PLATEAU_STEPS = 5\n",
    "S3_PLATEAU_FACT  = 0.5\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v2\"\n",
    "\n",
    "def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\"s3_patience\",\"s3_plateau_steps\",\"s3_plateau_factor\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    needs_header = True\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                first_line = f.readline().rstrip(\"\\n\")\n",
    "            needs_header = (first_line != \",\".join(header))\n",
    "        except Exception:\n",
    "            needs_header = True\n",
    "    if needs_header:\n",
    "        ensure_dir(os.path.dirname(path))\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(header)\n",
    "\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def std_instance_name(nq, n_latent, n_trash, n_layers, instance_id):\n",
    "    # keep your historical naming\n",
    "    return f\"{int(nq)}q_{int(n_latent)}l_{int(n_trash)}t_{int(n_layers)}ls_{int(instance_id):02d}.json\"\n",
    "\n",
    "def save_one_run(run):\n",
    "    dataset_folder = run[\"dataset_folder\"]\n",
    "    dataset_info   = run.get(\"dataset_info\", {})\n",
    "    scale_low      = float(dataset_info.get(\"scale_low\", np.nan))\n",
    "    scale_high     = float(dataset_info.get(\"scale_high\", np.nan))\n",
    "\n",
    "    # THIS notebook's fixed architecture\n",
    "    n_qubits = 4\n",
    "    n_latent = 2\n",
    "    n_trash  = n_qubits - n_latent\n",
    "\n",
    "    # per-dataset, per-model-set subroot:\n",
    "    subroot = ensure_dir(f\"{OUT_BASE}/{dataset_folder}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "    # CSV is stored inside the model-set folder (so each set has its own CSVs)\n",
    "    csv_path = f\"{subroot}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "    ensure_csv(csv_path, CSV_HEADER)\n",
    "\n",
    "    inst = int(run[\"instance_id\"])\n",
    "    L    = int(run[\"n_layers\"])\n",
    "    seed = int(run.get(\"seed\", inst))\n",
    "\n",
    "    # JSONs live in L1/ or L3/\n",
    "    out_dir = ensure_dir(f\"{subroot}/L{L}\")\n",
    "    fname = std_instance_name(n_qubits, n_latent, n_trash, L, inst)\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    s1 = run[\"stage1\"]; s3 = run[\"stage3\"]\n",
    "\n",
    "    # Stage-1\n",
    "    s1_hist_val   = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val   = float(s1.get(\"best_val\", np.nan))\n",
    "    s1_final_val  = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = int(s1.get(\"best_epoch\", _safe_argmin(s1_hist_val)))\n",
    "    s1_epochs     = int(s1.get(\"epochs\", len(s1_hist_val)))\n",
    "    s1_seconds    = float(s1.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # Stage-3\n",
    "    s3_hist_val   = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "\n",
    "    s3_best_val   = float(s3.get(\"best_val\", np.nan))\n",
    "    s3_final_val  = _safe_last(s3_hist_val)\n",
    "    s3_best_epoch = int(s3.get(\"best_epoch\", _safe_argmin(s3_hist_val)))\n",
    "    s3_epochs     = int(s3.get(\"epochs\", len(s3_hist_val)))\n",
    "    s3_seconds    = float(s3.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = float(np.nanmax(s3_hist_delta))  if len(s3_hist_delta)  else np.nan\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"id\": dataset_folder,\n",
    "            \"scale_low\":  scale_low,\n",
    "            \"scale_high\": scale_high,\n",
    "            \"window_size\": int(n_qubits),\n",
    "            \"window_stride\": int(WINDOW_STRIDE),\n",
    "            \"noise_sigma\": float(NOISE_SIGMA),\n",
    "            \"noise_mode\": \"pre_series_normal_clip_false\"\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(NOISE_SIGMA),\n",
    "            \"sigma_eval\":  float(NOISE_SIGMA),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(n_qubits),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(n_latent),\n",
    "            \"n_trash\":  int(n_trash),\n",
    "            \"latent_wires\": list(range(n_latent)),\n",
    "            \"trash_wires\":  list(range(n_latent, n_qubits)),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"patience\": S3_PATIENCE, \"plateau_steps\": S3_PLATEAU_STEPS, \"plateau_factor\": S3_PLATEAU_FACT,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val,\n",
    "                \"noisy_curve\": s3.get(\"hist_noisy\", []), \"delta_curve\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    ensure_dir(os.path.dirname(bundle_path))\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # CSV row (per-model-set CSV inside subroot)\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        dataset_folder,\n",
    "        inst, seed,\n",
    "        int(n_qubits), int(n_latent), int(n_trash), int(L),\n",
    "        f\"{NOISE_SIGMA:.3f}\", f\"{NOISE_SIGMA:.3f}\", int(WINDOW_STRIDE),\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS), int(S3_PATIENCE), int(S3_PLATEAU_STEPS), f\"{S3_PLATEAU_FACT:.3f}\",\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert into per-model-set CSV\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(csv_path).exists():\n",
    "        df_old = pd.read_csv(csv_path)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(csv_path, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Upserted CSV row  → {csv_path}\")\n",
    "\n",
    "# ---- save all runs ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved per dataset & model-set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved per-layer summary → ./runs_halfqae/summary_by_layers_v2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4q_2l_2t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>32.788465</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>11.878268</td>\n",
       "      <td>11.878268</td>\n",
       "      <td>[0.00017403888647591283, -1.9710404535850785, ...</td>\n",
       "      <td>[-0.0006123296470692364, -2.0262762252890805, ...</td>\n",
       "      <td>86.059745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4q_2l_2t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>28.605802</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>11.336218</td>\n",
       "      <td>11.336218</td>\n",
       "      <td>[1.8056755714875147e-05, -1.9193147771584536, ...</td>\n",
       "      <td>[-0.005484925176926273, -2.0216083452573383, -...</td>\n",
       "      <td>81.824216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4q_2l_2t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>64.523617</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>23.067725</td>\n",
       "      <td>23.067725</td>\n",
       "      <td>[0.00021396068902014424, 1.0656310158671813, 0...</td>\n",
       "      <td>[0.0022221163621398794, 1.0046762387673538, 1....</td>\n",
       "      <td>102.940782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4q_2l_2t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>37.238774</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>15.307742</td>\n",
       "      <td>15.307742</td>\n",
       "      <td>[3.9331420911905245e-05, -2.0191278619373705, ...</td>\n",
       "      <td>[0.0016684085266360368, -2.0860432857437528, 0...</td>\n",
       "      <td>91.399046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4q_2l_2t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>27.810920</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>12.813947</td>\n",
       "      <td>12.813947</td>\n",
       "      <td>[-3.1875221778984855e-05, -1.909854775842016, ...</td>\n",
       "      <td>[-0.012663044461042546, -2.009391188962638, -0...</td>\n",
       "      <td>75.792944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4q_2l_2t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>36</td>\n",
       "      <td>47</td>\n",
       "      <td>192.629691</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>32.257238</td>\n",
       "      <td>32.257238</td>\n",
       "      <td>[1.4721530341460392, 0.03237289093849179, -0.1...</td>\n",
       "      <td>[1.5373755944746403, 0.013355311686751007, -0....</td>\n",
       "      <td>328.944969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4q_2l_2t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>140.302912</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>33.813029</td>\n",
       "      <td>33.813029</td>\n",
       "      <td>[0.019330858162530336, -0.28521034298242903, -...</td>\n",
       "      <td>[-0.008000250494283483, -0.2607754835185739, -...</td>\n",
       "      <td>277.924827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4q_2l_2t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>83.783044</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>21.239540</td>\n",
       "      <td>21.239540</td>\n",
       "      <td>[0.7013220902762077, 1.1604341975538626, 0.576...</td>\n",
       "      <td>[0.5618612928893267, 1.2522178748846688, 0.577...</td>\n",
       "      <td>222.222205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4q_2l_2t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>90.352279</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>24.546582</td>\n",
       "      <td>24.546582</td>\n",
       "      <td>[-0.20819083620731277, 0.033144625958318935, 1...</td>\n",
       "      <td>[-0.14742413582161445, 0.06266292593559304, 1....</td>\n",
       "      <td>229.604356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4q_2l_2t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>107.694295</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>32.257212</td>\n",
       "      <td>32.257212</td>\n",
       "      <td>[1.3362803038788154, 0.03851371435683986, 1.06...</td>\n",
       "      <td>[1.574032281646895, 0.03304526435829487, 1.299...</td>\n",
       "      <td>241.914114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag     dataset_folder  instance_id  rng_seed  \\\n",
       "0  4q_2l_2t_1ls_01.json  inst1_L1  mackey_glass_n100            1         1   \n",
       "1  4q_2l_2t_1ls_02.json  inst2_L1  mackey_glass_n100            2         2   \n",
       "2  4q_2l_2t_1ls_03.json  inst3_L1  mackey_glass_n100            3         3   \n",
       "3  4q_2l_2t_1ls_04.json  inst4_L1  mackey_glass_n100            4         4   \n",
       "4  4q_2l_2t_1ls_05.json  inst5_L1  mackey_glass_n100            5         5   \n",
       "5  4q_2l_2t_3ls_01.json  inst1_L3  mackey_glass_n100            1         1   \n",
       "6  4q_2l_2t_3ls_02.json  inst2_L3  mackey_glass_n100            2         2   \n",
       "7  4q_2l_2t_3ls_03.json  inst3_L3  mackey_glass_n100            3         3   \n",
       "8  4q_2l_2t_3ls_04.json  inst4_L3  mackey_glass_n100            4         4   \n",
       "9  4q_2l_2t_3ls_05.json  inst5_L3  mackey_glass_n100            5         5   \n",
       "\n",
       "   n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  s3_final_val_mse  \\\n",
       "0         4         2        2         1          0.1  ...          0.003574   \n",
       "1         4         2        2         1          0.1  ...          0.003603   \n",
       "2         4         2        2         1          0.1  ...          0.003016   \n",
       "3         4         2        2         1          0.1  ...          0.003487   \n",
       "4         4         2        2         1          0.1  ...          0.003599   \n",
       "5         4         2        2         3          0.1  ...          0.002705   \n",
       "6         4         2        2         3          0.1  ...          0.002606   \n",
       "7         4         2        2         3          0.1  ...          0.003348   \n",
       "8         4         2        2         3          0.1  ...          0.003031   \n",
       "9         4         2        2         3          0.1  ...          0.002757   \n",
       "\n",
       "   s3_best_epoch  s3_epochs  s3_train_seconds  s3_noisy_baseline_mse  \\\n",
       "0              9         20         32.788465               0.003908   \n",
       "1              6         17         28.605802               0.003908   \n",
       "2             28         39         64.523617               0.003908   \n",
       "3             10         21         37.238774               0.003908   \n",
       "4              6         17         27.810920               0.003908   \n",
       "5             36         47        192.629691               0.003908   \n",
       "6             23         34        140.302912               0.003908   \n",
       "7              9         20         83.783044               0.003908   \n",
       "8             11         22         90.352279               0.003908   \n",
       "9             16         27        107.694295               0.003908   \n",
       "\n",
       "   s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0          11.878268           11.878268   \n",
       "1          11.336218           11.336218   \n",
       "2          23.067725           23.067725   \n",
       "3          15.307742           15.307742   \n",
       "4          12.813947           12.813947   \n",
       "5          32.257238           32.257238   \n",
       "6          33.813029           33.813029   \n",
       "7          21.239540           21.239540   \n",
       "8          24.546582           24.546582   \n",
       "9          32.257212           32.257212   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [0.00017403888647591283, -1.9710404535850785, ...   \n",
       "1  [1.8056755714875147e-05, -1.9193147771584536, ...   \n",
       "2  [0.00021396068902014424, 1.0656310158671813, 0...   \n",
       "3  [3.9331420911905245e-05, -2.0191278619373705, ...   \n",
       "4  [-3.1875221778984855e-05, -1.909854775842016, ...   \n",
       "5  [1.4721530341460392, 0.03237289093849179, -0.1...   \n",
       "6  [0.019330858162530336, -0.28521034298242903, -...   \n",
       "7  [0.7013220902762077, 1.1604341975538626, 0.576...   \n",
       "8  [-0.20819083620731277, 0.033144625958318935, 1...   \n",
       "9  [1.3362803038788154, 0.03851371435683986, 1.06...   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [-0.0006123296470692364, -2.0262762252890805, ...            86.059745  \n",
       "1  [-0.005484925176926273, -2.0216083452573383, -...            81.824216  \n",
       "2  [0.0022221163621398794, 1.0046762387673538, 1....           102.940782  \n",
       "3  [0.0016684085266360368, -2.0860432857437528, 0...            91.399046  \n",
       "4  [-0.012663044461042546, -2.009391188962638, -0...            75.792944  \n",
       "5  [1.5373755944746403, 0.013355311686751007, -0....           328.944969  \n",
       "6  [-0.008000250494283483, -0.2607754835185739, -...           277.924827  \n",
       "7  [0.5618612928893267, 1.2522178748846688, 0.577...           222.222205  \n",
       "8  [-0.14742413582161445, 0.06266292593559304, 1....           229.604356  \n",
       "9  [1.574032281646895, 0.03304526435829487, 1.299...           241.914114  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003908 ± 0.000000</td>\n",
       "      <td>0.003326 ± 0.000169</td>\n",
       "      <td>0.003456 ± 0.000224</td>\n",
       "      <td>14.880780 ± 4.314143</td>\n",
       "      <td>14.880780 ± 4.314143</td>\n",
       "      <td>0.231989 ± 0.003387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003908 ± 0.000000</td>\n",
       "      <td>0.002781 ± 0.000195</td>\n",
       "      <td>0.002889 ± 0.000269</td>\n",
       "      <td>28.822720 ± 4.985676</td>\n",
       "      <td>28.822720 ± 4.985676</td>\n",
       "      <td>0.055975 ± 0.015050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.003908 ± 0.000000     0.003326 ± 0.000169   \n",
       "3            5           0.003908 ± 0.000000     0.002781 ± 0.000195   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.003456 ± 0.000224      14.880780 ± 4.314143   \n",
       "3             0.002889 ± 0.000269      28.822720 ± 4.985676   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1              14.880780 ± 4.314143    0.231989 ± 0.003387  \n",
       "3              28.822720 ± 4.985676    0.055975 ± 0.015050  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results tables\n",
    "#            (one CSV per dataset *and* per model-set)\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v2\"\n",
    "\n",
    "# THIS notebook = q4_l2t2 only\n",
    "n_qubits, n_latent, n_trash = 4, 2, 2\n",
    "\n",
    "for dataset_folder in DATASETS_TO_RUN:\n",
    "    subroot = f\"{OUT_BASE}/{dataset_folder}/q{n_qubits}_l{n_latent}t{n_trash}\"\n",
    "    csv_path = f\"{subroot}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "    if not Path(csv_path).exists():\n",
    "        print(f\"[WARN] CSV not found for {dataset_folder}: {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # drop dups per file (keep last)\n",
    "    if \"filename\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "    else:\n",
    "        df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "    # numeric casts (safe)\n",
    "    for col in [\n",
    "        \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "        \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "        \"s1_best_val\",\"s1_final_val\",\n",
    "        \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "        \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\"\n",
    "    ]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "    clean_path = f\"{subroot}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "    Path(subroot).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(clean_path, index=False)\n",
    "    print(f\"[{dataset_folder}] Saved table → {clean_path}\")\n",
    "\n",
    "    # per-layer summary\n",
    "    grp = df.groupby(\"n_layers\", dropna=False)\n",
    "    summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "    for col, label in [\n",
    "        (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "        (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "        (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "        (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "        (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "        (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "    ]:\n",
    "        if col in df.columns and np.isfinite(df[col]).any():\n",
    "            summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "    summary_path = f\"{subroot}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "    summary.to_csv(summary_path, index=True)\n",
    "    print(f\"[{dataset_folder}] Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "    display(df.head(8))\n",
    "    display(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
