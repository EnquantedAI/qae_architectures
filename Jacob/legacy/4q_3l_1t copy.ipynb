{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e979dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# We'll run 5 instances and two depths: 1 and 3 layers.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # used in filenames as ..._ls_01.json, ..._ls_02.json, ...\n",
    "LAYER_OPTIONS  = [1, 3]            # train 1-layer first, then 3-layers\n",
    "EVAL_SIGMA     = 0.10              # fixed noise everywhere (train & eval)\n",
    "\n",
    "# where to save artifacts (JSON bundles, instance records, CSV summary)\n",
    "# tip: new folder so these runs don't mix with your 2L/2T ones\n",
    "OUT_BASE = \"./runs_halfqae_3L1T\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "458cfa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils import OK\n",
      "Seed/filename utils ready.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Cell 1 — Imports, utils, reproducibility (fixed seed)\n",
    "# =====================================================\n",
    "import os, sys, json, math, random, time, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- repo utils (your existing readers) -----\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "parent_dir = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'Jacob' else current_dir\n",
    "sys.path.insert(0, parent_dir); sys.path.insert(0, '../')\n",
    "try:\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Utils import OK\")\n",
    "except Exception as e:\n",
    "    print(\"Import error:\", e)\n",
    "    qae_utils_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(''))), 'qae_utils')\n",
    "    sys.path.insert(0, os.path.dirname(qae_utils_path))\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Absolute path fallback OK\")\n",
    "\n",
    "assert callable(read_ts_file) and callable(read_json_file)\n",
    "\n",
    "# ----- plotting defaults -----\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4)\n",
    "\n",
    "# ----- reproducibility -----\n",
    "def set_global_seed(instance_id: int):\n",
    "    \"\"\"\n",
    "    Derive all RNGs from a simple instance ID (1..5).\n",
    "    Keep the mapping stable across notebooks.\n",
    "    \"\"\"\n",
    "    base = 10_000 + int(instance_id)  # simple, memorable\n",
    "    random.seed(base + 11)\n",
    "    np.random.seed(base + 22)\n",
    "    try:\n",
    "        pnp.random.seed(base + 33)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Pennylane's default.qubit is deterministic given params; no device seeding needed.\n",
    "    return dict(global_seed=base, numpy_seed=base+22, pnp_seed=base+33)\n",
    "\n",
    "def std_instance_name(nq, n_latent, n_trash, n_layers, instance_id):\n",
    "    \"\"\"\n",
    "    Standardized filename pattern used across the project.\n",
    "    Example: 4q_2l_2t_3ls_01.json\n",
    "    \"\"\"\n",
    "    return f\"{int(nq)}q_{int(n_latent)}l_{int(n_trash)}t_{int(n_layers)}ls_{int(instance_id):02d}.json\"\n",
    "\n",
    "def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "print(\"Seed/filename utils ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a103b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: mackey_glass_n100\n",
      "Loaded 100 samples; scale [0.200,0.800]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 2 — Data loading (deterministic)\n",
    "# =========================================\n",
    "DATA_PATH = '../jacobs_examples/aintern/data'\n",
    "DATA_NAME = 'mackey_glass'  # your folder prefix\n",
    "\n",
    "# fixed split across all instances/layers (so results are comparable)\n",
    "SPLIT_RANDOM_STATE = 42\n",
    "WINDOW_STRIDE = 1\n",
    "\n",
    "# ---- pick most recent MG folder starting with mackey_glass* ----\n",
    "data_folders = [f for f in os.listdir(DATA_PATH) if f.startswith(DATA_NAME)]\n",
    "if not data_folders:\n",
    "    raise FileNotFoundError(\"No Mackey-Glass data found. Generate it first.\")\n",
    "data_folders.sort()\n",
    "data_folder = data_folders[-1]     # take the last one if multiple\n",
    "print(f\"Using data folder: {data_folder}\")\n",
    "\n",
    "# ---- load series + scaling info ----\n",
    "X_idx = read_ts_file(f'{DATA_PATH}/{data_folder}/x_org.arr')   # indices for plotting, not used\n",
    "y_all = read_ts_file(f'{DATA_PATH}/{data_folder}/y_org.arr')   # values\n",
    "info  = read_json_file(f'{DATA_PATH}/{data_folder}/info.json')\n",
    "\n",
    "print(f\"Loaded {len(y_all)} samples; scale [{info['scale_low']:.3f},{info['scale_high']:.3f}]\")\n",
    "\n",
    "# ---- helper: uniform embed wrapper (works with/without explicit info param)\n",
    "def embed_input(x, info_=None):\n",
    "    \"\"\"\n",
    "    Map value-domain window x (in [lo,hi]) to RY(π·v01).\n",
    "    Accepts optional info to match Stage-3 call signatures.\n",
    "    \"\"\"\n",
    "    if info_ is None:\n",
    "        info_ = info\n",
    "    lo, hi = info_['scale_low'], info_['scale_high']\n",
    "    xn = (pnp.array(x) - lo) / max(hi - lo, 1e-12)   # -> [0,1]\n",
    "    for i, v in enumerate(xn):\n",
    "        qml.RY(v * pnp.pi, wires=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2caea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture set: 4q (3 latent and 1 trash).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — Architecture (do NOT change your brick/entanglers)\n",
    "# ============================================================\n",
    "# This notebook = 4 qubits, 3 latent, 1 trash.\n",
    "n_qubits = 4\n",
    "n_latent = 3\n",
    "n_trash  = n_qubits - n_latent\n",
    "latent_wires = list(range(n_latent))            # [0, 1, 2]\n",
    "trash_wires  = list(range(n_latent, n_qubits))  # [3]\n",
    "signal_wires = list(range(n_qubits))            # Stage-1 diagnostics use all wires\n",
    "assert n_latent + n_trash == n_qubits\n",
    "\n",
    "# --- device factory (simple; default.qubit) ---\n",
    "def make_device(nq): \n",
    "    return qml.device('default.qubit', wires=nq)\n",
    "\n",
    "# --- Stage-1 encoder template (unchanged architecture) ---\n",
    "def encoder_template(params, n_layers):\n",
    "    \"\"\"RX/RY/RZ per qubit + ring CNOT per layer.\"\"\"\n",
    "    assert len(params) == n_layers * n_qubits * 3\n",
    "    for l in range(n_layers):\n",
    "        # local rotations\n",
    "        for q in range(n_qubits):\n",
    "            idx = l * n_qubits * 3 + q * 3\n",
    "            qml.RX(params[idx + 0], wires=q)\n",
    "            qml.RY(params[idx + 1], wires=q)\n",
    "            qml.RZ(params[idx + 2], wires=q)\n",
    "        # ring entanglers\n",
    "        for q in range(n_qubits-1):\n",
    "            qml.CNOT(wires=[q, q+1])\n",
    "        qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "print(\"Architecture set: 4q (3 latent and 1 trash).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539c5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Cell 4 — Stage-1 QNodes, loss, and training (seeded)\n",
    "# ====================================================\n",
    "def stage1_qnodes(n_layers):\n",
    "    dev = make_device(n_qubits)\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def trash_expectations(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in trash_wires]\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def recon_EdagE(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        qml.adjoint(encoder_template)(phi, n_layers)   # E†\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return trash_expectations, recon_EdagE\n",
    "\n",
    "def stage1_batch_loss(trash_expectations, phi, clean_batch):\n",
    "    # L = mean_{batch,trash} P(|1>) = (1 - Z)/2\n",
    "    zs = []\n",
    "    for c in clean_batch:\n",
    "        z = pnp.array(trash_expectations(phi, c))  # shape (n_trash,)\n",
    "        zs.append(z)\n",
    "    zs = pnp.stack(zs, axis=0)\n",
    "    prob_one = (1.0 - zs) * 0.5\n",
    "    return pnp.mean(prob_one)\n",
    "\n",
    "def train_stage1(X_train, X_val, n_layers, instance_id, \n",
    "                 n_epochs=120, batch_size=32, lr_init=0.010,\n",
    "                 patience=10, lr_patience=8, min_delta=1e-6):\n",
    "    set_global_seed(instance_id)\n",
    "    # init\n",
    "    enc_shape = n_layers * n_qubits * 3\n",
    "    phi = pnp.array(np.random.normal(0, 0.5, enc_shape), requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr_init)\n",
    "    lr = lr_init\n",
    "\n",
    "    trash_expectations, recon_EdagE = stage1_qnodes(n_layers)\n",
    "\n",
    "    # helper\n",
    "    def minibatches(N, B, rng_seed=123456):\n",
    "        rng = np.random.default_rng(rng_seed)  # fixed per-epoch seed below\n",
    "        idx = rng.permutation(N)\n",
    "        for i in range(0, N, B):\n",
    "            yield idx[i:i+B]\n",
    "\n",
    "    train_hist, val_hist, lr_hist = [], [], []\n",
    "    best_phi, best_val = None, float(\"inf\")\n",
    "    no_improve = 0\n",
    "    for ep in range(n_epochs):\n",
    "        # batch order deterministic per-epoch per-instance\n",
    "        seed_ep = 77_000 + 100*instance_id + ep\n",
    "        acc = 0.0; nb = 0\n",
    "        for ix in minibatches(len(X_train), batch_size, rng_seed=seed_ep):\n",
    "            clean_batch = X_train[ix]\n",
    "            def loss_fn(p): return stage1_batch_loss(trash_expectations, p, clean_batch)\n",
    "            phi, cost = opt.step_and_cost(loss_fn, phi)\n",
    "            acc += float(cost); nb += 1\n",
    "        train_cost = acc / max(nb, 1)\n",
    "\n",
    "        # validation\n",
    "        v_costs = []\n",
    "        for c in X_val:\n",
    "            v_costs.append(stage1_batch_loss(trash_expectations, phi, pnp.array([c])))\n",
    "        val_cost = float(pnp.mean(pnp.stack(v_costs)))\n",
    "\n",
    "        train_hist.append(train_cost); val_hist.append(val_cost); lr_hist.append(lr)\n",
    "\n",
    "        if val_cost + min_delta < best_val:\n",
    "            best_val, best_phi = val_cost, pnp.array(phi, requires_grad=False); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % lr_patience) == 0:\n",
    "                lr = max(lr * 0.5, 1e-4)\n",
    "                opt = qml.AdamOptimizer(stepsize=lr)\n",
    "                print(f\"[Stage1] ↓ LR → {lr:.5f}\")\n",
    "            if no_improve >= patience:\n",
    "                print(\"[Stage1] Early stopping.\"); break\n",
    "\n",
    "        print(f\"[Stage1] L={n_layers} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {lr:.5f}\")\n",
    "\n",
    "    phi_best = best_phi if best_phi is not None else phi\n",
    "    return dict(\n",
    "        phi=phi_best, best_val=float(best_val),\n",
    "        hist_train=list(map(float, train_hist)),\n",
    "        hist_val=list(map(float, val_hist)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        recon_EdagE=recon_EdagE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3943eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Cell 5 — Stage-3 (ψ) with fixed decoder = adjoint(Stage-1 encoder φ)\n",
    "#          (records hist_noisy, hist_delta, best_epoch, epochs, seconds)\n",
    "# ======================================================================\n",
    "import time\n",
    "\n",
    "def stage3_qnodes(n_layers, phi_stage1):\n",
    "    dev3 = make_device(n_qubits)\n",
    "    theta_fixed = pnp.array(phi_stage1, requires_grad=False).reshape((n_layers, n_qubits, 3))\n",
    "\n",
    "    def encoder_fixed_body(theta):\n",
    "        for l in range(n_layers):\n",
    "            for q in range(n_qubits):\n",
    "                qml.RX(theta[l, q, 0], wires=q)\n",
    "                qml.RY(theta[l, q, 1], wires=q)\n",
    "                qml.RZ(theta[l, q, 2], wires=q)\n",
    "            for q in range(n_qubits-1):\n",
    "                qml.CNOT(wires=[q, q+1])\n",
    "            qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "    def decoder_fixed():\n",
    "        qml.adjoint(encoder_fixed_body)(theta_fixed)\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def encoder_only_expZ_all(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def teacher_code_latents(x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_fixed_body(theta_fixed)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def student_code_latents(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def denoiser_qnode_all(flat_params, x_noisy):\n",
    "        embed_input(x_noisy)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        decoder_fixed()\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return dict(\n",
    "        theta_fixed=theta_fixed,\n",
    "        encoder_only_expZ_all=encoder_only_expZ_all,\n",
    "        teacher_code_latents=teacher_code_latents,\n",
    "        student_code_latents=student_code_latents,\n",
    "        denoiser_qnode_all=denoiser_qnode_all\n",
    "    )\n",
    "\n",
    "# ----- value readout helpers (unchanged)\n",
    "def Z_to_values_autograd(z_all):\n",
    "    z_all = pnp.clip(pnp.asarray(z_all), -0.999999, 0.999999)\n",
    "    v01 = pnp.arccos(z_all) / pnp.pi\n",
    "    return v01 * (info[\"scale_high\"] - info[\"scale_low\"]) + info[\"scale_low\"]\n",
    "\n",
    "def first_diff(x): \n",
    "    x = pnp.array(x); return x[1:] - x[:-1]\n",
    "\n",
    "def p1_from_expZ(z): \n",
    "    return (1 - pnp.asarray(z)) * 0.5\n",
    "\n",
    "# ----- deterministic noisy window (shared with eval)\n",
    "def ts_add_noise_window_det(x, sigma, seed):\n",
    "    low, high = float(info[\"scale_low\"]), float(info[\"scale_high\"])\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    noise = rng.normal(0.0, sigma * (high - low), size=np.asarray(x).shape)\n",
    "    return np.clip(np.asarray(x) + noise, low, high)\n",
    "\n",
    "# ----- validation with fixed per-window seeds (unchanged)\n",
    "VAL_BASE_SEED = 12345\n",
    "\n",
    "def stage3_val_values_det(psi, X_clean, sigma=EVAL_SIGMA):\n",
    "    ms_noisy, ms_deno = [], []\n",
    "    for i, c in enumerate(X_clean):\n",
    "        n = ts_add_noise_window_det(c, sigma, seed=VAL_BASE_SEED + i)\n",
    "        zD = np.array(stage3_handles[\"denoiser_qnode_all\"](psi, n))\n",
    "        v_hat = np.array(Z_to_values_autograd(zD))\n",
    "        ms_noisy.append(np.mean((np.asarray(c) - np.asarray(n))**2))\n",
    "        ms_deno.append(np.mean((np.asarray(c) - v_hat)**2))\n",
    "    mN, mD = float(np.mean(ms_noisy)), float(np.mean(ms_deno))\n",
    "    d_pct = 100.0 * (1.0 - mD / max(mN, 1e-12))\n",
    "    return mN, mD, d_pct\n",
    "\n",
    "# ----- small Huber\n",
    "def huber(residual, delta):\n",
    "    r = pnp.abs(residual)\n",
    "    return pnp.where(r <= delta, 0.5*r**2, delta*(r - 0.5*delta))\n",
    "\n",
    "\n",
    "def train_stage3(X_train, X_val, phi_stage1, n_layers, instance_id,\n",
    "                 TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16, \n",
    "                 LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "                 CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99):\n",
    "\n",
    "    # ---- seeds: varied but reproducible across (instance, layers, epoch, window)\n",
    "    def make_train_seed(instance_id, layers, ep, k, view=0):\n",
    "        return (1_000_003 * (instance_id * 10 + layers) + 97 * ep + 31 * int(k) + view) % 2_147_483_647\n",
    "\n",
    "    set_global_seed(instance_id)\n",
    "\n",
    "    global stage3_handles\n",
    "    stage3_handles = stage3_qnodes(n_layers, phi_stage1)\n",
    "    enc_all = stage3_handles[\"encoder_only_expZ_all\"]\n",
    "    teacher_lat = stage3_handles[\"teacher_code_latents\"]\n",
    "    denoise_all = stage3_handles[\"denoiser_qnode_all\"]\n",
    "\n",
    "    # ---- init ψ near φ\n",
    "    phi_flat = pnp.array(phi_stage1, requires_grad=False)\n",
    "    psi = pnp.array(np.array(phi_flat) + 0.05*np.random.randn(len(phi_flat)), requires_grad=True)\n",
    "\n",
    "    # ---- loss weights\n",
    "    ALPHA_REC, BETA_TF, GAMMA_TRASH, L_TV, L_ANCH = 1.0, 0.05, 0.5, 0.05, 2e-4\n",
    "    DELTA_TV, DELTA_Z = 0.02, 0.25\n",
    "\n",
    "    # loss on a single window with a specific noise seed\n",
    "    def loss_on_window_seeded(params, clean_values, seed):\n",
    "        v_noisy = pnp.array(ts_add_noise_window_det(clean_values, TARGET_NOISE, seed=seed))\n",
    "        z_all = pnp.array(enc_all(params, v_noisy))\n",
    "        z_sig, z_tr = z_all[:n_latent], z_all[n_latent:]\n",
    "        zD = pnp.array(denoise_all(params, v_noisy))\n",
    "        v_hat = Z_to_values_autograd(zD)\n",
    "\n",
    "        L_rec = pnp.mean((pnp.array(clean_values) - v_hat)**2)\n",
    "        z_t_sig = pnp.array(teacher_lat(clean_values))\n",
    "        L_tf = pnp.mean(huber(z_t_sig - z_sig, DELTA_Z))\n",
    "        L_tr = pnp.mean(p1_from_expZ(z_tr))\n",
    "        L_tv = pnp.mean(huber(first_diff(clean_values) - first_diff(v_hat), DELTA_TV))\n",
    "        L_anchor = pnp.mean((params - phi_flat)**2)\n",
    "        return (ALPHA_REC*L_rec + BETA_TF*L_tf + GAMMA_TRASH*L_tr + L_TV*L_tv + L_ANCH*L_anchor)\n",
    "\n",
    "    # manual Adam\n",
    "    m = pnp.zeros_like(psi); v = pnp.zeros_like(psi)\n",
    "    b1, b2, eps = 0.9, 0.999, 1e-8\n",
    "    t = 0\n",
    "    def adam_step(params, grad, lr):\n",
    "        nonlocal m, v, t\n",
    "        t += 1\n",
    "        m = b1*m + (1-b1)*grad\n",
    "        v = b2*v + (1-b2)*(grad*grad)\n",
    "        mhat = m/(1-b1**t); vhat = v/(1-b2**t)\n",
    "        return params - lr * (mhat/(pnp.sqrt(vhat)+eps))\n",
    "\n",
    "    # batches deterministic per-epoch\n",
    "    def batch_indices(N, B, ep_seed):\n",
    "        rng = np.random.default_rng(ep_seed)\n",
    "        idx = rng.permutation(N)\n",
    "        for s in range(0, N, B):\n",
    "            yield idx[s:s+B]\n",
    "\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve, lr = 0, LR_START\n",
    "    ema = pnp.array(psi, requires_grad=False) if USE_EMA else None\n",
    "\n",
    "    # history buffers (for CSV/reporting)\n",
    "    hist_train, hist_val = [], []\n",
    "    hist_noisy, hist_delta = [], []\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(MAX_EPOCHS):\n",
    "        seed_ep = 88_000 + 100*instance_id + ep  # reproducible shuffling\n",
    "        acc, nb = 0.0, 0\n",
    "        for ix in batch_indices(len(X_train), BATCH, ep_seed=seed_ep):\n",
    "            for k in ix:                                   # k = absolute index in X_train\n",
    "                c = X_train[k]\n",
    "                seed = make_train_seed(instance_id, n_layers, ep, int(k))\n",
    "                L = loss_on_window_seeded(psi, c, seed)\n",
    "                if not pnp.isfinite(L): \n",
    "                    continue\n",
    "                g = qml.grad(lambda p: loss_on_window_seeded(p, c, seed))(psi)\n",
    "                if not pnp.all(pnp.isfinite(g)): \n",
    "                    continue\n",
    "                # clip\n",
    "                gnorm = pnp.linalg.norm(g) + 1e-12\n",
    "                if gnorm > CLIP_NORM:\n",
    "                    g = g * (CLIP_NORM / gnorm)\n",
    "                psi = adam_step(psi, g, lr)\n",
    "                if USE_EMA: \n",
    "                    ema = EMA_DECAY*ema + (1-EMA_DECAY)*psi\n",
    "                acc += float(L); nb += 1\n",
    "\n",
    "        train_loss = acc / max(nb, 1)\n",
    "        eval_params = ema if USE_EMA else psi\n",
    "\n",
    "        # strict value-domain validation at σ=EVAL_SIGMA (deterministic per window)\n",
    "        mN, mD, dV = stage3_val_values_det(eval_params, X_val, sigma=EVAL_SIGMA)\n",
    "        hist_train.append(train_loss); hist_val.append(mD)\n",
    "        hist_noisy.append(mN);        hist_delta.append(dV)\n",
    "\n",
    "        if mD < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = mD, pnp.array(eval_params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % PLATEAU_STEPS) == 0:\n",
    "                lr *= PLATEAU_FACTOR\n",
    "                print(f\"[Stage3] Plateau → LR {lr:.5f}\")\n",
    "\n",
    "        norm_diff = float(pnp.linalg.norm((eval_params - phi_flat)))\n",
    "        print(f\"[Stage3] L={n_layers} ep {ep:03d} | train {train_loss:.5f} | \"\n",
    "              f\"val {mD:.5f} | noisy {mN:.5f} | Δ {dV:+.1f}% | LR {lr:.5f} | ||ψ-φ|| {norm_diff:.3f}\")\n",
    "\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"[Stage3] Early stopping.\"); break\n",
    "\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    epochs_run = len(hist_val)\n",
    "\n",
    "    psi_best = best_params if best_params is not None else (ema if USE_EMA else psi)\n",
    "\n",
    "    return dict(\n",
    "        psi=psi_best, \n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=int(epochs_run),\n",
    "        hist_train=list(map(float, hist_train)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50129aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows built: 97 (W=4, step=1)\n",
      "Split sizes → train=57, val=20, test=20\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# Cell 6 — Build windows & deterministic train/val/test\n",
    "# ===================================================\n",
    "window_size = n_qubits\n",
    "stride = WINDOW_STRIDE\n",
    "\n",
    "X_windows = np.array([y_all[i:i+window_size] for i in range(0, len(y_all)-window_size+1, stride)], dtype=float)\n",
    "print(f\"Total windows built: {len(X_windows)} (W={window_size}, step={stride})\")\n",
    "\n",
    "# 60/20/20 split (deterministic)\n",
    "X_temp, X_test = train_test_split(X_windows, test_size=0.20, random_state=SPLIT_RANDOM_STATE)\n",
    "X_train, X_val = train_test_split(X_temp,   test_size=0.25, random_state=SPLIT_RANDOM_STATE)  # 0.25 of 0.8 = 0.2\n",
    "print(f\"Split sizes → train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6711326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.479147 | val 0.492094 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.473263 | val 0.488782 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.468699 | val 0.485577 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.466494 | val 0.482410 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.464922 | val 0.479362 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.457612 | val 0.476435 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.458353 | val 0.473671 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.454292 | val 0.471091 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.451598 | val 0.468693 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.450211 | val 0.466493 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.447326 | val 0.464429 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.443890 | val 0.462511 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.440614 | val 0.460709 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.439578 | val 0.458964 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.440212 | val 0.457252 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.434771 | val 0.455547 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.434728 | val 0.453824 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.434366 | val 0.452070 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.430274 | val 0.450289 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.428881 | val 0.448457 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.425030 | val 0.446552 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.424962 | val 0.444509 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.424677 | val 0.442324 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.421255 | val 0.440002 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.418484 | val 0.437544 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.418335 | val 0.434860 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.412932 | val 0.432005 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.412085 | val 0.428932 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.405606 | val 0.425714 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.403592 | val 0.422240 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.398043 | val 0.418568 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.394926 | val 0.414684 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.393446 | val 0.410582 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.390126 | val 0.406279 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.379970 | val 0.401789 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.378746 | val 0.397151 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.374194 | val 0.392322 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.364600 | val 0.387437 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.362854 | val 0.382436 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.358717 | val 0.377430 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.350859 | val 0.372484 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.346503 | val 0.367543 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.344777 | val 0.362619 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.332515 | val 0.357732 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.328754 | val 0.352953 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.325664 | val 0.348217 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.324679 | val 0.343549 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.319464 | val 0.339026 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.309305 | val 0.334684 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.303032 | val 0.330484 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.303430 | val 0.326455 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.304560 | val 0.322588 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.299209 | val 0.318819 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.293190 | val 0.315312 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.296496 | val 0.311865 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.282506 | val 0.308680 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.289949 | val 0.305557 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.291244 | val 0.302632 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.286035 | val 0.299784 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.281796 | val 0.297144 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.278055 | val 0.294723 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.280645 | val 0.292391 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.278982 | val 0.290178 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.279402 | val 0.288084 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.274030 | val 0.286065 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.275940 | val 0.284200 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.278259 | val 0.282442 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.276810 | val 0.280805 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.272982 | val 0.279296 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.270312 | val 0.277768 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.268582 | val 0.276388 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.270995 | val 0.275200 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.272169 | val 0.274011 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.273798 | val 0.272900 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.271319 | val 0.271855 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.264316 | val 0.270913 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.272728 | val 0.270097 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.266816 | val 0.269311 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.269912 | val 0.268617 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.269744 | val 0.267894 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.269014 | val 0.267210 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.268724 | val 0.266701 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.268205 | val 0.266140 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.268049 | val 0.265547 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.274108 | val 0.264985 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.266557 | val 0.264531 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.267837 | val 0.263997 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.270599 | val 0.263529 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.260425 | val 0.263132 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.267741 | val 0.262776 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.272065 | val 0.262408 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.264842 | val 0.262091 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.272857 | val 0.261781 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.262869 | val 0.261558 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.265844 | val 0.261448 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.268526 | val 0.261409 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.269106 | val 0.261366 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.270613 | val 0.261305 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.267217 | val 0.261216 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.261663 | val 0.261087 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.271650 | val 0.261065 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.265790 | val 0.260876 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.268739 | val 0.260788 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.268256 | val 0.260801 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.262814 | val 0.260838 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.269738 | val 0.260830 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.265378 | val 0.260860 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.266842 | val 0.260822 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.265410 | val 0.260744 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.264831 | val 0.260671 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.266868 | val 0.260691 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.268660 | val 0.260727 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.268341 | val 0.260763 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.269540 | val 0.260834 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.270366 | val 0.260919 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.272272 | val 0.260912 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.270060 | val 0.261020 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 117 | train 0.268728 | val 0.261108 | LR 0.00500\n",
      "[Stage1] L=1 ep 118 | train 0.267092 | val 0.261174 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.15009 | val 0.00374 | noisy 0.00391 | Δ +4.3% | LR 0.00300 | ||ψ-φ|| 0.151\n",
      "[Stage3] L=1 ep 001 | train 0.15149 | val 0.00370 | noisy 0.00391 | Δ +5.3% | LR 0.00300 | ||ψ-φ|| 0.185\n",
      "[Stage3] L=1 ep 002 | train 0.15518 | val 0.00362 | noisy 0.00391 | Δ +7.3% | LR 0.00300 | ||ψ-φ|| 0.262\n",
      "[Stage3] L=1 ep 003 | train 0.15623 | val 0.00351 | noisy 0.00391 | Δ +10.3% | LR 0.00300 | ||ψ-φ|| 0.358\n",
      "[Stage3] L=1 ep 004 | train 0.15658 | val 0.00340 | noisy 0.00391 | Δ +12.9% | LR 0.00300 | ||ψ-φ|| 0.460\n",
      "[Stage3] L=1 ep 005 | train 0.15768 | val 0.00331 | noisy 0.00391 | Δ +15.2% | LR 0.00300 | ||ψ-φ|| 0.551\n",
      "[Stage3] L=1 ep 006 | train 0.16656 | val 0.00320 | noisy 0.00391 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 0.642\n",
      "[Stage3] L=1 ep 007 | train 0.15959 | val 0.00310 | noisy 0.00391 | Δ +20.6% | LR 0.00300 | ||ψ-φ|| 0.724\n",
      "[Stage3] L=1 ep 008 | train 0.15723 | val 0.00307 | noisy 0.00391 | Δ +21.5% | LR 0.00300 | ||ψ-φ|| 0.788\n",
      "[Stage3] L=1 ep 009 | train 0.16604 | val 0.00303 | noisy 0.00391 | Δ +22.5% | LR 0.00300 | ||ψ-φ|| 0.829\n",
      "[Stage3] L=1 ep 010 | train 0.16312 | val 0.00301 | noisy 0.00391 | Δ +23.1% | LR 0.00300 | ||ψ-φ|| 0.852\n",
      "[Stage3] L=1 ep 011 | train 0.14941 | val 0.00298 | noisy 0.00391 | Δ +23.6% | LR 0.00300 | ||ψ-φ|| 0.868\n",
      "[Stage3] L=1 ep 012 | train 0.15678 | val 0.00298 | noisy 0.00391 | Δ +23.8% | LR 0.00300 | ||ψ-φ|| 0.877\n",
      "[Stage3] L=1 ep 013 | train 0.16504 | val 0.00298 | noisy 0.00391 | Δ +23.8% | LR 0.00300 | ||ψ-φ|| 0.890\n",
      "[Stage3] L=1 ep 014 | train 0.16000 | val 0.00297 | noisy 0.00391 | Δ +24.0% | LR 0.00300 | ||ψ-φ|| 0.913\n",
      "[Stage3] L=1 ep 015 | train 0.15637 | val 0.00298 | noisy 0.00391 | Δ +23.7% | LR 0.00300 | ||ψ-φ|| 0.927\n",
      "[Stage3] L=1 ep 016 | train 0.15307 | val 0.00298 | noisy 0.00391 | Δ +23.7% | LR 0.00300 | ||ψ-φ|| 0.922\n",
      "[Stage3] L=1 ep 017 | train 0.15399 | val 0.00297 | noisy 0.00391 | Δ +24.0% | LR 0.00300 | ||ψ-φ|| 0.925\n",
      "[Stage3] L=1 ep 018 | train 0.15474 | val 0.00296 | noisy 0.00391 | Δ +24.4% | LR 0.00300 | ||ψ-φ|| 0.924\n",
      "[Stage3] L=1 ep 019 | train 0.15614 | val 0.00294 | noisy 0.00391 | Δ +24.7% | LR 0.00300 | ||ψ-φ|| 0.923\n",
      "[Stage3] L=1 ep 020 | train 0.15588 | val 0.00294 | noisy 0.00391 | Δ +24.8% | LR 0.00300 | ||ψ-φ|| 0.909\n",
      "[Stage3] L=1 ep 021 | train 0.14294 | val 0.00295 | noisy 0.00391 | Δ +24.4% | LR 0.00300 | ||ψ-φ|| 0.894\n",
      "[Stage3] L=1 ep 022 | train 0.15250 | val 0.00297 | noisy 0.00391 | Δ +23.9% | LR 0.00300 | ||ψ-φ|| 0.880\n",
      "[Stage3] L=1 ep 023 | train 0.15267 | val 0.00298 | noisy 0.00391 | Δ +23.8% | LR 0.00300 | ||ψ-φ|| 0.881\n",
      "[Stage3] L=1 ep 024 | train 0.15751 | val 0.00298 | noisy 0.00391 | Δ +23.7% | LR 0.00300 | ||ψ-φ|| 0.888\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 025 | train 0.15962 | val 0.00298 | noisy 0.00391 | Δ +23.8% | LR 0.00150 | ||ψ-φ|| 0.896\n",
      "[Stage3] L=1 ep 026 | train 0.15657 | val 0.00298 | noisy 0.00391 | Δ +23.7% | LR 0.00150 | ||ψ-φ|| 0.908\n",
      "[Stage3] L=1 ep 027 | train 0.15847 | val 0.00298 | noisy 0.00391 | Δ +23.8% | LR 0.00150 | ||ψ-φ|| 0.919\n",
      "[Stage3] L=1 ep 028 | train 0.15053 | val 0.00298 | noisy 0.00391 | Δ +23.7% | LR 0.00150 | ||ψ-φ|| 0.922\n",
      "[Stage3] L=1 ep 029 | train 0.15653 | val 0.00298 | noisy 0.00391 | Δ +23.7% | LR 0.00150 | ||ψ-φ|| 0.921\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 030 | train 0.15128 | val 0.00298 | noisy 0.00391 | Δ +23.7% | LR 0.00075 | ||ψ-φ|| 0.919\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.485874 | val 0.484714 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.483567 | val 0.482679 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.481448 | val 0.480625 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.474767 | val 0.478552 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.473459 | val 0.476425 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.468980 | val 0.474276 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.466052 | val 0.472095 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.462696 | val 0.469905 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.460560 | val 0.467723 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.457289 | val 0.465567 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.452208 | val 0.463435 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.446409 | val 0.461323 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.444425 | val 0.459233 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.441588 | val 0.457157 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.438180 | val 0.455122 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.436281 | val 0.453106 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.434087 | val 0.451119 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.431471 | val 0.449157 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.427042 | val 0.447255 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.430149 | val 0.445371 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.421268 | val 0.443543 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.421005 | val 0.441674 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.416975 | val 0.439809 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.417451 | val 0.437926 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.414355 | val 0.436022 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.412211 | val 0.434081 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.413130 | val 0.432055 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.405323 | val 0.430045 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.409648 | val 0.427927 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.404993 | val 0.425814 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.402188 | val 0.423630 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.396250 | val 0.421367 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.400254 | val 0.419005 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.398183 | val 0.416611 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.393212 | val 0.414228 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.395642 | val 0.411767 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.388555 | val 0.409266 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.391993 | val 0.406743 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.385331 | val 0.404185 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.386836 | val 0.401620 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.387871 | val 0.399023 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.382073 | val 0.396467 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.383130 | val 0.393891 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.381998 | val 0.391307 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.373837 | val 0.388732 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.373058 | val 0.386219 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.375212 | val 0.383662 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.374775 | val 0.381099 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.367430 | val 0.378599 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.367764 | val 0.376051 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.357861 | val 0.373627 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.370338 | val 0.371291 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.362392 | val 0.368878 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.367283 | val 0.366594 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.360103 | val 0.364261 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.364943 | val 0.362032 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.362213 | val 0.359798 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.364844 | val 0.357567 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.354837 | val 0.355365 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.353434 | val 0.353324 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.351853 | val 0.351196 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.353907 | val 0.349156 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.353056 | val 0.347216 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.345266 | val 0.345272 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.354741 | val 0.343245 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.346573 | val 0.341208 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.345891 | val 0.339246 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.345358 | val 0.337274 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.345236 | val 0.335352 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.346477 | val 0.333478 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.345781 | val 0.331596 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.349110 | val 0.329678 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.342503 | val 0.327837 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.341634 | val 0.325822 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.339664 | val 0.323943 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.335608 | val 0.322041 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.341492 | val 0.320085 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.336087 | val 0.318230 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.336533 | val 0.316352 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.334556 | val 0.314470 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.336622 | val 0.312582 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.335497 | val 0.310720 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.332267 | val 0.308871 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.336299 | val 0.307019 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.328958 | val 0.305121 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.330115 | val 0.303412 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.330662 | val 0.301600 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.330481 | val 0.299909 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.326920 | val 0.298039 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.328473 | val 0.296193 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.327719 | val 0.294373 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.321251 | val 0.292702 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.323541 | val 0.291000 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.318466 | val 0.289226 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.322273 | val 0.287458 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.318636 | val 0.285724 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.313804 | val 0.283941 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.315729 | val 0.282237 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.315681 | val 0.280588 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.310767 | val 0.278856 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.313609 | val 0.277125 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.309186 | val 0.275513 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.314325 | val 0.273831 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.308289 | val 0.272250 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.305899 | val 0.270638 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.303519 | val 0.269082 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.298565 | val 0.267680 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.299576 | val 0.266311 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.297371 | val 0.265015 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.300741 | val 0.263826 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.294259 | val 0.262635 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.294837 | val 0.261525 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.286847 | val 0.260534 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.289917 | val 0.259674 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.288924 | val 0.258879 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.286146 | val 0.258255 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.283029 | val 0.257636 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.283621 | val 0.257047 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.283760 | val 0.256516 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.280338 | val 0.256247 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.16652 | val 0.00368 | noisy 0.00391 | Δ +5.9% | LR 0.00300 | ||ψ-φ|| 0.158\n",
      "[Stage3] L=1 ep 001 | train 0.14964 | val 0.00365 | noisy 0.00391 | Δ +6.5% | LR 0.00300 | ||ψ-φ|| 0.204\n",
      "[Stage3] L=1 ep 002 | train 0.16300 | val 0.00363 | noisy 0.00391 | Δ +7.2% | LR 0.00300 | ||ψ-φ|| 0.274\n",
      "[Stage3] L=1 ep 003 | train 0.15421 | val 0.00361 | noisy 0.00391 | Δ +7.6% | LR 0.00300 | ||ψ-φ|| 0.358\n",
      "[Stage3] L=1 ep 004 | train 0.16460 | val 0.00354 | noisy 0.00391 | Δ +9.4% | LR 0.00300 | ||ψ-φ|| 0.454\n",
      "[Stage3] L=1 ep 005 | train 0.16045 | val 0.00353 | noisy 0.00391 | Δ +9.6% | LR 0.00300 | ||ψ-φ|| 0.553\n",
      "[Stage3] L=1 ep 006 | train 0.16910 | val 0.00348 | noisy 0.00391 | Δ +10.9% | LR 0.00300 | ||ψ-φ|| 0.653\n",
      "[Stage3] L=1 ep 007 | train 0.15879 | val 0.00350 | noisy 0.00391 | Δ +10.4% | LR 0.00300 | ||ψ-φ|| 0.738\n",
      "[Stage3] L=1 ep 008 | train 0.15936 | val 0.00350 | noisy 0.00391 | Δ +10.4% | LR 0.00300 | ||ψ-φ|| 0.795\n",
      "[Stage3] L=1 ep 009 | train 0.14888 | val 0.00354 | noisy 0.00391 | Δ +9.5% | LR 0.00300 | ||ψ-φ|| 0.847\n",
      "[Stage3] L=1 ep 010 | train 0.15541 | val 0.00355 | noisy 0.00391 | Δ +9.3% | LR 0.00300 | ||ψ-φ|| 0.899\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 011 | train 0.15886 | val 0.00357 | noisy 0.00391 | Δ +8.6% | LR 0.00150 | ||ψ-φ|| 0.943\n",
      "[Stage3] L=1 ep 012 | train 0.16897 | val 0.00358 | noisy 0.00391 | Δ +8.3% | LR 0.00150 | ||ψ-φ|| 0.982\n",
      "[Stage3] L=1 ep 013 | train 0.15495 | val 0.00358 | noisy 0.00391 | Δ +8.4% | LR 0.00150 | ||ψ-φ|| 1.006\n",
      "[Stage3] L=1 ep 014 | train 0.15843 | val 0.00359 | noisy 0.00391 | Δ +8.1% | LR 0.00150 | ||ψ-φ|| 1.023\n",
      "[Stage3] L=1 ep 015 | train 0.15689 | val 0.00359 | noisy 0.00391 | Δ +8.0% | LR 0.00150 | ||ψ-φ|| 1.033\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 016 | train 0.16359 | val 0.00359 | noisy 0.00391 | Δ +8.1% | LR 0.00075 | ||ψ-φ|| 1.042\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.486891 | val 0.493655 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.484963 | val 0.492341 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.483242 | val 0.491073 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.479633 | val 0.489775 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.477617 | val 0.488408 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.473628 | val 0.486940 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.473181 | val 0.485358 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.469787 | val 0.483580 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.465985 | val 0.481621 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.464308 | val 0.479383 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.461561 | val 0.476905 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.458713 | val 0.474076 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.453166 | val 0.470962 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.452899 | val 0.467387 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.446551 | val 0.463382 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.442013 | val 0.458910 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.437349 | val 0.453997 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.432645 | val 0.448561 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.427260 | val 0.442668 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.422855 | val 0.436320 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.415585 | val 0.429564 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.412403 | val 0.422405 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.402543 | val 0.414999 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.394063 | val 0.407339 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.388646 | val 0.399474 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.383075 | val 0.391558 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.374667 | val 0.383640 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.369929 | val 0.375804 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.359172 | val 0.368072 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.356400 | val 0.360513 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.347492 | val 0.353248 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.342578 | val 0.346210 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.338693 | val 0.339526 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.332454 | val 0.333154 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.326018 | val 0.327230 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.322904 | val 0.321652 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.323698 | val 0.316520 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.315928 | val 0.311777 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.305000 | val 0.307489 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.305450 | val 0.303576 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.300487 | val 0.300034 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.295743 | val 0.296805 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.295307 | val 0.293964 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.299235 | val 0.291402 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.294448 | val 0.289087 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.291041 | val 0.287040 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.294886 | val 0.285182 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.292085 | val 0.283407 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.290285 | val 0.281846 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.289670 | val 0.280446 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.288961 | val 0.279091 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.287689 | val 0.277764 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.288511 | val 0.276569 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.285936 | val 0.275439 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.282626 | val 0.274345 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.277758 | val 0.273306 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.280183 | val 0.272304 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.277215 | val 0.271285 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.280499 | val 0.270368 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.279392 | val 0.269418 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.285217 | val 0.268529 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.279125 | val 0.267701 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.279241 | val 0.266831 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.275002 | val 0.266029 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.277065 | val 0.265214 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.276317 | val 0.264524 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.275653 | val 0.263712 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.273215 | val 0.263086 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.274830 | val 0.262387 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.271039 | val 0.261768 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.273163 | val 0.261211 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.268084 | val 0.260673 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.273229 | val 0.260196 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.276298 | val 0.259812 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.278211 | val 0.259451 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.268872 | val 0.259144 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.271032 | val 0.258916 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.267469 | val 0.258663 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.265828 | val 0.258467 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.268272 | val 0.258303 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.267685 | val 0.258206 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.271771 | val 0.258139 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.270485 | val 0.258154 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.270275 | val 0.258212 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.271496 | val 0.258293 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.261546 | val 0.258373 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.270168 | val 0.258391 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.272405 | val 0.258438 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.269773 | val 0.258628 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 089 | train 0.268935 | val 0.258714 | LR 0.00500\n",
      "[Stage1] L=1 ep 090 | train 0.268970 | val 0.258861 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.15425 | val 0.00372 | noisy 0.00391 | Δ +4.9% | LR 0.00300 | ||ψ-φ|| 0.177\n",
      "[Stage3] L=1 ep 001 | train 0.15798 | val 0.00368 | noisy 0.00391 | Δ +5.7% | LR 0.00300 | ||ψ-φ|| 0.205\n",
      "[Stage3] L=1 ep 002 | train 0.15293 | val 0.00361 | noisy 0.00391 | Δ +7.6% | LR 0.00300 | ||ψ-φ|| 0.265\n",
      "[Stage3] L=1 ep 003 | train 0.15471 | val 0.00351 | noisy 0.00391 | Δ +10.3% | LR 0.00300 | ||ψ-φ|| 0.349\n",
      "[Stage3] L=1 ep 004 | train 0.15862 | val 0.00342 | noisy 0.00391 | Δ +12.5% | LR 0.00300 | ||ψ-φ|| 0.432\n",
      "[Stage3] L=1 ep 005 | train 0.16018 | val 0.00333 | noisy 0.00391 | Δ +14.7% | LR 0.00300 | ||ψ-φ|| 0.512\n",
      "[Stage3] L=1 ep 006 | train 0.15679 | val 0.00326 | noisy 0.00391 | Δ +16.7% | LR 0.00300 | ||ψ-φ|| 0.596\n",
      "[Stage3] L=1 ep 007 | train 0.16083 | val 0.00317 | noisy 0.00391 | Δ +18.8% | LR 0.00300 | ||ψ-φ|| 0.675\n",
      "[Stage3] L=1 ep 008 | train 0.15975 | val 0.00313 | noisy 0.00391 | Δ +20.0% | LR 0.00300 | ||ψ-φ|| 0.741\n",
      "[Stage3] L=1 ep 009 | train 0.15392 | val 0.00312 | noisy 0.00391 | Δ +20.3% | LR 0.00300 | ||ψ-φ|| 0.791\n",
      "[Stage3] L=1 ep 010 | train 0.15652 | val 0.00309 | noisy 0.00391 | Δ +20.8% | LR 0.00300 | ||ψ-φ|| 0.852\n",
      "[Stage3] L=1 ep 011 | train 0.15672 | val 0.00307 | noisy 0.00391 | Δ +21.4% | LR 0.00300 | ||ψ-φ|| 0.911\n",
      "[Stage3] L=1 ep 012 | train 0.15902 | val 0.00306 | noisy 0.00391 | Δ +21.8% | LR 0.00300 | ||ψ-φ|| 0.946\n",
      "[Stage3] L=1 ep 013 | train 0.15323 | val 0.00306 | noisy 0.00391 | Δ +21.8% | LR 0.00300 | ||ψ-φ|| 0.976\n",
      "[Stage3] L=1 ep 014 | train 0.15404 | val 0.00304 | noisy 0.00391 | Δ +22.3% | LR 0.00300 | ||ψ-φ|| 1.002\n",
      "[Stage3] L=1 ep 015 | train 0.15585 | val 0.00302 | noisy 0.00391 | Δ +22.6% | LR 0.00300 | ||ψ-φ|| 1.014\n",
      "[Stage3] L=1 ep 016 | train 0.15404 | val 0.00302 | noisy 0.00391 | Δ +22.8% | LR 0.00300 | ||ψ-φ|| 1.017\n",
      "[Stage3] L=1 ep 017 | train 0.15278 | val 0.00303 | noisy 0.00391 | Δ +22.4% | LR 0.00300 | ||ψ-φ|| 1.026\n",
      "[Stage3] L=1 ep 018 | train 0.15811 | val 0.00304 | noisy 0.00391 | Δ +22.2% | LR 0.00300 | ||ψ-φ|| 1.036\n",
      "[Stage3] L=1 ep 019 | train 0.15599 | val 0.00304 | noisy 0.00391 | Δ +22.2% | LR 0.00300 | ||ψ-φ|| 1.058\n",
      "[Stage3] L=1 ep 020 | train 0.15441 | val 0.00304 | noisy 0.00391 | Δ +22.3% | LR 0.00300 | ||ψ-φ|| 1.066\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 021 | train 0.16003 | val 0.00303 | noisy 0.00391 | Δ +22.5% | LR 0.00150 | ||ψ-φ|| 1.074\n",
      "[Stage3] L=1 ep 022 | train 0.15260 | val 0.00302 | noisy 0.00391 | Δ +22.7% | LR 0.00150 | ||ψ-φ|| 1.072\n",
      "[Stage3] L=1 ep 023 | train 0.16188 | val 0.00302 | noisy 0.00391 | Δ +22.7% | LR 0.00150 | ||ψ-φ|| 1.069\n",
      "[Stage3] L=1 ep 024 | train 0.15288 | val 0.00302 | noisy 0.00391 | Δ +22.7% | LR 0.00150 | ||ψ-φ|| 1.068\n",
      "[Stage3] L=1 ep 025 | train 0.15458 | val 0.00301 | noisy 0.00391 | Δ +22.9% | LR 0.00150 | ||ψ-φ|| 1.062\n",
      "[Stage3] L=1 ep 026 | train 0.15217 | val 0.00300 | noisy 0.00391 | Δ +23.3% | LR 0.00150 | ||ψ-φ|| 1.059\n",
      "[Stage3] L=1 ep 027 | train 0.15263 | val 0.00298 | noisy 0.00391 | Δ +23.6% | LR 0.00150 | ||ψ-φ|| 1.054\n",
      "[Stage3] L=1 ep 028 | train 0.15445 | val 0.00298 | noisy 0.00391 | Δ +23.8% | LR 0.00150 | ||ψ-φ|| 1.048\n",
      "[Stage3] L=1 ep 029 | train 0.16461 | val 0.00298 | noisy 0.00391 | Δ +23.6% | LR 0.00150 | ||ψ-φ|| 1.048\n",
      "[Stage3] L=1 ep 030 | train 0.16213 | val 0.00298 | noisy 0.00391 | Δ +23.6% | LR 0.00150 | ||ψ-φ|| 1.050\n",
      "[Stage3] L=1 ep 031 | train 0.14820 | val 0.00300 | noisy 0.00391 | Δ +23.3% | LR 0.00150 | ||ψ-φ|| 1.050\n",
      "[Stage3] L=1 ep 032 | train 0.15201 | val 0.00300 | noisy 0.00391 | Δ +23.2% | LR 0.00150 | ||ψ-φ|| 1.048\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 033 | train 0.15671 | val 0.00301 | noisy 0.00391 | Δ +23.1% | LR 0.00075 | ||ψ-φ|| 1.046\n",
      "[Stage3] L=1 ep 034 | train 0.15155 | val 0.00301 | noisy 0.00391 | Δ +23.1% | LR 0.00075 | ||ψ-φ|| 1.042\n",
      "[Stage3] L=1 ep 035 | train 0.15704 | val 0.00301 | noisy 0.00391 | Δ +23.0% | LR 0.00075 | ||ψ-φ|| 1.039\n",
      "[Stage3] L=1 ep 036 | train 0.15283 | val 0.00300 | noisy 0.00391 | Δ +23.1% | LR 0.00075 | ||ψ-φ|| 1.036\n",
      "[Stage3] L=1 ep 037 | train 0.14662 | val 0.00300 | noisy 0.00391 | Δ +23.2% | LR 0.00075 | ||ψ-φ|| 1.035\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=1 ep 038 | train 0.15362 | val 0.00299 | noisy 0.00391 | Δ +23.4% | LR 0.00038 | ||ψ-φ|| 1.033\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.507786 | val 0.515945 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.504381 | val 0.512373 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.500945 | val 0.508959 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.498073 | val 0.505775 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.495523 | val 0.502760 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.493175 | val 0.499908 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.491255 | val 0.497152 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.487558 | val 0.494489 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.486195 | val 0.491910 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.483194 | val 0.489408 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.480226 | val 0.486977 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.478052 | val 0.484616 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.476163 | val 0.482321 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.472056 | val 0.480110 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.471627 | val 0.477962 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.469556 | val 0.475903 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.465274 | val 0.473945 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.463918 | val 0.472090 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.463150 | val 0.470343 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.460284 | val 0.468706 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.458210 | val 0.467172 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.456286 | val 0.465732 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.453777 | val 0.464388 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.450006 | val 0.463153 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.450164 | val 0.461991 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.448080 | val 0.460893 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.448506 | val 0.459857 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.444962 | val 0.458876 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.445081 | val 0.457930 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.442867 | val 0.457019 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.439444 | val 0.456163 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.438886 | val 0.455321 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.436675 | val 0.454495 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.437930 | val 0.453691 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.432905 | val 0.452892 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.434199 | val 0.452104 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.434436 | val 0.451309 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.429885 | val 0.450553 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.431026 | val 0.449757 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.432073 | val 0.448959 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.431417 | val 0.448148 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.427239 | val 0.447313 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.425540 | val 0.446448 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.422914 | val 0.445485 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.422143 | val 0.444462 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.422189 | val 0.443352 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.418015 | val 0.442208 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.415643 | val 0.440949 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.417641 | val 0.439544 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.415701 | val 0.438075 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.411305 | val 0.436394 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.415022 | val 0.434518 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.412513 | val 0.432606 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.405116 | val 0.430638 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.409353 | val 0.428476 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.406507 | val 0.426161 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.404540 | val 0.423781 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.402658 | val 0.421232 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.399398 | val 0.418526 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.394082 | val 0.415718 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.392950 | val 0.412784 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.393541 | val 0.409775 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.387331 | val 0.406694 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.384891 | val 0.403531 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.384251 | val 0.400444 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.376328 | val 0.397270 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.377289 | val 0.394050 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.375615 | val 0.390789 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.376105 | val 0.387563 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.371715 | val 0.384322 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.362993 | val 0.381062 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.365176 | val 0.377984 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.368611 | val 0.374952 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.362630 | val 0.371881 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.365327 | val 0.368885 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.359636 | val 0.365957 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.355585 | val 0.363052 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.358813 | val 0.360149 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.353323 | val 0.357360 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.355463 | val 0.354528 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.352403 | val 0.351907 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.354934 | val 0.349276 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.351031 | val 0.346679 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.355819 | val 0.344165 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.349231 | val 0.341643 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.348560 | val 0.339088 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.347937 | val 0.336715 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.343223 | val 0.334240 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.346822 | val 0.331852 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.340831 | val 0.329582 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.344814 | val 0.327369 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.342979 | val 0.325146 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.342258 | val 0.322889 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.345015 | val 0.320682 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.339771 | val 0.318456 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.345859 | val 0.316240 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.332348 | val 0.314059 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.338247 | val 0.311944 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.332964 | val 0.309916 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.335826 | val 0.307745 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.331965 | val 0.305538 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.328660 | val 0.303467 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.330973 | val 0.301367 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.331203 | val 0.299222 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.330306 | val 0.297076 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.325492 | val 0.294994 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.327139 | val 0.292942 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.321349 | val 0.290897 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.319882 | val 0.288749 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.323369 | val 0.286752 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.316804 | val 0.284704 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.315517 | val 0.282679 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.312473 | val 0.280621 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.312567 | val 0.278637 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.314718 | val 0.276609 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.309000 | val 0.274804 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.306236 | val 0.273011 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.304776 | val 0.271267 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.302387 | val 0.269594 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.304167 | val 0.268028 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.17506 | val 0.00382 | noisy 0.00391 | Δ +2.2% | LR 0.00300 | ||ψ-φ|| 0.176\n",
      "[Stage3] L=1 ep 001 | train 0.16688 | val 0.00370 | noisy 0.00391 | Δ +5.4% | LR 0.00300 | ||ψ-φ|| 0.208\n",
      "[Stage3] L=1 ep 002 | train 0.17016 | val 0.00355 | noisy 0.00391 | Δ +9.1% | LR 0.00300 | ||ψ-φ|| 0.261\n",
      "[Stage3] L=1 ep 003 | train 0.17186 | val 0.00348 | noisy 0.00391 | Δ +10.9% | LR 0.00300 | ||ψ-φ|| 0.321\n",
      "[Stage3] L=1 ep 004 | train 0.16749 | val 0.00349 | noisy 0.00391 | Δ +10.6% | LR 0.00300 | ||ψ-φ|| 0.384\n",
      "[Stage3] L=1 ep 005 | train 0.15879 | val 0.00357 | noisy 0.00391 | Δ +8.7% | LR 0.00300 | ||ψ-φ|| 0.435\n",
      "[Stage3] L=1 ep 006 | train 0.16181 | val 0.00364 | noisy 0.00391 | Δ +6.7% | LR 0.00300 | ||ψ-φ|| 0.479\n",
      "[Stage3] L=1 ep 007 | train 0.16506 | val 0.00377 | noisy 0.00391 | Δ +3.6% | LR 0.00300 | ||ψ-φ|| 0.532\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 008 | train 0.16750 | val 0.00386 | noisy 0.00391 | Δ +1.2% | LR 0.00150 | ||ψ-φ|| 0.588\n",
      "[Stage3] L=1 ep 009 | train 0.16741 | val 0.00397 | noisy 0.00391 | Δ -1.7% | LR 0.00150 | ||ψ-φ|| 0.647\n",
      "[Stage3] L=1 ep 010 | train 0.16769 | val 0.00407 | noisy 0.00391 | Δ -4.0% | LR 0.00150 | ||ψ-φ|| 0.697\n",
      "[Stage3] L=1 ep 011 | train 0.15777 | val 0.00416 | noisy 0.00391 | Δ -6.4% | LR 0.00150 | ||ψ-φ|| 0.730\n",
      "[Stage3] L=1 ep 012 | train 0.17352 | val 0.00427 | noisy 0.00391 | Δ -9.2% | LR 0.00150 | ||ψ-φ|| 0.764\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 013 | train 0.17045 | val 0.00437 | noisy 0.00391 | Δ -11.9% | LR 0.00075 | ||ψ-φ|| 0.804\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.467295 | val 0.476753 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.466858 | val 0.474848 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.462268 | val 0.472914 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.460698 | val 0.470900 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.457582 | val 0.468814 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.454522 | val 0.466645 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.450960 | val 0.464414 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.449451 | val 0.462117 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.443184 | val 0.459787 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.446084 | val 0.457354 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.440910 | val 0.454916 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.434500 | val 0.452386 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.434835 | val 0.449768 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.427730 | val 0.447044 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.421359 | val 0.444283 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.424737 | val 0.441355 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.419887 | val 0.438349 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.416226 | val 0.435270 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.414755 | val 0.432104 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.412307 | val 0.428822 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.411184 | val 0.425532 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.403138 | val 0.422166 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.400438 | val 0.418759 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.397033 | val 0.415291 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.393125 | val 0.411870 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.393683 | val 0.408361 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.392358 | val 0.404855 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.387718 | val 0.401306 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.386400 | val 0.397745 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.377808 | val 0.394237 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.377565 | val 0.390707 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.378492 | val 0.387181 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.378751 | val 0.383704 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.371538 | val 0.380286 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.368000 | val 0.376888 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.370172 | val 0.373605 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.364479 | val 0.370317 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.363718 | val 0.367072 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.360985 | val 0.363997 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.360309 | val 0.361009 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.365326 | val 0.358007 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.359469 | val 0.355075 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.353485 | val 0.352153 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.357140 | val 0.349333 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.355653 | val 0.346581 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.350591 | val 0.343810 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.349890 | val 0.341170 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.346043 | val 0.338609 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.347994 | val 0.336164 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.349534 | val 0.333766 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.344414 | val 0.331391 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.346877 | val 0.329048 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.343084 | val 0.326686 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.337366 | val 0.324255 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.339425 | val 0.321935 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.340500 | val 0.319637 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.342007 | val 0.317302 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.333904 | val 0.315021 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.339911 | val 0.312952 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.336144 | val 0.310804 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.333662 | val 0.308724 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.334808 | val 0.306721 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.335356 | val 0.304651 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.334316 | val 0.302555 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.331361 | val 0.300419 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.330584 | val 0.298389 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.327794 | val 0.296338 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.327630 | val 0.294318 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.321628 | val 0.292220 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.318636 | val 0.290244 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.321304 | val 0.288274 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.319092 | val 0.286261 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.321033 | val 0.284383 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.318413 | val 0.282514 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.315674 | val 0.280714 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.312569 | val 0.278937 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.311156 | val 0.277210 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.310872 | val 0.275415 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.305191 | val 0.273821 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.306778 | val 0.272275 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.306717 | val 0.270716 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.303344 | val 0.269248 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.300641 | val 0.267895 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.297189 | val 0.266597 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.296844 | val 0.265368 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.296921 | val 0.264265 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.294590 | val 0.263250 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.294166 | val 0.262342 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.291035 | val 0.261508 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.292197 | val 0.260807 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.289467 | val 0.260204 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.287297 | val 0.259615 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.280003 | val 0.259168 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.276297 | val 0.258777 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.279305 | val 0.258527 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.278090 | val 0.258343 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.272805 | val 0.258146 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.274235 | val 0.257934 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.275169 | val 0.257812 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.276308 | val 0.257793 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.272269 | val 0.257712 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.276618 | val 0.257686 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.272669 | val 0.257863 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.274019 | val 0.257912 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.271312 | val 0.258034 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.270691 | val 0.258200 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.271877 | val 0.258303 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.272794 | val 0.258467 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.269948 | val 0.258540 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 109 | train 0.273623 | val 0.258723 | LR 0.00500\n",
      "[Stage1] L=1 ep 110 | train 0.267480 | val 0.257921 | LR 0.00500\n",
      "[Stage1] L=1 ep 111 | train 0.272931 | val 0.257657 | LR 0.00500\n",
      "[Stage1] L=1 ep 112 | train 0.262112 | val 0.257599 | LR 0.00500\n",
      "[Stage1] L=1 ep 113 | train 0.263819 | val 0.257622 | LR 0.00500\n",
      "[Stage1] L=1 ep 114 | train 0.263823 | val 0.257926 | LR 0.00500\n",
      "[Stage1] L=1 ep 115 | train 0.267611 | val 0.258005 | LR 0.00500\n",
      "[Stage1] L=1 ep 116 | train 0.271916 | val 0.258122 | LR 0.00500\n",
      "[Stage1] L=1 ep 117 | train 0.270120 | val 0.258310 | LR 0.00500\n",
      "[Stage1] L=1 ep 118 | train 0.275185 | val 0.258432 | LR 0.00500\n",
      "[Stage1] L=1 ep 119 | train 0.269655 | val 0.258656 | LR 0.00500\n",
      "[Stage3] L=1 ep 000 | train 0.16155 | val 0.00380 | noisy 0.00391 | Δ +2.7% | LR 0.00300 | ||ψ-φ|| 0.142\n",
      "[Stage3] L=1 ep 001 | train 0.16425 | val 0.00371 | noisy 0.00391 | Δ +5.2% | LR 0.00300 | ||ψ-φ|| 0.182\n",
      "[Stage3] L=1 ep 002 | train 0.15430 | val 0.00362 | noisy 0.00391 | Δ +7.5% | LR 0.00300 | ||ψ-φ|| 0.261\n",
      "[Stage3] L=1 ep 003 | train 0.15437 | val 0.00352 | noisy 0.00391 | Δ +9.9% | LR 0.00300 | ||ψ-φ|| 0.350\n",
      "[Stage3] L=1 ep 004 | train 0.16785 | val 0.00342 | noisy 0.00391 | Δ +12.6% | LR 0.00300 | ||ψ-φ|| 0.437\n",
      "[Stage3] L=1 ep 005 | train 0.15018 | val 0.00332 | noisy 0.00391 | Δ +14.9% | LR 0.00300 | ||ψ-φ|| 0.515\n",
      "[Stage3] L=1 ep 006 | train 0.15375 | val 0.00323 | noisy 0.00391 | Δ +17.3% | LR 0.00300 | ||ψ-φ|| 0.585\n",
      "[Stage3] L=1 ep 007 | train 0.15815 | val 0.00316 | noisy 0.00391 | Δ +19.1% | LR 0.00300 | ||ψ-φ|| 0.653\n",
      "[Stage3] L=1 ep 008 | train 0.16227 | val 0.00312 | noisy 0.00391 | Δ +20.2% | LR 0.00300 | ||ψ-φ|| 0.717\n",
      "[Stage3] L=1 ep 009 | train 0.15957 | val 0.00309 | noisy 0.00391 | Δ +21.0% | LR 0.00300 | ||ψ-φ|| 0.770\n",
      "[Stage3] L=1 ep 010 | train 0.15517 | val 0.00306 | noisy 0.00391 | Δ +21.8% | LR 0.00300 | ||ψ-φ|| 0.803\n",
      "[Stage3] L=1 ep 011 | train 0.15773 | val 0.00303 | noisy 0.00391 | Δ +22.6% | LR 0.00300 | ||ψ-φ|| 0.832\n",
      "[Stage3] L=1 ep 012 | train 0.15829 | val 0.00302 | noisy 0.00391 | Δ +22.6% | LR 0.00300 | ||ψ-φ|| 0.850\n",
      "[Stage3] L=1 ep 013 | train 0.15297 | val 0.00301 | noisy 0.00391 | Δ +22.9% | LR 0.00300 | ||ψ-φ|| 0.873\n",
      "[Stage3] L=1 ep 014 | train 0.15655 | val 0.00300 | noisy 0.00391 | Δ +23.2% | LR 0.00300 | ||ψ-φ|| 0.889\n",
      "[Stage3] L=1 ep 015 | train 0.15441 | val 0.00298 | noisy 0.00391 | Δ +23.8% | LR 0.00300 | ||ψ-φ|| 0.909\n",
      "[Stage3] L=1 ep 016 | train 0.15053 | val 0.00298 | noisy 0.00391 | Δ +23.6% | LR 0.00300 | ||ψ-φ|| 0.926\n",
      "[Stage3] L=1 ep 017 | train 0.15931 | val 0.00301 | noisy 0.00391 | Δ +23.0% | LR 0.00300 | ||ψ-φ|| 0.935\n",
      "[Stage3] L=1 ep 018 | train 0.15721 | val 0.00300 | noisy 0.00391 | Δ +23.3% | LR 0.00300 | ||ψ-φ|| 0.939\n",
      "[Stage3] L=1 ep 019 | train 0.15440 | val 0.00301 | noisy 0.00391 | Δ +22.9% | LR 0.00300 | ||ψ-φ|| 0.945\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 020 | train 0.15799 | val 0.00299 | noisy 0.00391 | Δ +23.4% | LR 0.00150 | ||ψ-φ|| 0.951\n",
      "[Stage3] L=1 ep 021 | train 0.14771 | val 0.00299 | noisy 0.00391 | Δ +23.5% | LR 0.00150 | ||ψ-φ|| 0.957\n",
      "[Stage3] L=1 ep 022 | train 0.15034 | val 0.00299 | noisy 0.00391 | Δ +23.5% | LR 0.00150 | ||ψ-φ|| 0.958\n",
      "[Stage3] L=1 ep 023 | train 0.15792 | val 0.00299 | noisy 0.00391 | Δ +23.4% | LR 0.00150 | ||ψ-φ|| 0.960\n",
      "[Stage3] L=1 ep 024 | train 0.14795 | val 0.00299 | noisy 0.00391 | Δ +23.4% | LR 0.00150 | ||ψ-φ|| 0.955\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 025 | train 0.15976 | val 0.00300 | noisy 0.00391 | Δ +23.3% | LR 0.00075 | ||ψ-φ|| 0.949\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 1 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.417260 | val 0.392836 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.390094 | val 0.367409 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.369817 | val 0.343355 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.354018 | val 0.321270 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.327517 | val 0.301308 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.315119 | val 0.283415 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.296230 | val 0.267629 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.282944 | val 0.253534 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.273295 | val 0.240793 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.261640 | val 0.229066 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.252466 | val 0.218226 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.240786 | val 0.207984 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.233676 | val 0.198350 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.224441 | val 0.189331 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.213139 | val 0.180916 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.208825 | val 0.172986 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.203054 | val 0.165629 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.193676 | val 0.158623 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.186827 | val 0.151984 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.178828 | val 0.145639 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.173794 | val 0.139806 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.170527 | val 0.134381 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.161689 | val 0.129191 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.155816 | val 0.124346 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.148841 | val 0.120160 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.146691 | val 0.116184 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.143426 | val 0.112686 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.140156 | val 0.109647 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.133316 | val 0.106920 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.132817 | val 0.104465 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.129164 | val 0.102443 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.124971 | val 0.100723 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.122093 | val 0.099131 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.118240 | val 0.097814 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.116741 | val 0.096559 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.113904 | val 0.095515 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.113318 | val 0.094618 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.111173 | val 0.093722 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.111119 | val 0.092970 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.108864 | val 0.092176 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.106076 | val 0.091548 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.103000 | val 0.090718 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.101732 | val 0.089800 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.100607 | val 0.088915 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.097760 | val 0.088104 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.097572 | val 0.087067 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.095398 | val 0.085830 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.095407 | val 0.084702 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.091718 | val 0.083304 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.090815 | val 0.082035 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.089345 | val 0.080806 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.086000 | val 0.079554 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.087156 | val 0.078204 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.083278 | val 0.076798 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.082393 | val 0.075291 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.080411 | val 0.073763 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.077617 | val 0.072146 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.077436 | val 0.070708 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.075892 | val 0.069263 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.075212 | val 0.067936 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.072620 | val 0.066688 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.073126 | val 0.065480 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.069766 | val 0.064352 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.070320 | val 0.063236 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.068004 | val 0.062053 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.068682 | val 0.061016 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.066614 | val 0.059990 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.066296 | val 0.059067 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.063247 | val 0.058179 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.064193 | val 0.057210 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.062738 | val 0.056345 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.062780 | val 0.055651 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.060546 | val 0.054893 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.060711 | val 0.054149 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.060068 | val 0.053453 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.060118 | val 0.052818 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.058469 | val 0.052256 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.056308 | val 0.051660 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.057793 | val 0.051020 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.057727 | val 0.050299 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.057351 | val 0.049618 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.056955 | val 0.049030 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.055672 | val 0.048333 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.054655 | val 0.047595 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.054780 | val 0.046876 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.053344 | val 0.046251 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.054033 | val 0.045495 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.052343 | val 0.044767 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.050831 | val 0.044084 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.050945 | val 0.043388 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.052839 | val 0.042620 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.049821 | val 0.041924 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.050630 | val 0.041189 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.048812 | val 0.040509 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.048528 | val 0.039815 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.048253 | val 0.039121 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.047690 | val 0.038367 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.046876 | val 0.037550 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.045719 | val 0.036685 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.044460 | val 0.035794 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.042902 | val 0.034972 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.043357 | val 0.034005 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.042919 | val 0.033150 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.041786 | val 0.032379 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.039797 | val 0.031643 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.039443 | val 0.030855 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.038424 | val 0.030016 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.036862 | val 0.029133 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.036282 | val 0.028237 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.036214 | val 0.027339 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.034093 | val 0.026551 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.034731 | val 0.025796 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.033844 | val 0.025131 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.032834 | val 0.024582 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.031978 | val 0.024032 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.032162 | val 0.023424 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.030020 | val 0.022942 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.030174 | val 0.022397 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.029879 | val 0.021863 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.027574 | val 0.021426 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.02807 | val 0.00371 | noisy 0.00391 | Δ +4.9% | LR 0.00300 | ||ψ-φ|| 0.244\n",
      "[Stage3] L=3 ep 001 | train 0.02396 | val 0.00375 | noisy 0.00391 | Δ +4.0% | LR 0.00300 | ||ψ-φ|| 0.214\n",
      "[Stage3] L=3 ep 002 | train 0.02093 | val 0.00372 | noisy 0.00391 | Δ +4.9% | LR 0.00300 | ||ψ-φ|| 0.236\n",
      "[Stage3] L=3 ep 003 | train 0.02344 | val 0.00368 | noisy 0.00391 | Δ +5.7% | LR 0.00300 | ||ψ-φ|| 0.285\n",
      "[Stage3] L=3 ep 004 | train 0.02572 | val 0.00364 | noisy 0.00391 | Δ +6.8% | LR 0.00300 | ||ψ-φ|| 0.343\n",
      "[Stage3] L=3 ep 005 | train 0.02206 | val 0.00357 | noisy 0.00391 | Δ +8.5% | LR 0.00300 | ||ψ-φ|| 0.407\n",
      "[Stage3] L=3 ep 006 | train 0.02304 | val 0.00351 | noisy 0.00391 | Δ +10.2% | LR 0.00300 | ||ψ-φ|| 0.479\n",
      "[Stage3] L=3 ep 007 | train 0.02293 | val 0.00343 | noisy 0.00391 | Δ +12.3% | LR 0.00300 | ||ψ-φ|| 0.561\n",
      "[Stage3] L=3 ep 008 | train 0.02138 | val 0.00337 | noisy 0.00391 | Δ +13.8% | LR 0.00300 | ||ψ-φ|| 0.631\n",
      "[Stage3] L=3 ep 009 | train 0.02122 | val 0.00331 | noisy 0.00391 | Δ +15.2% | LR 0.00300 | ||ψ-φ|| 0.677\n",
      "[Stage3] L=3 ep 010 | train 0.02278 | val 0.00328 | noisy 0.00391 | Δ +16.0% | LR 0.00300 | ||ψ-φ|| 0.718\n",
      "[Stage3] L=3 ep 011 | train 0.02322 | val 0.00326 | noisy 0.00391 | Δ +16.6% | LR 0.00300 | ||ψ-φ|| 0.761\n",
      "[Stage3] L=3 ep 012 | train 0.02134 | val 0.00323 | noisy 0.00391 | Δ +17.3% | LR 0.00300 | ||ψ-φ|| 0.783\n",
      "[Stage3] L=3 ep 013 | train 0.02136 | val 0.00321 | noisy 0.00391 | Δ +17.9% | LR 0.00300 | ||ψ-φ|| 0.802\n",
      "[Stage3] L=3 ep 014 | train 0.02224 | val 0.00320 | noisy 0.00391 | Δ +18.1% | LR 0.00300 | ||ψ-φ|| 0.823\n",
      "[Stage3] L=3 ep 015 | train 0.01978 | val 0.00317 | noisy 0.00391 | Δ +18.8% | LR 0.00300 | ||ψ-φ|| 0.847\n",
      "[Stage3] L=3 ep 016 | train 0.02236 | val 0.00313 | noisy 0.00391 | Δ +19.8% | LR 0.00300 | ||ψ-φ|| 0.876\n",
      "[Stage3] L=3 ep 017 | train 0.02142 | val 0.00312 | noisy 0.00391 | Δ +20.0% | LR 0.00300 | ||ψ-φ|| 0.902\n",
      "[Stage3] L=3 ep 018 | train 0.01966 | val 0.00310 | noisy 0.00391 | Δ +20.7% | LR 0.00300 | ||ψ-φ|| 0.920\n",
      "[Stage3] L=3 ep 019 | train 0.01790 | val 0.00305 | noisy 0.00391 | Δ +21.9% | LR 0.00300 | ||ψ-φ|| 0.939\n",
      "[Stage3] L=3 ep 020 | train 0.02206 | val 0.00305 | noisy 0.00391 | Δ +21.9% | LR 0.00300 | ||ψ-φ|| 0.949\n",
      "[Stage3] L=3 ep 021 | train 0.01751 | val 0.00304 | noisy 0.00391 | Δ +22.3% | LR 0.00300 | ||ψ-φ|| 0.964\n",
      "[Stage3] L=3 ep 022 | train 0.01974 | val 0.00303 | noisy 0.00391 | Δ +22.4% | LR 0.00300 | ||ψ-φ|| 0.975\n",
      "[Stage3] L=3 ep 023 | train 0.02101 | val 0.00307 | noisy 0.00391 | Δ +21.5% | LR 0.00300 | ||ψ-φ|| 0.986\n",
      "[Stage3] L=3 ep 024 | train 0.02288 | val 0.00307 | noisy 0.00391 | Δ +21.6% | LR 0.00300 | ||ψ-φ|| 0.999\n",
      "[Stage3] L=3 ep 025 | train 0.02133 | val 0.00304 | noisy 0.00391 | Δ +22.2% | LR 0.00300 | ||ψ-φ|| 1.009\n",
      "[Stage3] L=3 ep 026 | train 0.01826 | val 0.00301 | noisy 0.00391 | Δ +23.1% | LR 0.00300 | ||ψ-φ|| 1.019\n",
      "[Stage3] L=3 ep 027 | train 0.02148 | val 0.00299 | noisy 0.00391 | Δ +23.4% | LR 0.00300 | ||ψ-φ|| 1.028\n",
      "[Stage3] L=3 ep 028 | train 0.02108 | val 0.00300 | noisy 0.00391 | Δ +23.2% | LR 0.00300 | ||ψ-φ|| 1.044\n",
      "[Stage3] L=3 ep 029 | train 0.02236 | val 0.00304 | noisy 0.00391 | Δ +22.2% | LR 0.00300 | ||ψ-φ|| 1.055\n",
      "[Stage3] L=3 ep 030 | train 0.02363 | val 0.00304 | noisy 0.00391 | Δ +22.2% | LR 0.00300 | ||ψ-φ|| 1.063\n",
      "[Stage3] L=3 ep 031 | train 0.01886 | val 0.00305 | noisy 0.00391 | Δ +21.9% | LR 0.00300 | ||ψ-φ|| 1.069\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 032 | train 0.02389 | val 0.00301 | noisy 0.00391 | Δ +23.1% | LR 0.00150 | ||ψ-φ|| 1.067\n",
      "[Stage3] L=3 ep 033 | train 0.02221 | val 0.00296 | noisy 0.00391 | Δ +24.3% | LR 0.00150 | ||ψ-φ|| 1.074\n",
      "[Stage3] L=3 ep 034 | train 0.02008 | val 0.00293 | noisy 0.00391 | Δ +24.9% | LR 0.00150 | ||ψ-φ|| 1.081\n",
      "[Stage3] L=3 ep 035 | train 0.01940 | val 0.00292 | noisy 0.00391 | Δ +25.3% | LR 0.00150 | ||ψ-φ|| 1.089\n",
      "[Stage3] L=3 ep 036 | train 0.01821 | val 0.00292 | noisy 0.00391 | Δ +25.2% | LR 0.00150 | ||ψ-φ|| 1.091\n",
      "[Stage3] L=3 ep 037 | train 0.01744 | val 0.00292 | noisy 0.00391 | Δ +25.2% | LR 0.00150 | ||ψ-φ|| 1.089\n",
      "[Stage3] L=3 ep 038 | train 0.02237 | val 0.00294 | noisy 0.00391 | Δ +24.8% | LR 0.00150 | ||ψ-φ|| 1.088\n",
      "[Stage3] L=3 ep 039 | train 0.02160 | val 0.00296 | noisy 0.00391 | Δ +24.3% | LR 0.00150 | ||ψ-φ|| 1.090\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 040 | train 0.01943 | val 0.00294 | noisy 0.00391 | Δ +24.6% | LR 0.00075 | ||ψ-φ|| 1.090\n",
      "[Stage3] L=3 ep 041 | train 0.01924 | val 0.00295 | noisy 0.00391 | Δ +24.6% | LR 0.00075 | ||ψ-φ|| 1.088\n",
      "[Stage3] L=3 ep 042 | train 0.02075 | val 0.00295 | noisy 0.00391 | Δ +24.6% | LR 0.00075 | ||ψ-φ|| 1.086\n",
      "[Stage3] L=3 ep 043 | train 0.02280 | val 0.00294 | noisy 0.00391 | Δ +24.8% | LR 0.00075 | ||ψ-φ|| 1.086\n",
      "[Stage3] L=3 ep 044 | train 0.01884 | val 0.00294 | noisy 0.00391 | Δ +24.8% | LR 0.00075 | ||ψ-φ|| 1.087\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=3 ep 045 | train 0.01796 | val 0.00293 | noisy 0.00391 | Δ +24.9% | LR 0.00038 | ||ψ-φ|| 1.088\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.441268 | val 0.415395 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.423688 | val 0.390060 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.400395 | val 0.364917 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.372110 | val 0.340551 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.348837 | val 0.317512 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.325336 | val 0.296066 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.302971 | val 0.276453 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.281976 | val 0.258460 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.266539 | val 0.241948 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.250757 | val 0.226870 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.230942 | val 0.213194 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.218397 | val 0.200969 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.204856 | val 0.190197 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.193757 | val 0.180842 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.183632 | val 0.172705 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.176637 | val 0.165501 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.166473 | val 0.159172 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.159944 | val 0.153290 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.152411 | val 0.147586 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.144888 | val 0.142117 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.139866 | val 0.136831 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.136051 | val 0.131539 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.131019 | val 0.126359 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.125949 | val 0.121305 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.122723 | val 0.116391 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.115806 | val 0.111797 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.110890 | val 0.107467 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.109687 | val 0.103408 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.104364 | val 0.099525 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.100445 | val 0.095880 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.096950 | val 0.092356 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.093534 | val 0.088902 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.091078 | val 0.085553 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.087542 | val 0.082218 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.084439 | val 0.079109 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.081527 | val 0.076130 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.079648 | val 0.073245 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.077797 | val 0.070591 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.075877 | val 0.068007 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.072482 | val 0.065610 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.071202 | val 0.063379 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.068021 | val 0.061278 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.066480 | val 0.059314 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.064689 | val 0.057464 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.062831 | val 0.055694 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.061210 | val 0.054013 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.059642 | val 0.052367 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.058834 | val 0.050835 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.056223 | val 0.049290 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.057120 | val 0.047852 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.053889 | val 0.046479 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.051954 | val 0.045210 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.051806 | val 0.043989 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.051114 | val 0.042854 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.048966 | val 0.041716 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.047975 | val 0.040621 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.047550 | val 0.039611 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.046357 | val 0.038676 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.045384 | val 0.037757 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.045098 | val 0.037013 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.044588 | val 0.036328 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.044163 | val 0.035726 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.042246 | val 0.035138 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.041157 | val 0.034549 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.041447 | val 0.033960 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.041942 | val 0.033411 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.039916 | val 0.032908 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.040051 | val 0.032489 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.039662 | val 0.032144 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.039198 | val 0.031859 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.038348 | val 0.031600 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.038420 | val 0.031339 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.037393 | val 0.031092 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.037789 | val 0.030787 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.037117 | val 0.030559 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.036823 | val 0.030361 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.036387 | val 0.030127 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.035754 | val 0.029953 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.035363 | val 0.029787 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.035851 | val 0.029614 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.035561 | val 0.029450 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.034957 | val 0.029306 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.034588 | val 0.029154 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.034732 | val 0.029035 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.034116 | val 0.028909 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.033556 | val 0.028866 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.033357 | val 0.028723 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.032814 | val 0.028637 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.032933 | val 0.028476 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.032652 | val 0.028342 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.032434 | val 0.028262 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.031743 | val 0.028239 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.031958 | val 0.028216 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.031314 | val 0.028116 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.031601 | val 0.027962 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.030909 | val 0.027905 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.030406 | val 0.027801 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.030704 | val 0.027694 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.029621 | val 0.027590 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.030192 | val 0.027502 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.030223 | val 0.027416 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.029461 | val 0.027360 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.029751 | val 0.027205 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.029520 | val 0.027165 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.029365 | val 0.027147 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.029563 | val 0.027118 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.028369 | val 0.027165 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.028588 | val 0.027145 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.028426 | val 0.027112 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.028608 | val 0.027046 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.028303 | val 0.027076 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.028235 | val 0.027021 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.028722 | val 0.027008 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.027848 | val 0.026902 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.028148 | val 0.026719 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.028367 | val 0.026588 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.027981 | val 0.026484 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.028299 | val 0.026349 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.027615 | val 0.026254 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.026838 | val 0.026253 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.03002 | val 0.00351 | noisy 0.00391 | Δ +10.1% | LR 0.00300 | ||ψ-φ|| 0.229\n",
      "[Stage3] L=3 ep 001 | train 0.03600 | val 0.00346 | noisy 0.00391 | Δ +11.6% | LR 0.00300 | ||ψ-φ|| 0.240\n",
      "[Stage3] L=3 ep 002 | train 0.04269 | val 0.00327 | noisy 0.00391 | Δ +16.4% | LR 0.00300 | ||ψ-φ|| 0.286\n",
      "[Stage3] L=3 ep 003 | train 0.03237 | val 0.00310 | noisy 0.00391 | Δ +20.6% | LR 0.00300 | ||ψ-φ|| 0.337\n",
      "[Stage3] L=3 ep 004 | train 0.02802 | val 0.00307 | noisy 0.00391 | Δ +21.4% | LR 0.00300 | ||ψ-φ|| 0.366\n",
      "[Stage3] L=3 ep 005 | train 0.03178 | val 0.00306 | noisy 0.00391 | Δ +21.6% | LR 0.00300 | ||ψ-φ|| 0.393\n",
      "[Stage3] L=3 ep 006 | train 0.03348 | val 0.00307 | noisy 0.00391 | Δ +21.5% | LR 0.00300 | ||ψ-φ|| 0.415\n",
      "[Stage3] L=3 ep 007 | train 0.02814 | val 0.00304 | noisy 0.00391 | Δ +22.2% | LR 0.00300 | ||ψ-φ|| 0.447\n",
      "[Stage3] L=3 ep 008 | train 0.03076 | val 0.00305 | noisy 0.00391 | Δ +21.9% | LR 0.00300 | ||ψ-φ|| 0.472\n",
      "[Stage3] L=3 ep 009 | train 0.02823 | val 0.00303 | noisy 0.00391 | Δ +22.3% | LR 0.00300 | ||ψ-φ|| 0.502\n",
      "[Stage3] L=3 ep 010 | train 0.02938 | val 0.00300 | noisy 0.00391 | Δ +23.2% | LR 0.00300 | ||ψ-φ|| 0.538\n",
      "[Stage3] L=3 ep 011 | train 0.02706 | val 0.00296 | noisy 0.00391 | Δ +24.3% | LR 0.00300 | ||ψ-φ|| 0.578\n",
      "[Stage3] L=3 ep 012 | train 0.02453 | val 0.00290 | noisy 0.00391 | Δ +25.7% | LR 0.00300 | ||ψ-φ|| 0.622\n",
      "[Stage3] L=3 ep 013 | train 0.02804 | val 0.00287 | noisy 0.00391 | Δ +26.5% | LR 0.00300 | ||ψ-φ|| 0.649\n",
      "[Stage3] L=3 ep 014 | train 0.03255 | val 0.00289 | noisy 0.00391 | Δ +26.1% | LR 0.00300 | ||ψ-φ|| 0.667\n",
      "[Stage3] L=3 ep 015 | train 0.02597 | val 0.00289 | noisy 0.00391 | Δ +26.0% | LR 0.00300 | ||ψ-φ|| 0.698\n",
      "[Stage3] L=3 ep 016 | train 0.03247 | val 0.00287 | noisy 0.00391 | Δ +26.6% | LR 0.00300 | ||ψ-φ|| 0.732\n",
      "[Stage3] L=3 ep 017 | train 0.03227 | val 0.00281 | noisy 0.00391 | Δ +28.2% | LR 0.00300 | ||ψ-φ|| 0.769\n",
      "[Stage3] L=3 ep 018 | train 0.02551 | val 0.00279 | noisy 0.00391 | Δ +28.6% | LR 0.00300 | ||ψ-φ|| 0.789\n",
      "[Stage3] L=3 ep 019 | train 0.02900 | val 0.00275 | noisy 0.00391 | Δ +29.6% | LR 0.00300 | ||ψ-φ|| 0.813\n",
      "[Stage3] L=3 ep 020 | train 0.02914 | val 0.00272 | noisy 0.00391 | Δ +30.4% | LR 0.00300 | ||ψ-φ|| 0.841\n",
      "[Stage3] L=3 ep 021 | train 0.02523 | val 0.00272 | noisy 0.00391 | Δ +30.4% | LR 0.00300 | ||ψ-φ|| 0.856\n",
      "[Stage3] L=3 ep 022 | train 0.02472 | val 0.00272 | noisy 0.00391 | Δ +30.5% | LR 0.00300 | ||ψ-φ|| 0.870\n",
      "[Stage3] L=3 ep 023 | train 0.02705 | val 0.00270 | noisy 0.00391 | Δ +30.8% | LR 0.00300 | ||ψ-φ|| 0.885\n",
      "[Stage3] L=3 ep 024 | train 0.02764 | val 0.00271 | noisy 0.00391 | Δ +30.7% | LR 0.00300 | ||ψ-φ|| 0.893\n",
      "[Stage3] L=3 ep 025 | train 0.02902 | val 0.00273 | noisy 0.00391 | Δ +30.1% | LR 0.00300 | ||ψ-φ|| 0.904\n",
      "[Stage3] L=3 ep 026 | train 0.03180 | val 0.00274 | noisy 0.00391 | Δ +29.9% | LR 0.00300 | ||ψ-φ|| 0.919\n",
      "[Stage3] L=3 ep 027 | train 0.02591 | val 0.00272 | noisy 0.00391 | Δ +30.4% | LR 0.00300 | ||ψ-φ|| 0.932\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 028 | train 0.02469 | val 0.00271 | noisy 0.00391 | Δ +30.8% | LR 0.00150 | ||ψ-φ|| 0.952\n",
      "[Stage3] L=3 ep 029 | train 0.03021 | val 0.00273 | noisy 0.00391 | Δ +30.2% | LR 0.00150 | ||ψ-φ|| 0.963\n",
      "[Stage3] L=3 ep 030 | train 0.02698 | val 0.00272 | noisy 0.00391 | Δ +30.3% | LR 0.00150 | ||ψ-φ|| 0.969\n",
      "[Stage3] L=3 ep 031 | train 0.03422 | val 0.00273 | noisy 0.00391 | Δ +30.2% | LR 0.00150 | ||ψ-φ|| 0.977\n",
      "[Stage3] L=3 ep 032 | train 0.02387 | val 0.00271 | noisy 0.00391 | Δ +30.6% | LR 0.00150 | ||ψ-φ|| 0.987\n",
      "[Stage3] L=3 ep 033 | train 0.03301 | val 0.00270 | noisy 0.00391 | Δ +30.8% | LR 0.00150 | ||ψ-φ|| 0.994\n",
      "[Stage3] L=3 ep 034 | train 0.02650 | val 0.00268 | noisy 0.00391 | Δ +31.5% | LR 0.00150 | ||ψ-φ|| 1.005\n",
      "[Stage3] L=3 ep 035 | train 0.02599 | val 0.00266 | noisy 0.00391 | Δ +31.9% | LR 0.00150 | ||ψ-φ|| 1.015\n",
      "[Stage3] L=3 ep 036 | train 0.02641 | val 0.00266 | noisy 0.00391 | Δ +31.9% | LR 0.00150 | ||ψ-φ|| 1.020\n",
      "[Stage3] L=3 ep 037 | train 0.02704 | val 0.00265 | noisy 0.00391 | Δ +32.2% | LR 0.00150 | ||ψ-φ|| 1.024\n",
      "[Stage3] L=3 ep 038 | train 0.02836 | val 0.00264 | noisy 0.00391 | Δ +32.3% | LR 0.00150 | ||ψ-φ|| 1.026\n",
      "[Stage3] L=3 ep 039 | train 0.02838 | val 0.00265 | noisy 0.00391 | Δ +32.3% | LR 0.00150 | ||ψ-φ|| 1.025\n",
      "[Stage3] L=3 ep 040 | train 0.02430 | val 0.00266 | noisy 0.00391 | Δ +32.0% | LR 0.00150 | ||ψ-φ|| 1.027\n",
      "[Stage3] L=3 ep 041 | train 0.02969 | val 0.00266 | noisy 0.00391 | Δ +31.9% | LR 0.00150 | ||ψ-φ|| 1.027\n",
      "[Stage3] L=3 ep 042 | train 0.02629 | val 0.00267 | noisy 0.00391 | Δ +31.8% | LR 0.00150 | ||ψ-φ|| 1.027\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 043 | train 0.02955 | val 0.00267 | noisy 0.00391 | Δ +31.7% | LR 0.00075 | ||ψ-φ|| 1.027\n",
      "[Stage3] L=3 ep 044 | train 0.02337 | val 0.00267 | noisy 0.00391 | Δ +31.7% | LR 0.00075 | ||ψ-φ|| 1.026\n",
      "[Stage3] L=3 ep 045 | train 0.02613 | val 0.00267 | noisy 0.00391 | Δ +31.6% | LR 0.00075 | ||ψ-φ|| 1.025\n",
      "[Stage3] L=3 ep 046 | train 0.02691 | val 0.00268 | noisy 0.00391 | Δ +31.4% | LR 0.00075 | ||ψ-φ|| 1.025\n",
      "[Stage3] L=3 ep 047 | train 0.03322 | val 0.00269 | noisy 0.00391 | Δ +31.2% | LR 0.00075 | ||ψ-φ|| 1.027\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=3 ep 048 | train 0.03004 | val 0.00268 | noisy 0.00391 | Δ +31.3% | LR 0.00038 | ||ψ-φ|| 1.030\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.480243 | val 0.443030 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.454674 | val 0.424968 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.438677 | val 0.408180 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.417838 | val 0.391395 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.403064 | val 0.374089 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.384203 | val 0.355978 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.364294 | val 0.337431 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.345762 | val 0.318752 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.327065 | val 0.300107 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.309136 | val 0.282112 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.290329 | val 0.265149 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.273614 | val 0.249430 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.255863 | val 0.234932 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.243383 | val 0.221769 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.230761 | val 0.209638 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.216473 | val 0.198650 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.207209 | val 0.188511 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.198181 | val 0.179001 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.188657 | val 0.170046 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.179942 | val 0.161698 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.172561 | val 0.153539 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.167140 | val 0.145739 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.156792 | val 0.138208 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.150497 | val 0.130817 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.144117 | val 0.123750 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.137250 | val 0.116992 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.129793 | val 0.110529 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.123362 | val 0.104499 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.116953 | val 0.098770 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.111416 | val 0.093563 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.103366 | val 0.088907 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.099803 | val 0.084659 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.095132 | val 0.081020 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.090420 | val 0.077577 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.085681 | val 0.074480 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.082776 | val 0.071450 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.079298 | val 0.068744 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.075511 | val 0.066029 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.071068 | val 0.063501 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.067941 | val 0.061227 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.065516 | val 0.059275 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.063377 | val 0.057416 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.061230 | val 0.055918 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.059581 | val 0.054585 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.056721 | val 0.053419 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.055650 | val 0.052583 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.054056 | val 0.051765 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.052773 | val 0.050656 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.052219 | val 0.049821 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.049984 | val 0.048997 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.049512 | val 0.048180 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.048395 | val 0.047134 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.048157 | val 0.046287 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.047383 | val 0.045535 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.045688 | val 0.044768 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.045174 | val 0.044111 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.044810 | val 0.043528 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.043910 | val 0.042856 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.043424 | val 0.042396 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.042447 | val 0.041769 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.043931 | val 0.041375 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.043423 | val 0.041075 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.042052 | val 0.040587 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.042150 | val 0.040284 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.042086 | val 0.039887 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.041037 | val 0.039660 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.040901 | val 0.039191 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.040816 | val 0.039000 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.041544 | val 0.038616 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.040160 | val 0.038291 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.040306 | val 0.038088 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.039875 | val 0.037802 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.040026 | val 0.037484 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.040589 | val 0.037328 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.039740 | val 0.037074 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.038601 | val 0.036884 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.039034 | val 0.036715 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.038894 | val 0.036519 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.038565 | val 0.036335 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.038468 | val 0.036046 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.038250 | val 0.035779 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.038838 | val 0.035495 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.039213 | val 0.035391 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.038459 | val 0.035337 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.038200 | val 0.035181 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.037019 | val 0.034989 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.038110 | val 0.034805 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.038339 | val 0.034682 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.038458 | val 0.034700 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.036833 | val 0.034441 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.037564 | val 0.034327 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.037406 | val 0.034160 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.037809 | val 0.034025 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.036774 | val 0.033965 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.036677 | val 0.033800 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.037073 | val 0.033803 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.037484 | val 0.033707 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.036076 | val 0.033596 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.037459 | val 0.033575 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.035871 | val 0.033796 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.036585 | val 0.033700 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.035934 | val 0.033610 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.037352 | val 0.033638 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.036553 | val 0.033624 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.037157 | val 0.033710 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.036326 | val 0.033715 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.036071 | val 0.033550 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.035394 | val 0.033397 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.036719 | val 0.033371 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.036000 | val 0.033316 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.036532 | val 0.033233 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.036599 | val 0.033311 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.036157 | val 0.033311 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.035909 | val 0.033327 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.035915 | val 0.033214 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.035947 | val 0.033150 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.035422 | val 0.033121 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.036527 | val 0.033204 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.035836 | val 0.033372 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.035041 | val 0.033369 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.04440 | val 0.00337 | noisy 0.00391 | Δ +13.8% | LR 0.00300 | ||ψ-φ|| 0.295\n",
      "[Stage3] L=3 ep 001 | train 0.04676 | val 0.00334 | noisy 0.00391 | Δ +14.6% | LR 0.00300 | ||ψ-φ|| 0.319\n",
      "[Stage3] L=3 ep 002 | train 0.03506 | val 0.00326 | noisy 0.00391 | Δ +16.5% | LR 0.00300 | ||ψ-φ|| 0.376\n",
      "[Stage3] L=3 ep 003 | train 0.03617 | val 0.00320 | noisy 0.00391 | Δ +18.0% | LR 0.00300 | ||ψ-φ|| 0.428\n",
      "[Stage3] L=3 ep 004 | train 0.03701 | val 0.00316 | noisy 0.00391 | Δ +19.3% | LR 0.00300 | ||ψ-φ|| 0.468\n",
      "[Stage3] L=3 ep 005 | train 0.03787 | val 0.00310 | noisy 0.00391 | Δ +20.6% | LR 0.00300 | ||ψ-φ|| 0.504\n",
      "[Stage3] L=3 ep 006 | train 0.04143 | val 0.00307 | noisy 0.00391 | Δ +21.5% | LR 0.00300 | ||ψ-φ|| 0.525\n",
      "[Stage3] L=3 ep 007 | train 0.03520 | val 0.00304 | noisy 0.00391 | Δ +22.2% | LR 0.00300 | ||ψ-φ|| 0.533\n",
      "[Stage3] L=3 ep 008 | train 0.03738 | val 0.00306 | noisy 0.00391 | Δ +21.7% | LR 0.00300 | ||ψ-φ|| 0.544\n",
      "[Stage3] L=3 ep 009 | train 0.04101 | val 0.00303 | noisy 0.00391 | Δ +22.5% | LR 0.00300 | ||ψ-φ|| 0.549\n",
      "[Stage3] L=3 ep 010 | train 0.04060 | val 0.00303 | noisy 0.00391 | Δ +22.5% | LR 0.00300 | ||ψ-φ|| 0.565\n",
      "[Stage3] L=3 ep 011 | train 0.03574 | val 0.00300 | noisy 0.00391 | Δ +23.2% | LR 0.00300 | ||ψ-φ|| 0.580\n",
      "[Stage3] L=3 ep 012 | train 0.04142 | val 0.00301 | noisy 0.00391 | Δ +23.1% | LR 0.00300 | ||ψ-φ|| 0.582\n",
      "[Stage3] L=3 ep 013 | train 0.03604 | val 0.00299 | noisy 0.00391 | Δ +23.4% | LR 0.00300 | ||ψ-φ|| 0.598\n",
      "[Stage3] L=3 ep 014 | train 0.04084 | val 0.00294 | noisy 0.00391 | Δ +24.8% | LR 0.00300 | ||ψ-φ|| 0.614\n",
      "[Stage3] L=3 ep 015 | train 0.04361 | val 0.00293 | noisy 0.00391 | Δ +25.0% | LR 0.00300 | ||ψ-φ|| 0.625\n",
      "[Stage3] L=3 ep 016 | train 0.03899 | val 0.00294 | noisy 0.00391 | Δ +24.9% | LR 0.00300 | ||ψ-φ|| 0.630\n",
      "[Stage3] L=3 ep 017 | train 0.04089 | val 0.00293 | noisy 0.00391 | Δ +25.1% | LR 0.00300 | ||ψ-φ|| 0.636\n",
      "[Stage3] L=3 ep 018 | train 0.03602 | val 0.00290 | noisy 0.00391 | Δ +25.7% | LR 0.00300 | ||ψ-φ|| 0.651\n",
      "[Stage3] L=3 ep 019 | train 0.04105 | val 0.00290 | noisy 0.00391 | Δ +25.8% | LR 0.00300 | ||ψ-φ|| 0.658\n",
      "[Stage3] L=3 ep 020 | train 0.04353 | val 0.00288 | noisy 0.00391 | Δ +26.4% | LR 0.00300 | ||ψ-φ|| 0.675\n",
      "[Stage3] L=3 ep 021 | train 0.03898 | val 0.00288 | noisy 0.00391 | Δ +26.3% | LR 0.00300 | ||ψ-φ|| 0.686\n",
      "[Stage3] L=3 ep 022 | train 0.03915 | val 0.00286 | noisy 0.00391 | Δ +26.8% | LR 0.00300 | ||ψ-φ|| 0.695\n",
      "[Stage3] L=3 ep 023 | train 0.03675 | val 0.00284 | noisy 0.00391 | Δ +27.4% | LR 0.00300 | ||ψ-φ|| 0.705\n",
      "[Stage3] L=3 ep 024 | train 0.04005 | val 0.00285 | noisy 0.00391 | Δ +27.1% | LR 0.00300 | ||ψ-φ|| 0.725\n",
      "[Stage3] L=3 ep 025 | train 0.03436 | val 0.00283 | noisy 0.00391 | Δ +27.6% | LR 0.00300 | ||ψ-φ|| 0.730\n",
      "[Stage3] L=3 ep 026 | train 0.03122 | val 0.00286 | noisy 0.00391 | Δ +26.9% | LR 0.00300 | ||ψ-φ|| 0.738\n",
      "[Stage3] L=3 ep 027 | train 0.03481 | val 0.00286 | noisy 0.00391 | Δ +26.8% | LR 0.00300 | ||ψ-φ|| 0.729\n",
      "[Stage3] L=3 ep 028 | train 0.04641 | val 0.00285 | noisy 0.00391 | Δ +27.1% | LR 0.00300 | ||ψ-φ|| 0.732\n",
      "[Stage3] L=3 ep 029 | train 0.03685 | val 0.00281 | noisy 0.00391 | Δ +28.0% | LR 0.00300 | ||ψ-φ|| 0.746\n",
      "[Stage3] L=3 ep 030 | train 0.04214 | val 0.00280 | noisy 0.00391 | Δ +28.4% | LR 0.00300 | ||ψ-φ|| 0.755\n",
      "[Stage3] L=3 ep 031 | train 0.04118 | val 0.00279 | noisy 0.00391 | Δ +28.5% | LR 0.00300 | ||ψ-φ|| 0.760\n",
      "[Stage3] L=3 ep 032 | train 0.04240 | val 0.00277 | noisy 0.00391 | Δ +29.2% | LR 0.00300 | ||ψ-φ|| 0.774\n",
      "[Stage3] L=3 ep 033 | train 0.03686 | val 0.00278 | noisy 0.00391 | Δ +29.0% | LR 0.00300 | ||ψ-φ|| 0.789\n",
      "[Stage3] L=3 ep 034 | train 0.03702 | val 0.00277 | noisy 0.00391 | Δ +29.0% | LR 0.00300 | ||ψ-φ|| 0.792\n",
      "[Stage3] L=3 ep 035 | train 0.03411 | val 0.00275 | noisy 0.00391 | Δ +29.6% | LR 0.00300 | ||ψ-φ|| 0.797\n",
      "[Stage3] L=3 ep 036 | train 0.04172 | val 0.00275 | noisy 0.00391 | Δ +29.6% | LR 0.00300 | ||ψ-φ|| 0.810\n",
      "[Stage3] L=3 ep 037 | train 0.03746 | val 0.00274 | noisy 0.00391 | Δ +29.9% | LR 0.00300 | ||ψ-φ|| 0.824\n",
      "[Stage3] L=3 ep 038 | train 0.03934 | val 0.00273 | noisy 0.00391 | Δ +30.3% | LR 0.00300 | ||ψ-φ|| 0.840\n",
      "[Stage3] L=3 ep 039 | train 0.04468 | val 0.00272 | noisy 0.00391 | Δ +30.4% | LR 0.00300 | ||ψ-φ|| 0.844\n",
      "[Stage3] L=3 ep 040 | train 0.03886 | val 0.00275 | noisy 0.00391 | Δ +29.7% | LR 0.00300 | ||ψ-φ|| 0.847\n",
      "[Stage3] L=3 ep 041 | train 0.03857 | val 0.00275 | noisy 0.00391 | Δ +29.6% | LR 0.00300 | ||ψ-φ|| 0.843\n",
      "[Stage3] L=3 ep 042 | train 0.04226 | val 0.00274 | noisy 0.00391 | Δ +29.8% | LR 0.00300 | ||ψ-φ|| 0.846\n",
      "[Stage3] L=3 ep 043 | train 0.03285 | val 0.00273 | noisy 0.00391 | Δ +30.2% | LR 0.00300 | ||ψ-φ|| 0.848\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 044 | train 0.03664 | val 0.00275 | noisy 0.00391 | Δ +29.8% | LR 0.00150 | ||ψ-φ|| 0.860\n",
      "[Stage3] L=3 ep 045 | train 0.04762 | val 0.00276 | noisy 0.00391 | Δ +29.3% | LR 0.00150 | ||ψ-φ|| 0.867\n",
      "[Stage3] L=3 ep 046 | train 0.03958 | val 0.00276 | noisy 0.00391 | Δ +29.4% | LR 0.00150 | ||ψ-φ|| 0.874\n",
      "[Stage3] L=3 ep 047 | train 0.04197 | val 0.00277 | noisy 0.00391 | Δ +29.0% | LR 0.00150 | ||ψ-φ|| 0.884\n",
      "[Stage3] L=3 ep 048 | train 0.03672 | val 0.00277 | noisy 0.00391 | Δ +29.0% | LR 0.00150 | ||ψ-φ|| 0.887\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 049 | train 0.03742 | val 0.00275 | noisy 0.00391 | Δ +29.5% | LR 0.00075 | ||ψ-φ|| 0.892\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.510438 | val 0.492451 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.494195 | val 0.471825 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.471175 | val 0.451660 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.455250 | val 0.431339 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.429601 | val 0.410581 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.411992 | val 0.388998 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.390611 | val 0.367064 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.372499 | val 0.345357 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.350976 | val 0.324703 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.331548 | val 0.304978 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.314367 | val 0.286732 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.297004 | val 0.270409 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.282355 | val 0.255590 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.267335 | val 0.242129 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.252804 | val 0.229975 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.239144 | val 0.218804 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.230478 | val 0.208372 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.220071 | val 0.198515 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.208332 | val 0.189360 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.197358 | val 0.180433 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.190273 | val 0.171921 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.180860 | val 0.163958 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.171079 | val 0.156503 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.164829 | val 0.149342 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.155969 | val 0.142921 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.151344 | val 0.137230 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.142929 | val 0.132110 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.139835 | val 0.127509 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.133787 | val 0.123464 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.131139 | val 0.119674 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.127475 | val 0.116239 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.124300 | val 0.112898 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.120868 | val 0.109703 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.116877 | val 0.106607 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.114309 | val 0.103681 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.111045 | val 0.100714 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.108459 | val 0.097874 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.106181 | val 0.095104 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.104042 | val 0.092494 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.101381 | val 0.089868 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.100327 | val 0.087514 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.096577 | val 0.085468 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.096285 | val 0.083685 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.094926 | val 0.082121 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.092895 | val 0.080589 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.091465 | val 0.079088 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.089970 | val 0.077723 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.088824 | val 0.076453 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.087199 | val 0.075215 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.085291 | val 0.073891 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.084126 | val 0.072719 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.085232 | val 0.071623 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.082908 | val 0.070697 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.081777 | val 0.069761 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.079010 | val 0.068840 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.079116 | val 0.067812 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.078580 | val 0.066958 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.076319 | val 0.066000 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.076589 | val 0.064958 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.074953 | val 0.063941 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.074647 | val 0.062935 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.071988 | val 0.062242 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.072549 | val 0.061394 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.071335 | val 0.060728 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.070449 | val 0.060137 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.069889 | val 0.059474 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.069536 | val 0.058905 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.069361 | val 0.058338 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.066928 | val 0.057870 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.067480 | val 0.057462 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.067530 | val 0.057069 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.064580 | val 0.056719 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.066804 | val 0.056174 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.064218 | val 0.055777 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.063272 | val 0.055426 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.062594 | val 0.054956 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.060955 | val 0.054563 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.061889 | val 0.054012 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.060854 | val 0.053650 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.061408 | val 0.053235 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.059242 | val 0.052912 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.058950 | val 0.052554 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.058112 | val 0.052292 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.057298 | val 0.052003 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.055604 | val 0.051653 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.056115 | val 0.051164 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.055481 | val 0.050900 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.054917 | val 0.050623 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.055017 | val 0.050400 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.053148 | val 0.050357 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.053605 | val 0.050164 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.053668 | val 0.049946 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.053735 | val 0.049791 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.051708 | val 0.049657 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.051652 | val 0.049417 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.052384 | val 0.049217 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.050646 | val 0.049082 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.050725 | val 0.048916 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.050641 | val 0.048811 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.050796 | val 0.048605 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.049800 | val 0.048530 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.049591 | val 0.048528 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.048614 | val 0.048425 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.049175 | val 0.048269 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.048265 | val 0.048168 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.049700 | val 0.048087 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.048881 | val 0.047972 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.048198 | val 0.047852 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.047763 | val 0.047692 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.047859 | val 0.047564 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.047430 | val 0.047427 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.047193 | val 0.047186 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.045876 | val 0.047005 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.046039 | val 0.046762 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.046613 | val 0.046556 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.046482 | val 0.046415 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.046405 | val 0.046258 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.045955 | val 0.046078 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.045544 | val 0.045874 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.046337 | val 0.045609 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.04227 | val 0.00345 | noisy 0.00391 | Δ +11.6% | LR 0.00300 | ||ψ-φ|| 0.285\n",
      "[Stage3] L=3 ep 001 | train 0.03828 | val 0.00344 | noisy 0.00391 | Δ +11.9% | LR 0.00300 | ||ψ-φ|| 0.279\n",
      "[Stage3] L=3 ep 002 | train 0.03957 | val 0.00339 | noisy 0.00391 | Δ +13.3% | LR 0.00300 | ||ψ-φ|| 0.290\n",
      "[Stage3] L=3 ep 003 | train 0.04160 | val 0.00336 | noisy 0.00391 | Δ +14.1% | LR 0.00300 | ||ψ-φ|| 0.313\n",
      "[Stage3] L=3 ep 004 | train 0.04244 | val 0.00333 | noisy 0.00391 | Δ +14.8% | LR 0.00300 | ||ψ-φ|| 0.345\n",
      "[Stage3] L=3 ep 005 | train 0.04216 | val 0.00330 | noisy 0.00391 | Δ +15.6% | LR 0.00300 | ||ψ-φ|| 0.394\n",
      "[Stage3] L=3 ep 006 | train 0.04137 | val 0.00326 | noisy 0.00391 | Δ +16.5% | LR 0.00300 | ||ψ-φ|| 0.453\n",
      "[Stage3] L=3 ep 007 | train 0.03997 | val 0.00321 | noisy 0.00391 | Δ +17.8% | LR 0.00300 | ||ψ-φ|| 0.515\n",
      "[Stage3] L=3 ep 008 | train 0.04489 | val 0.00317 | noisy 0.00391 | Δ +18.9% | LR 0.00300 | ||ψ-φ|| 0.577\n",
      "[Stage3] L=3 ep 009 | train 0.04466 | val 0.00316 | noisy 0.00391 | Δ +19.1% | LR 0.00300 | ||ψ-φ|| 0.634\n",
      "[Stage3] L=3 ep 010 | train 0.03307 | val 0.00311 | noisy 0.00391 | Δ +20.5% | LR 0.00300 | ||ψ-φ|| 0.674\n",
      "[Stage3] L=3 ep 011 | train 0.04011 | val 0.00310 | noisy 0.00391 | Δ +20.6% | LR 0.00300 | ||ψ-φ|| 0.704\n",
      "[Stage3] L=3 ep 012 | train 0.03478 | val 0.00307 | noisy 0.00391 | Δ +21.5% | LR 0.00300 | ||ψ-φ|| 0.736\n",
      "[Stage3] L=3 ep 013 | train 0.03624 | val 0.00305 | noisy 0.00391 | Δ +21.8% | LR 0.00300 | ||ψ-φ|| 0.774\n",
      "[Stage3] L=3 ep 014 | train 0.03565 | val 0.00305 | noisy 0.00391 | Δ +22.0% | LR 0.00300 | ||ψ-φ|| 0.810\n",
      "[Stage3] L=3 ep 015 | train 0.03659 | val 0.00304 | noisy 0.00391 | Δ +22.3% | LR 0.00300 | ||ψ-φ|| 0.846\n",
      "[Stage3] L=3 ep 016 | train 0.04043 | val 0.00302 | noisy 0.00391 | Δ +22.7% | LR 0.00300 | ||ψ-φ|| 0.882\n",
      "[Stage3] L=3 ep 017 | train 0.03001 | val 0.00304 | noisy 0.00391 | Δ +22.2% | LR 0.00300 | ||ψ-φ|| 0.920\n",
      "[Stage3] L=3 ep 018 | train 0.03420 | val 0.00304 | noisy 0.00391 | Δ +22.1% | LR 0.00300 | ||ψ-φ|| 0.958\n",
      "[Stage3] L=3 ep 019 | train 0.03439 | val 0.00303 | noisy 0.00391 | Δ +22.5% | LR 0.00300 | ||ψ-φ|| 0.993\n",
      "[Stage3] L=3 ep 020 | train 0.03505 | val 0.00299 | noisy 0.00391 | Δ +23.4% | LR 0.00300 | ||ψ-φ|| 1.032\n",
      "[Stage3] L=3 ep 021 | train 0.02487 | val 0.00295 | noisy 0.00391 | Δ +24.4% | LR 0.00300 | ||ψ-φ|| 1.065\n",
      "[Stage3] L=3 ep 022 | train 0.03125 | val 0.00291 | noisy 0.00391 | Δ +25.4% | LR 0.00300 | ||ψ-φ|| 1.106\n",
      "[Stage3] L=3 ep 023 | train 0.02886 | val 0.00289 | noisy 0.00391 | Δ +26.2% | LR 0.00300 | ||ψ-φ|| 1.146\n",
      "[Stage3] L=3 ep 024 | train 0.03457 | val 0.00286 | noisy 0.00391 | Δ +26.8% | LR 0.00300 | ||ψ-φ|| 1.184\n",
      "[Stage3] L=3 ep 025 | train 0.03261 | val 0.00285 | noisy 0.00391 | Δ +27.1% | LR 0.00300 | ||ψ-φ|| 1.232\n",
      "[Stage3] L=3 ep 026 | train 0.03304 | val 0.00283 | noisy 0.00391 | Δ +27.6% | LR 0.00300 | ||ψ-φ|| 1.280\n",
      "[Stage3] L=3 ep 027 | train 0.02614 | val 0.00286 | noisy 0.00391 | Δ +26.9% | LR 0.00300 | ||ψ-φ|| 1.326\n",
      "[Stage3] L=3 ep 028 | train 0.02620 | val 0.00289 | noisy 0.00391 | Δ +26.0% | LR 0.00300 | ||ψ-φ|| 1.365\n",
      "[Stage3] L=3 ep 029 | train 0.03088 | val 0.00290 | noisy 0.00391 | Δ +25.8% | LR 0.00300 | ||ψ-φ|| 1.408\n",
      "[Stage3] L=3 ep 030 | train 0.02904 | val 0.00288 | noisy 0.00391 | Δ +26.3% | LR 0.00300 | ||ψ-φ|| 1.453\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 031 | train 0.02839 | val 0.00287 | noisy 0.00391 | Δ +26.5% | LR 0.00150 | ||ψ-φ|| 1.499\n",
      "[Stage3] L=3 ep 032 | train 0.03076 | val 0.00291 | noisy 0.00391 | Δ +25.5% | LR 0.00150 | ||ψ-φ|| 1.536\n",
      "[Stage3] L=3 ep 033 | train 0.02778 | val 0.00293 | noisy 0.00391 | Δ +25.0% | LR 0.00150 | ||ψ-φ|| 1.562\n",
      "[Stage3] L=3 ep 034 | train 0.02938 | val 0.00294 | noisy 0.00391 | Δ +24.7% | LR 0.00150 | ||ψ-φ|| 1.586\n",
      "[Stage3] L=3 ep 035 | train 0.02840 | val 0.00297 | noisy 0.00391 | Δ +24.1% | LR 0.00150 | ||ψ-φ|| 1.606\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 036 | train 0.03032 | val 0.00299 | noisy 0.00391 | Δ +23.6% | LR 0.00075 | ||ψ-φ|| 1.628\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.569974 | val 0.568412 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.545327 | val 0.539654 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.516784 | val 0.509820 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.491448 | val 0.479191 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.460797 | val 0.448040 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.433756 | val 0.416858 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.400945 | val 0.386026 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.376616 | val 0.356190 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.347785 | val 0.327580 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.318523 | val 0.300662 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.296942 | val 0.275988 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.272594 | val 0.253582 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.250434 | val 0.233547 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.235356 | val 0.216074 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.222891 | val 0.200910 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.207547 | val 0.187919 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.195518 | val 0.176758 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.184845 | val 0.167148 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.179388 | val 0.158923 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.172592 | val 0.151598 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.165939 | val 0.145075 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.161045 | val 0.139293 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.156861 | val 0.133940 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.153323 | val 0.129002 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.149592 | val 0.124461 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.143682 | val 0.120264 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.139640 | val 0.116341 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.136750 | val 0.112748 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.130664 | val 0.109519 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.128671 | val 0.106567 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.125081 | val 0.103742 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.121261 | val 0.101231 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.115362 | val 0.098861 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.112530 | val 0.096664 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.109025 | val 0.094587 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.108264 | val 0.092552 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.105106 | val 0.090631 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.101762 | val 0.088823 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.099355 | val 0.087004 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.096188 | val 0.085205 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.093884 | val 0.083344 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.092169 | val 0.081579 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.088313 | val 0.079868 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.085451 | val 0.078175 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.082936 | val 0.076476 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.082225 | val 0.074767 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.081146 | val 0.073089 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.079100 | val 0.071549 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.077199 | val 0.069858 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.073169 | val 0.068322 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.071232 | val 0.066867 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.072389 | val 0.065456 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.070035 | val 0.064132 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.068855 | val 0.062864 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.067009 | val 0.061700 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.066445 | val 0.060453 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.065878 | val 0.059413 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.064024 | val 0.058410 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.064388 | val 0.057421 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.062905 | val 0.056578 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.060920 | val 0.055756 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.060305 | val 0.054959 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.060045 | val 0.054165 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.058579 | val 0.053353 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.057729 | val 0.052573 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.057559 | val 0.051786 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.055966 | val 0.051088 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.055526 | val 0.050417 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.055075 | val 0.049717 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.054138 | val 0.049045 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.052867 | val 0.048355 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.053674 | val 0.047687 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.052803 | val 0.047009 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.052043 | val 0.046362 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.051052 | val 0.045761 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.049798 | val 0.045176 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.048942 | val 0.044626 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.049467 | val 0.044034 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.047984 | val 0.043448 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.047771 | val 0.042819 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.047967 | val 0.042210 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.046403 | val 0.041622 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.045698 | val 0.041027 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.045052 | val 0.040487 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.045344 | val 0.039946 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.044246 | val 0.039376 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.043623 | val 0.038874 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.043695 | val 0.038373 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.042631 | val 0.037893 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.042507 | val 0.037436 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.041266 | val 0.037026 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.041286 | val 0.036615 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.040027 | val 0.036234 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.039348 | val 0.035844 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.039232 | val 0.035446 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.038441 | val 0.035066 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.038012 | val 0.034709 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.037745 | val 0.034346 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.038028 | val 0.034011 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.037784 | val 0.033711 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.037072 | val 0.033424 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.037123 | val 0.033168 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.036589 | val 0.032963 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.036371 | val 0.032745 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.035619 | val 0.032557 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.035906 | val 0.032385 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.036040 | val 0.032199 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.035556 | val 0.032000 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.035332 | val 0.031787 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.034873 | val 0.031626 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.034760 | val 0.031478 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.034678 | val 0.031324 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.033496 | val 0.031244 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.033760 | val 0.031129 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.033545 | val 0.031020 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.033952 | val 0.030839 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.033532 | val 0.030651 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.033727 | val 0.030521 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.034145 | val 0.030381 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.033499 | val 0.030312 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.03646 | val 0.00328 | noisy 0.00391 | Δ +16.0% | LR 0.00300 | ||ψ-φ|| 0.290\n",
      "[Stage3] L=3 ep 001 | train 0.03661 | val 0.00318 | noisy 0.00391 | Δ +18.7% | LR 0.00300 | ||ψ-φ|| 0.309\n",
      "[Stage3] L=3 ep 002 | train 0.03758 | val 0.00304 | noisy 0.00391 | Δ +22.1% | LR 0.00300 | ||ψ-φ|| 0.356\n",
      "[Stage3] L=3 ep 003 | train 0.03183 | val 0.00292 | noisy 0.00391 | Δ +25.4% | LR 0.00300 | ||ψ-φ|| 0.414\n",
      "[Stage3] L=3 ep 004 | train 0.03572 | val 0.00286 | noisy 0.00391 | Δ +26.8% | LR 0.00300 | ||ψ-φ|| 0.460\n",
      "[Stage3] L=3 ep 005 | train 0.03168 | val 0.00284 | noisy 0.00391 | Δ +27.2% | LR 0.00300 | ||ψ-φ|| 0.486\n",
      "[Stage3] L=3 ep 006 | train 0.03600 | val 0.00279 | noisy 0.00391 | Δ +28.5% | LR 0.00300 | ||ψ-φ|| 0.511\n",
      "[Stage3] L=3 ep 007 | train 0.03395 | val 0.00276 | noisy 0.00391 | Δ +29.3% | LR 0.00300 | ||ψ-φ|| 0.536\n",
      "[Stage3] L=3 ep 008 | train 0.03544 | val 0.00275 | noisy 0.00391 | Δ +29.6% | LR 0.00300 | ||ψ-φ|| 0.566\n",
      "[Stage3] L=3 ep 009 | train 0.03331 | val 0.00274 | noisy 0.00391 | Δ +29.8% | LR 0.00300 | ||ψ-φ|| 0.571\n",
      "[Stage3] L=3 ep 010 | train 0.03475 | val 0.00273 | noisy 0.00391 | Δ +30.0% | LR 0.00300 | ||ψ-φ|| 0.580\n",
      "[Stage3] L=3 ep 011 | train 0.03388 | val 0.00271 | noisy 0.00391 | Δ +30.6% | LR 0.00300 | ||ψ-φ|| 0.592\n",
      "[Stage3] L=3 ep 012 | train 0.03598 | val 0.00269 | noisy 0.00391 | Δ +31.3% | LR 0.00300 | ||ψ-φ|| 0.606\n",
      "[Stage3] L=3 ep 013 | train 0.03080 | val 0.00267 | noisy 0.00391 | Δ +31.7% | LR 0.00300 | ||ψ-φ|| 0.627\n",
      "[Stage3] L=3 ep 014 | train 0.03642 | val 0.00268 | noisy 0.00391 | Δ +31.5% | LR 0.00300 | ||ψ-φ|| 0.637\n",
      "[Stage3] L=3 ep 015 | train 0.03514 | val 0.00267 | noisy 0.00391 | Δ +31.8% | LR 0.00300 | ||ψ-φ|| 0.656\n",
      "[Stage3] L=3 ep 016 | train 0.03621 | val 0.00266 | noisy 0.00391 | Δ +31.8% | LR 0.00300 | ||ψ-φ|| 0.677\n",
      "[Stage3] L=3 ep 017 | train 0.03718 | val 0.00267 | noisy 0.00391 | Δ +31.8% | LR 0.00300 | ||ψ-φ|| 0.687\n",
      "[Stage3] L=3 ep 018 | train 0.03141 | val 0.00265 | noisy 0.00391 | Δ +32.3% | LR 0.00300 | ||ψ-φ|| 0.693\n",
      "[Stage3] L=3 ep 019 | train 0.03718 | val 0.00264 | noisy 0.00391 | Δ +32.5% | LR 0.00300 | ||ψ-φ|| 0.699\n",
      "[Stage3] L=3 ep 020 | train 0.03509 | val 0.00261 | noisy 0.00391 | Δ +33.2% | LR 0.00300 | ||ψ-φ|| 0.716\n",
      "[Stage3] L=3 ep 021 | train 0.03326 | val 0.00261 | noisy 0.00391 | Δ +33.2% | LR 0.00300 | ||ψ-φ|| 0.724\n",
      "[Stage3] L=3 ep 022 | train 0.03520 | val 0.00264 | noisy 0.00391 | Δ +32.4% | LR 0.00300 | ||ψ-φ|| 0.736\n",
      "[Stage3] L=3 ep 023 | train 0.03443 | val 0.00262 | noisy 0.00391 | Δ +32.8% | LR 0.00300 | ||ψ-φ|| 0.746\n",
      "[Stage3] L=3 ep 024 | train 0.03784 | val 0.00263 | noisy 0.00391 | Δ +32.7% | LR 0.00300 | ||ψ-φ|| 0.755\n",
      "[Stage3] L=3 ep 025 | train 0.03932 | val 0.00262 | noisy 0.00391 | Δ +32.9% | LR 0.00300 | ||ψ-φ|| 0.770\n",
      "[Stage3] L=3 ep 026 | train 0.03119 | val 0.00258 | noisy 0.00391 | Δ +34.0% | LR 0.00300 | ||ψ-φ|| 0.792\n",
      "[Stage3] L=3 ep 027 | train 0.03698 | val 0.00259 | noisy 0.00391 | Δ +33.7% | LR 0.00300 | ||ψ-φ|| 0.814\n",
      "[Stage3] L=3 ep 028 | train 0.03645 | val 0.00261 | noisy 0.00391 | Δ +33.1% | LR 0.00300 | ||ψ-φ|| 0.825\n",
      "[Stage3] L=3 ep 029 | train 0.03442 | val 0.00261 | noisy 0.00391 | Δ +33.2% | LR 0.00300 | ||ψ-φ|| 0.837\n",
      "[Stage3] L=3 ep 030 | train 0.03678 | val 0.00260 | noisy 0.00391 | Δ +33.4% | LR 0.00300 | ||ψ-φ|| 0.839\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 031 | train 0.03318 | val 0.00259 | noisy 0.00391 | Δ +33.6% | LR 0.00150 | ||ψ-φ|| 0.848\n",
      "[Stage3] L=3 ep 032 | train 0.04087 | val 0.00260 | noisy 0.00391 | Δ +33.6% | LR 0.00150 | ||ψ-φ|| 0.857\n",
      "[Stage3] L=3 ep 033 | train 0.03421 | val 0.00259 | noisy 0.00391 | Δ +33.8% | LR 0.00150 | ||ψ-φ|| 0.861\n",
      "[Stage3] L=3 ep 034 | train 0.03173 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00150 | ||ψ-φ|| 0.865\n",
      "[Stage3] L=3 ep 035 | train 0.03769 | val 0.00257 | noisy 0.00391 | Δ +34.1% | LR 0.00150 | ||ψ-φ|| 0.867\n",
      "[Stage3] L=3 ep 036 | train 0.03114 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00150 | ||ψ-φ|| 0.874\n",
      "[Stage3] L=3 ep 037 | train 0.03457 | val 0.00258 | noisy 0.00391 | Δ +34.0% | LR 0.00150 | ||ψ-φ|| 0.876\n",
      "[Stage3] L=3 ep 038 | train 0.03373 | val 0.00258 | noisy 0.00391 | Δ +34.0% | LR 0.00150 | ||ψ-φ|| 0.878\n",
      "[Stage3] L=3 ep 039 | train 0.03603 | val 0.00259 | noisy 0.00391 | Δ +33.6% | LR 0.00150 | ||ψ-φ|| 0.882\n",
      "[Stage3] L=3 ep 040 | train 0.03352 | val 0.00257 | noisy 0.00391 | Δ +34.1% | LR 0.00150 | ||ψ-φ|| 0.890\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 041 | train 0.03323 | val 0.00258 | noisy 0.00391 | Δ +34.1% | LR 0.00075 | ||ψ-φ|| 0.898\n",
      "[Stage3] L=3 ep 042 | train 0.03346 | val 0.00258 | noisy 0.00391 | Δ +34.1% | LR 0.00075 | ||ψ-φ|| 0.903\n",
      "[Stage3] L=3 ep 043 | train 0.03504 | val 0.00258 | noisy 0.00391 | Δ +34.0% | LR 0.00075 | ||ψ-φ|| 0.904\n",
      "[Stage3] L=3 ep 044 | train 0.03233 | val 0.00257 | noisy 0.00391 | Δ +34.2% | LR 0.00075 | ||ψ-φ|| 0.907\n",
      "[Stage3] L=3 ep 045 | train 0.03081 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00075 | ||ψ-φ|| 0.910\n",
      "[Stage3] L=3 ep 046 | train 0.03399 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00075 | ||ψ-φ|| 0.913\n",
      "[Stage3] L=3 ep 047 | train 0.03134 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00075 | ||ψ-φ|| 0.915\n",
      "[Stage3] L=3 ep 048 | train 0.03005 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00075 | ||ψ-φ|| 0.918\n",
      "[Stage3] L=3 ep 049 | train 0.03157 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00075 | ||ψ-φ|| 0.922\n",
      "[Stage3] L=3 ep 050 | train 0.03651 | val 0.00256 | noisy 0.00391 | Δ +34.5% | LR 0.00075 | ||ψ-φ|| 0.925\n",
      "[Stage3] L=3 ep 051 | train 0.03505 | val 0.00254 | noisy 0.00391 | Δ +34.9% | LR 0.00075 | ||ψ-φ|| 0.929\n",
      "[Stage3] L=3 ep 052 | train 0.03680 | val 0.00254 | noisy 0.00391 | Δ +35.0% | LR 0.00075 | ||ψ-φ|| 0.934\n",
      "[Stage3] L=3 ep 053 | train 0.03183 | val 0.00254 | noisy 0.00391 | Δ +35.0% | LR 0.00075 | ||ψ-φ|| 0.935\n",
      "[Stage3] L=3 ep 054 | train 0.03283 | val 0.00256 | noisy 0.00391 | Δ +34.6% | LR 0.00075 | ||ψ-φ|| 0.936\n",
      "[Stage3] L=3 ep 055 | train 0.03218 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00075 | ||ψ-φ|| 0.936\n",
      "[Stage3] L=3 ep 056 | train 0.03491 | val 0.00258 | noisy 0.00391 | Δ +34.1% | LR 0.00075 | ||ψ-φ|| 0.935\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=3 ep 057 | train 0.03351 | val 0.00257 | noisy 0.00391 | Δ +34.2% | LR 0.00038 | ||ψ-φ|| 0.935\n",
      "[Stage3] L=3 ep 058 | train 0.03660 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00038 | ||ψ-φ|| 0.937\n",
      "[Stage3] L=3 ep 059 | train 0.03582 | val 0.00257 | noisy 0.00391 | Δ +34.3% | LR 0.00038 | ||ψ-φ|| 0.938\n",
      "\n",
      "Completed 10 runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7 — Train runs (instances × layers)\n",
    "# ============================================\n",
    "RUNS = []  # we’ll save each run in the next cell\n",
    "\n",
    "for L in LAYER_OPTIONS:\n",
    "    for inst in INSTANCE_IDS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Instance {inst} | Layers {L}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        s1 = train_stage1(\n",
    "            X_train, X_val,\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=120, batch_size=32,\n",
    "            lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        s3 = train_stage3(\n",
    "            X_train, X_val,\n",
    "            phi_stage1=s1[\"phi\"],\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16,\n",
    "            LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "            CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        RUNS.append({\n",
    "            \"instance_id\": inst,\n",
    "            \"n_layers\": L,\n",
    "            \"stage1\": {\n",
    "                \"phi\": s1[\"phi\"],\n",
    "                \"best_val\": s1[\"best_val\"],\n",
    "                \"hist_train\": s1[\"hist_train\"],\n",
    "                \"hist_val\": s1[\"hist_val\"],\n",
    "                \"hist_lr\": s1[\"hist_lr\"],\n",
    "                \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                \"epochs\": s1.get(\"epochs\"),\n",
    "                \"train_seconds\": float(t1 - t0),\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"psi\": s3[\"psi\"],\n",
    "                \"best_val\": s3[\"best_val\"],\n",
    "                \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                \"epochs\": s3.get(\"epochs\"),\n",
    "                \"hist_train\": s3[\"hist_train\"],\n",
    "                \"hist_val\": s3[\"hist_val\"],\n",
    "                # NEW: capture these so Cell 8 has them\n",
    "                \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": float(t2 - t1),\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c039220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_1ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_1ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_1ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_1ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_1ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_3ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_3ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_3ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_3ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved bundle → ./runs_halfqae_3L1T/q4_l3t1/4q_3l_1t_3ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "\n",
      "All runs saved and recorded.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 8 — Save artifacts (JSON) and append a paper-ready CSV per run\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams logged (keep in sync with training cells) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.003\n",
    "S3_MAX_EPOCHS    = 60\n",
    "S3_PATIENCE      = 10\n",
    "S3_PLATEAU_STEPS = 5\n",
    "S3_PLATEAU_FACT  = 0.5\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v3\"  # keep same as earlier runs so we reuse the same CSV\n",
    "\n",
    "# --- ensure dirs ---\n",
    "ensure_dir(OUT_BASE)\n",
    "# one folder per architecture (e.g., runs_halfqae/q4_l3t1)\n",
    "subroot = ensure_dir(f\"{OUT_BASE}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "\n",
    "# --- CSV path (shared across ALL architectures/runs) ---\n",
    "CSV_PATH = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "\n",
    "# --- header for the full, paper-friendly table ---\n",
    "CSV_HEADER = [\n",
    "    # id / naming\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    # architecture\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    # noise & window\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    # stage-1 hyperparams + outcomes\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    # stage-3 hyperparams + outcomes\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\"s3_patience\",\"s3_plateau_steps\",\"s3_plateau_factor\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    # params (JSON)\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    # totals\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    # Create only if missing; never rewrite an existing header.\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(header)\n",
    "\n",
    "ensure_csv(CSV_PATH, CSV_HEADER)\n",
    "\n",
    "# ------------------------ safe helpers ------------------------\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def _safe_int(x, default):\n",
    "    if x is None:\n",
    "        return default\n",
    "    try:\n",
    "        # catch \"nan\" float case\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return default\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _safe_float(x, default=np.nan):\n",
    "    if x is None:\n",
    "        return default\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def save_one_run(run):\n",
    "    global stage3_handles  # needed by stage3_val_values_det fallback\n",
    "\n",
    "    inst = int(run[\"instance_id\"])\n",
    "    L    = int(run[\"n_layers\"])\n",
    "    seed = int(run.get(\"seed\", inst))\n",
    "\n",
    "    # standardized filename includes arch + layers + instance, so no collisions\n",
    "    fname = std_instance_name(n_qubits, n_latent, n_trash, L, inst)\n",
    "\n",
    "    # Save all instances for this architecture in the same folder (no per-layer subfolders)\n",
    "    out_dir = subroot\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    # pull stage results (robust to missing keys / None)\n",
    "    s1 = run[\"stage1\"]\n",
    "    s3 = run[\"stage3\"]\n",
    "\n",
    "    # Stage-1 metrics\n",
    "    s1_hist_val = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val = _safe_float(s1.get(\"best_val\"), np.nan)\n",
    "    s1_final_val = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = _safe_int(s1.get(\"best_epoch\"), _safe_argmin(s1_hist_val))\n",
    "    s1_epochs     = _safe_int(s1.get(\"epochs\"), len(s1_hist_val))\n",
    "    s1_seconds    = _safe_float(s1.get(\"train_seconds\"), np.nan)\n",
    "\n",
    "    # Stage-3 series\n",
    "    s3_hist_val   = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "\n",
    "    s3_best_val   = _safe_float(s3.get(\"best_val\"), np.nan)\n",
    "    s3_final_val  = _safe_last(s3_hist_val)\n",
    "    s3_best_epoch = _safe_int(s3.get(\"best_epoch\"), _safe_argmin(s3_hist_val))\n",
    "    s3_epochs     = _safe_int(s3.get(\"epochs\"), len(s3_hist_val))\n",
    "    s3_seconds    = _safe_float(s3.get(\"train_seconds\"), np.nan)\n",
    "\n",
    "    # --- compute metrics with FALLBACKS if curves are missing ---\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = (float(np.nanmax(s3_hist_delta)) if (len(s3_hist_delta) and np.isfinite(np.nanmax(s3_hist_delta)))\n",
    "                      else np.nan)\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    need_fallback = (not len(s3_hist_noisy)) or (not np.isfinite(noisy_baseline)) or (not np.isfinite(final_delta))\n",
    "\n",
    "    if need_fallback:\n",
    "        # Rebuild the QNodes for this (L, phi) so we can evaluate psi on X_val\n",
    "        phi_for_L = np.array(s1.get(\"phi\", []))\n",
    "        stage3_handles = stage3_qnodes(L, phi_for_L)  # sets the fixed decoder from φ\n",
    "        psi_params = np.array(s3.get(\"psi\", []))\n",
    "        # Deterministic validation at σ = EVAL_SIGMA\n",
    "        mN, mD, d_pct = stage3_val_values_det(psi_params, X_val, sigma=EVAL_SIGMA)\n",
    "        noisy_baseline = float(mN)\n",
    "        final_delta    = float(d_pct)\n",
    "        if not np.isfinite(best_delta):  # if we don't have a curve, use final as best\n",
    "            best_delta = final_delta\n",
    "\n",
    "    # bundle JSON (parameters + training curves)\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"id\": data_folder,\n",
    "            \"scale_low\":  float(info[\"scale_low\"]),\n",
    "            \"scale_high\": float(info[\"scale_high\"]),\n",
    "            \"window_size\": int(n_qubits),\n",
    "            \"window_stride\": int(WINDOW_STRIDE),\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(EVAL_SIGMA),\n",
    "            \"sigma_eval\":  float(EVAL_SIGMA),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(n_qubits),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(n_latent),\n",
    "            \"n_trash\":  int(n_trash),\n",
    "            \"latent_wires\": list(range(n_latent)),\n",
    "            \"trash_wires\":  list(range(n_latent, n_qubits)),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"patience\": S3_PATIENCE, \"plateau_steps\": S3_PLATEAU_STEPS, \"plateau_factor\": S3_PLATEAU_FACT,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val,\n",
    "                \"noisy_curve\": s3.get(\"hist_noisy\", []), \"delta_curve\": s3_hist_delta,\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # assemble CSV row\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        data_folder,\n",
    "        inst, seed,\n",
    "        int(n_qubits), int(n_latent), int(n_trash), int(L),\n",
    "        f\"{EVAL_SIGMA:.3f}\", f\"{EVAL_SIGMA:.3f}\", int(WINDOW_STRIDE),\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS), int(S3_PATIENCE), int(S3_PLATEAU_STEPS), f\"{S3_PLATEAU_FACT:.3f}\",\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert row into CSV (by unique filename)\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(CSV_PATH).exists():\n",
    "        df_old = pd.read_csv(CSV_PATH)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(CSV_PATH, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Upserted CSV row  → {CSV_PATH}\")\n",
    "\n",
    "# ---- save all runs from Cell 7 ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved and recorded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "137c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae_3L1T/all_training_instances_v3.csv\n",
      "Saved per-layer summary → ./runs_halfqae_3L1T/summary_by_layers_v3.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4q_3l_1t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>47.967000</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>24.797495</td>\n",
       "      <td>23.687649</td>\n",
       "      <td>[1.8694918844253553e-05, 0.9774636820433608, 0...</td>\n",
       "      <td>[-0.0021476862733762012, 0.996444032914851, -0...</td>\n",
       "      <td>93.664973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4q_3l_1t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>26.200279</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>10.927370</td>\n",
       "      <td>8.062937</td>\n",
       "      <td>[2.0115671551483984e-06, -1.9414957211684758, ...</td>\n",
       "      <td>[-0.005241657989718851, -2.042432850799373, -0...</td>\n",
       "      <td>72.431846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4q_3l_1t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>59.531065</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>23.789326</td>\n",
       "      <td>23.382216</td>\n",
       "      <td>[-0.00010595659577317256, 1.0257370823253933, ...</td>\n",
       "      <td>[-0.0002708157444107569, 0.988621714592207, 1....</td>\n",
       "      <td>95.403962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4q_3l_1t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>22.178038</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>10.904187</td>\n",
       "      <td>-11.892166</td>\n",
       "      <td>[-0.0019162555727775263, -1.718724799644914, -...</td>\n",
       "      <td>[0.08051271935437239, -1.752298458289274, -0.0...</td>\n",
       "      <td>68.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4q_3l_1t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>39.887537</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>23.758507</td>\n",
       "      <td>23.310451</td>\n",
       "      <td>[-0.002231335380760849, -2.116122838099371, 0....</td>\n",
       "      <td>[0.0013592018201680257, -2.1589208129393076, 0...</td>\n",
       "      <td>86.037052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4q_3l_1t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>35</td>\n",
       "      <td>46</td>\n",
       "      <td>166.734177</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>25.250994</td>\n",
       "      <td>24.922442</td>\n",
       "      <td>[2.012897733172297, 0.17102605298469328, 0.249...</td>\n",
       "      <td>[2.387139420937167, 0.004220215992417614, 0.09...</td>\n",
       "      <td>287.398430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4q_3l_1t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>38</td>\n",
       "      <td>49</td>\n",
       "      <td>213.243605</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>32.321002</td>\n",
       "      <td>31.322105</td>\n",
       "      <td>[0.17228507891403036, -0.1918817618118622, -0....</td>\n",
       "      <td>[0.012376197078302823, -0.16101771105191287, -...</td>\n",
       "      <td>329.642120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4q_3l_1t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>201.356277</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>30.427728</td>\n",
       "      <td>29.510512</td>\n",
       "      <td>[1.5650355176978477, -0.011291939947879737, 0....</td>\n",
       "      <td>[1.5519425762251506, -0.002817705950746741, 0....</td>\n",
       "      <td>317.262585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4q_3l_1t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>159.787756</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>27.563212</td>\n",
       "      <td>23.598995</td>\n",
       "      <td>[-1.1995772979913892, 0.03568559541760664, -1....</td>\n",
       "      <td>[-1.390070183105758, 0.1460636928406844, -1.27...</td>\n",
       "      <td>280.826698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4q_3l_1t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>52</td>\n",
       "      <td>60</td>\n",
       "      <td>312.112794</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>34.975312</td>\n",
       "      <td>34.338690</td>\n",
       "      <td>[-0.48486297678719315, -0.3432019283736144, 0....</td>\n",
       "      <td>[-0.6005963743263021, -0.3520486794710095, 0.7...</td>\n",
       "      <td>432.757829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag     dataset_folder  instance_id  rng_seed  \\\n",
       "0  4q_3l_1t_1ls_01.json  inst1_L1  mackey_glass_n100            1         1   \n",
       "1  4q_3l_1t_1ls_02.json  inst2_L1  mackey_glass_n100            2         2   \n",
       "2  4q_3l_1t_1ls_03.json  inst3_L1  mackey_glass_n100            3         3   \n",
       "3  4q_3l_1t_1ls_04.json  inst4_L1  mackey_glass_n100            4         4   \n",
       "4  4q_3l_1t_1ls_05.json  inst5_L1  mackey_glass_n100            5         5   \n",
       "5  4q_3l_1t_3ls_01.json  inst1_L3  mackey_glass_n100            1         1   \n",
       "6  4q_3l_1t_3ls_02.json  inst2_L3  mackey_glass_n100            2         2   \n",
       "7  4q_3l_1t_3ls_03.json  inst3_L3  mackey_glass_n100            3         3   \n",
       "8  4q_3l_1t_3ls_04.json  inst4_L3  mackey_glass_n100            4         4   \n",
       "9  4q_3l_1t_3ls_05.json  inst5_L3  mackey_glass_n100            5         5   \n",
       "\n",
       "   n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  s3_final_val_mse  \\\n",
       "0         4         3        1         1          0.1  ...          0.002982   \n",
       "1         4         3        1         1          0.1  ...          0.003593   \n",
       "2         4         3        1         1          0.1  ...          0.002994   \n",
       "3         4         3        1         1          0.1  ...          0.004372   \n",
       "4         4         3        1         1          0.1  ...          0.002997   \n",
       "5         4         3        1         3          0.1  ...          0.002934   \n",
       "6         4         3        1         3          0.1  ...          0.002684   \n",
       "7         4         3        1         3          0.1  ...          0.002755   \n",
       "8         4         3        1         3          0.1  ...          0.002986   \n",
       "9         4         3        1         3          0.1  ...          0.002566   \n",
       "\n",
       "   s3_best_epoch  s3_epochs  s3_train_seconds  s3_noisy_baseline_mse  \\\n",
       "0             20         31         47.967000               0.003908   \n",
       "1              6         17         26.200279               0.003908   \n",
       "2             28         39         59.531065               0.003908   \n",
       "3              3         14         22.178038               0.003908   \n",
       "4             15         26         39.887537               0.003908   \n",
       "5             35         46        166.734177               0.003908   \n",
       "6             38         49        213.243605               0.003908   \n",
       "7             39         50        201.356277               0.003908   \n",
       "8             26         37        159.787756               0.003908   \n",
       "9             52         60        312.112794               0.003908   \n",
       "\n",
       "   s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0          24.797495           23.687649   \n",
       "1          10.927370            8.062937   \n",
       "2          23.789326           23.382216   \n",
       "3          10.904187          -11.892166   \n",
       "4          23.758507           23.310451   \n",
       "5          25.250994           24.922442   \n",
       "6          32.321002           31.322105   \n",
       "7          30.427728           29.510512   \n",
       "8          27.563212           23.598995   \n",
       "9          34.975312           34.338690   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [1.8694918844253553e-05, 0.9774636820433608, 0...   \n",
       "1  [2.0115671551483984e-06, -1.9414957211684758, ...   \n",
       "2  [-0.00010595659577317256, 1.0257370823253933, ...   \n",
       "3  [-0.0019162555727775263, -1.718724799644914, -...   \n",
       "4  [-0.002231335380760849, -2.116122838099371, 0....   \n",
       "5  [2.012897733172297, 0.17102605298469328, 0.249...   \n",
       "6  [0.17228507891403036, -0.1918817618118622, -0....   \n",
       "7  [1.5650355176978477, -0.011291939947879737, 0....   \n",
       "8  [-1.1995772979913892, 0.03568559541760664, -1....   \n",
       "9  [-0.48486297678719315, -0.3432019283736144, 0....   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [-0.0021476862733762012, 0.996444032914851, -0...            93.664973  \n",
       "1  [-0.005241657989718851, -2.042432850799373, -0...            72.431846  \n",
       "2  [-0.0002708157444107569, 0.988621714592207, 1....            95.403962  \n",
       "3  [0.08051271935437239, -1.752298458289274, -0.0...            68.000417  \n",
       "4  [0.0013592018201680257, -2.1589208129393076, 0...            86.037052  \n",
       "5  [2.387139420937167, 0.004220215992417614, 0.09...           287.398430  \n",
       "6  [0.012376197078302823, -0.16101771105191287, -...           329.642120  \n",
       "7  [1.5519425762251506, -0.002817705950746741, 0....           317.262585  \n",
       "8  [-1.390070183105758, 0.1460636928406844, -1.27...           280.826698  \n",
       "9  [-0.6005963743263021, -0.3520486794710095, 0.7...           432.757829  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003908 ± 0.000000</td>\n",
       "      <td>0.003172 ± 0.000253</td>\n",
       "      <td>0.003388 ± 0.000545</td>\n",
       "      <td>18.835377 ± 6.477129</td>\n",
       "      <td>13.310217 ± 13.941557</td>\n",
       "      <td>0.260137 ± 0.004198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003908 ± 0.000000</td>\n",
       "      <td>0.002731 ± 0.000134</td>\n",
       "      <td>0.002785 ± 0.000156</td>\n",
       "      <td>30.107649 ± 3.428025</td>\n",
       "      <td>28.738549 ± 3.990237</td>\n",
       "      <td>0.031344 ± 0.008148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.003908 ± 0.000000     0.003172 ± 0.000253   \n",
       "3            5           0.003908 ± 0.000000     0.002731 ± 0.000134   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.003388 ± 0.000545      18.835377 ± 6.477129   \n",
       "3             0.002785 ± 0.000156      30.107649 ± 3.428025   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1             13.310217 ± 13.941557    0.260137 ± 0.004198  \n",
       "3              28.738549 ± 3.990237    0.031344 ± 0.008148  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results table\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(CSV_PATH).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Run Cell 8 first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Drop duplicate runs; keep the newest copy (with baseline/delta)\n",
    "if \"filename\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "# Typical numeric casts (safe)\n",
    "for col in [\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\n",
    "    \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "    \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\",\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\"instance_id\",\"rng_seed\",\n",
    "    \"window_stride\"\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "\n",
    "clean_path = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "Path(OUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved training-only table → {clean_path}\")\n",
    "\n",
    "# A compact per-layer summary (mean±std); guards against all-NaN\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "metrics = [\n",
    "    (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "    (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "    (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "    (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "    (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "    (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "]\n",
    "\n",
    "grp = df.groupby(\"n_layers\", dropna=False)\n",
    "summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "for col, label in metrics:\n",
    "    if col in df.columns and np.isfinite(df[col]).any():\n",
    "        summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "summary_path = f\"{OUT_BASE}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "summary.to_csv(summary_path, index=True)\n",
    "print(f\"Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
