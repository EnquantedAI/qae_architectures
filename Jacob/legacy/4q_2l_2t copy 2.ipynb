{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e979dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# We'll run 5 instances (same across all notebooks) and two depths: 1 and 3 layers.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # used in filenames as ..._ls_01.json, ..._ls_02.json, ...\n",
    "LAYER_OPTIONS  = [1, 3]            # train 1-layer first, then 3-layers\n",
    "EVAL_SIGMA     = 0.2             # fixed noise everywhere (train & eval)\n",
    "\n",
    "# where to save artifacts (JSON bundles, instance records, CSV summary)\n",
    "OUT_BASE = \"./runs_halfqae\"        # change if you like; subfolders will be created automatically\n",
    "CSV_PATH = f\"{OUT_BASE}/results_instances.csv\"  # will be appended-to if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458cfa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils import OK\n",
      "Seed/filename utils ready.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Cell 1 — Imports, utils, reproducibility (fixed seed)\n",
    "# =====================================================\n",
    "import os, sys, json, math, random, time, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- repo utils (your existing readers) -----\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "parent_dir = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'Jacob' else current_dir\n",
    "sys.path.insert(0, parent_dir); sys.path.insert(0, '../')\n",
    "try:\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Utils import OK\")\n",
    "except Exception as e:\n",
    "    print(\"Import error:\", e)\n",
    "    qae_utils_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(''))), 'qae_utils')\n",
    "    sys.path.insert(0, os.path.dirname(qae_utils_path))\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Absolute path fallback OK\")\n",
    "\n",
    "assert callable(read_ts_file) and callable(read_json_file)\n",
    "\n",
    "# ----- plotting defaults -----\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4)\n",
    "\n",
    "# ----- reproducibility -----\n",
    "def set_global_seed(instance_id: int):\n",
    "    \"\"\"\n",
    "    Derive all RNGs from a simple instance ID (1..5).\n",
    "    Keep the mapping stable across notebooks.\n",
    "    \"\"\"\n",
    "    base = 10_000 + int(instance_id)  # simple, memorable\n",
    "    random.seed(base + 11)\n",
    "    np.random.seed(base + 22)\n",
    "    try:\n",
    "        pnp.random.seed(base + 33)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Pennylane's default.qubit is deterministic given params; no device seeding needed.\n",
    "    return dict(global_seed=base, numpy_seed=base+22, pnp_seed=base+33)\n",
    "\n",
    "def std_instance_name(nq, n_latent, n_trash, n_layers, instance_id):\n",
    "    \"\"\"\n",
    "    Standardized filename pattern used across the project.\n",
    "    Example: 4q_2l_2t_3ls_01.json\n",
    "    \"\"\"\n",
    "    return f\"{int(nq)}q_{int(n_latent)}l_{int(n_trash)}t_{int(n_layers)}ls_{int(instance_id):02d}.json\"\n",
    "\n",
    "def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "print(\"Seed/filename utils ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a103b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: mackey_glass_tau30_n200\n",
      "Loaded 200 samples; scale [0.200,0.800]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 2 — Data loading (deterministic)\n",
    "# =========================================\n",
    "DATA_PATH = '../jacobs_examples/aintern/data'\n",
    "DATA_NAME = 'mackey_glass'  # your folder prefix\n",
    "\n",
    "# fixed split across all instances/layers (so results are comparable)\n",
    "SPLIT_RANDOM_STATE = 42\n",
    "WINDOW_STRIDE = 1\n",
    "\n",
    "# ---- pick most recent MG folder starting with mackey_glass* ----\n",
    "data_folders = [f for f in os.listdir(DATA_PATH) if f.startswith(DATA_NAME)]\n",
    "if not data_folders:\n",
    "    raise FileNotFoundError(\"No Mackey-Glass data found. Generate it first.\")\n",
    "data_folders.sort()\n",
    "data_folder = data_folders[-1]     # take the last one if multiple\n",
    "print(f\"Using data folder: {data_folder}\")\n",
    "\n",
    "# ---- load series + scaling info ----\n",
    "X_idx = read_ts_file(f'{DATA_PATH}/{data_folder}/x_org.arr')   # indices for plotting, not used\n",
    "y_all = read_ts_file(f'{DATA_PATH}/{data_folder}/y_org.arr')   # values\n",
    "info  = read_json_file(f'{DATA_PATH}/{data_folder}/info.json')\n",
    "\n",
    "print(f\"Loaded {len(y_all)} samples; scale [{info['scale_low']:.3f},{info['scale_high']:.3f}]\")\n",
    "\n",
    "# ---- helper: uniform embed wrapper (works with/without explicit info param)\n",
    "def embed_input(x, info_=None):\n",
    "    \"\"\"\n",
    "    Map value-domain window x (in [lo,hi]) to RY(π·v01).\n",
    "    Accepts optional info to match Stage-3 call signatures.\n",
    "    \"\"\"\n",
    "    if info_ is None:\n",
    "        info_ = info\n",
    "    lo, hi = info_['scale_low'], info_['scale_high']\n",
    "    xn = (pnp.array(x) - lo) / max(hi - lo, 1e-12)   # -> [0,1]\n",
    "    for i, v in enumerate(xn):\n",
    "        qml.RY(v * pnp.pi, wires=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2caea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture set: 4q (2 latent and 2 trash).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — Architecture (do NOT change your brick/entanglers)\n",
    "# ============================================================\n",
    "# This notebook = 4 qubits, 2 latent, 2 trash.\n",
    "n_qubits = 4\n",
    "n_latent = 2\n",
    "n_trash  = n_qubits - n_latent\n",
    "trash_wires = [2, 3]            # your original choice\n",
    "signal_wires = list(range(4))   # Stage-1 diagnostics use all wires\n",
    "\n",
    "# --- device factory (simple; default.qubit) ---\n",
    "def make_device(nq): \n",
    "    return qml.device('default.qubit', wires=nq)\n",
    "\n",
    "# --- Stage-1 encoder template (unchanged architecture) ---\n",
    "def encoder_template(params, n_layers):\n",
    "    \"\"\"RX/RY/RZ per qubit + ring CNOT per layer.\"\"\"\n",
    "    assert len(params) == n_layers * n_qubits * 3\n",
    "    for l in range(n_layers):\n",
    "        # local rotations\n",
    "        for q in range(n_qubits):\n",
    "            idx = l * n_qubits * 3 + q * 3\n",
    "            qml.RX(params[idx + 0], wires=q)\n",
    "            qml.RY(params[idx + 1], wires=q)\n",
    "            qml.RZ(params[idx + 2], wires=q)\n",
    "        # ring entanglers\n",
    "        for q in range(n_qubits-1):\n",
    "            qml.CNOT(wires=[q, q+1])\n",
    "        qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "print(\"Architecture set: 4q (2 latent and 2 trash).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539c5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Cell 4 — Stage-1 QNodes, loss, and training (seeded)\n",
    "# ====================================================\n",
    "def stage1_qnodes(n_layers):\n",
    "    dev = make_device(n_qubits)\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def trash_expectations(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in trash_wires]\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def recon_EdagE(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        qml.adjoint(encoder_template)(phi, n_layers)   # E†\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return trash_expectations, recon_EdagE\n",
    "\n",
    "def stage1_batch_loss(trash_expectations, phi, clean_batch):\n",
    "    # L = mean_{batch,trash} P(|1>) = (1 - Z)/2\n",
    "    zs = []\n",
    "    for c in clean_batch:\n",
    "        z = pnp.array(trash_expectations(phi, c))  # shape (n_trash,)\n",
    "        zs.append(z)\n",
    "    zs = pnp.stack(zs, axis=0)\n",
    "    prob_one = (1.0 - zs) * 0.5\n",
    "    return pnp.mean(prob_one)\n",
    "\n",
    "def train_stage1(X_train, X_val, n_layers, instance_id, \n",
    "                 n_epochs=120, batch_size=32, lr_init=0.010,\n",
    "                 patience=10, lr_patience=8, min_delta=1e-6):\n",
    "    set_global_seed(instance_id)\n",
    "    # init\n",
    "    enc_shape = n_layers * n_qubits * 3\n",
    "    phi = pnp.array(np.random.normal(0, 0.5, enc_shape), requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr_init)\n",
    "    lr = lr_init\n",
    "\n",
    "    trash_expectations, recon_EdagE = stage1_qnodes(n_layers)\n",
    "\n",
    "    # helper\n",
    "    def minibatches(N, B, rng_seed=123456):\n",
    "        rng = np.random.default_rng(rng_seed)  # fixed per-epoch seed below\n",
    "        idx = rng.permutation(N)\n",
    "        for i in range(0, N, B):\n",
    "            yield idx[i:i+B]\n",
    "\n",
    "    train_hist, val_hist, lr_hist = [], [], []\n",
    "    best_phi, best_val = None, float(\"inf\")\n",
    "    no_improve = 0\n",
    "    for ep in range(n_epochs):\n",
    "        # batch order deterministic per-epoch per-instance\n",
    "        seed_ep = 77_000 + 100*instance_id + ep\n",
    "        acc = 0.0; nb = 0\n",
    "        for ix in minibatches(len(X_train), batch_size, rng_seed=seed_ep):\n",
    "            clean_batch = X_train[ix]\n",
    "            def loss_fn(p): return stage1_batch_loss(trash_expectations, p, clean_batch)\n",
    "            phi, cost = opt.step_and_cost(loss_fn, phi)\n",
    "            acc += float(cost); nb += 1\n",
    "        train_cost = acc / max(nb, 1)\n",
    "\n",
    "        # validation\n",
    "        v_costs = []\n",
    "        for c in X_val:\n",
    "            v_costs.append(stage1_batch_loss(trash_expectations, phi, pnp.array([c])))\n",
    "        val_cost = float(pnp.mean(pnp.stack(v_costs)))\n",
    "\n",
    "        train_hist.append(train_cost); val_hist.append(val_cost); lr_hist.append(lr)\n",
    "\n",
    "        if val_cost + min_delta < best_val:\n",
    "            best_val, best_phi = val_cost, pnp.array(phi, requires_grad=False); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % lr_patience) == 0:\n",
    "                lr = max(lr * 0.5, 1e-4)\n",
    "                opt = qml.AdamOptimizer(stepsize=lr)\n",
    "                print(f\"[Stage1] ↓ LR → {lr:.5f}\")\n",
    "            if no_improve >= patience:\n",
    "                print(\"[Stage1] Early stopping.\"); break\n",
    "\n",
    "        print(f\"[Stage1] L={n_layers} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {lr:.5f}\")\n",
    "\n",
    "    phi_best = best_phi if best_phi is not None else phi\n",
    "    return dict(\n",
    "        phi=phi_best, best_val=float(best_val),\n",
    "        hist_train=list(map(float, train_hist)),\n",
    "        hist_val=list(map(float, val_hist)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        recon_EdagE=recon_EdagE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3943eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Cell 5 — Stage-3 (ψ) with fixed decoder = adjoint(Stage-1 encoder φ)\n",
    "#          (records hist_noisy, hist_delta, best_epoch, epochs, seconds)\n",
    "# ======================================================================\n",
    "import time\n",
    "\n",
    "def stage3_qnodes(n_layers, phi_stage1):\n",
    "    dev3 = make_device(n_qubits)\n",
    "    theta_fixed = pnp.array(phi_stage1, requires_grad=False).reshape((n_layers, n_qubits, 3))\n",
    "\n",
    "    def encoder_fixed_body(theta):\n",
    "        for l in range(n_layers):\n",
    "            for q in range(n_qubits):\n",
    "                qml.RX(theta[l, q, 0], wires=q)\n",
    "                qml.RY(theta[l, q, 1], wires=q)\n",
    "                qml.RZ(theta[l, q, 2], wires=q)\n",
    "            for q in range(n_qubits-1):\n",
    "                qml.CNOT(wires=[q, q+1])\n",
    "            qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "    def decoder_fixed():\n",
    "        qml.adjoint(encoder_fixed_body)(theta_fixed)\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def encoder_only_expZ_all(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def teacher_code_latents(x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_fixed_body(theta_fixed)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def student_code_latents(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def denoiser_qnode_all(flat_params, x_noisy):\n",
    "        embed_input(x_noisy)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        decoder_fixed()\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return dict(\n",
    "        theta_fixed=theta_fixed,\n",
    "        encoder_only_expZ_all=encoder_only_expZ_all,\n",
    "        teacher_code_latents=teacher_code_latents,\n",
    "        student_code_latents=student_code_latents,\n",
    "        denoiser_qnode_all=denoiser_qnode_all\n",
    "    )\n",
    "\n",
    "# ----- value readout helpers (unchanged)\n",
    "def Z_to_values_autograd(z_all):\n",
    "    z_all = pnp.clip(pnp.asarray(z_all), -0.999999, 0.999999)\n",
    "    v01 = pnp.arccos(z_all) / pnp.pi\n",
    "    return v01 * (info[\"scale_high\"] - info[\"scale_low\"]) + info[\"scale_low\"]\n",
    "\n",
    "def first_diff(x): \n",
    "    x = pnp.array(x); return x[1:] - x[:-1]\n",
    "\n",
    "def p1_from_expZ(z): \n",
    "    return (1 - pnp.asarray(z)) * 0.5\n",
    "\n",
    "# ----- deterministic noisy window (shared with eval)\n",
    "def ts_add_noise_window_det(x, sigma, seed):\n",
    "    low, high = float(info[\"scale_low\"]), float(info[\"scale_high\"])\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    noise = rng.normal(0.0, sigma * (high - low), size=np.asarray(x).shape)\n",
    "    return np.clip(np.asarray(x) + noise, low, high)\n",
    "\n",
    "# ----- validation with fixed per-window seeds (unchanged)\n",
    "VAL_BASE_SEED = 12345\n",
    "\n",
    "def stage3_val_values_det(psi, X_clean, sigma=EVAL_SIGMA):\n",
    "    ms_noisy, ms_deno = [], []\n",
    "    for i, c in enumerate(X_clean):\n",
    "        n = ts_add_noise_window_det(c, sigma, seed=VAL_BASE_SEED + i)\n",
    "        zD = np.array(stage3_handles[\"denoiser_qnode_all\"](psi, n))\n",
    "        v_hat = np.array(Z_to_values_autograd(zD))\n",
    "        ms_noisy.append(np.mean((np.asarray(c) - np.asarray(n))**2))\n",
    "        ms_deno.append(np.mean((np.asarray(c) - v_hat)**2))\n",
    "    mN, mD = float(np.mean(ms_noisy)), float(np.mean(ms_deno))\n",
    "    d_pct = 100.0 * (1.0 - mD / max(mN, 1e-12))\n",
    "    return mN, mD, d_pct\n",
    "\n",
    "# ----- small Huber\n",
    "def huber(residual, delta):\n",
    "    r = pnp.abs(residual)\n",
    "    return pnp.where(r <= delta, 0.5*r**2, delta*(r - 0.5*delta))\n",
    "\n",
    "\n",
    "def train_stage3(X_train, X_val, phi_stage1, n_layers, instance_id,\n",
    "                 TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16, \n",
    "                 LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "                 CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99):\n",
    "\n",
    "    # ---- seeds: varied but reproducible across (instance, layers, epoch, window)\n",
    "    def make_train_seed(instance_id, layers, ep, k, view=0):\n",
    "        return (1_000_003 * (instance_id * 10 + layers) + 97 * ep + 31 * int(k) + view) % 2_147_483_647\n",
    "\n",
    "    set_global_seed(instance_id)\n",
    "\n",
    "    global stage3_handles\n",
    "    stage3_handles = stage3_qnodes(n_layers, phi_stage1)\n",
    "    enc_all = stage3_handles[\"encoder_only_expZ_all\"]\n",
    "    teacher_lat = stage3_handles[\"teacher_code_latents\"]\n",
    "    denoise_all = stage3_handles[\"denoiser_qnode_all\"]\n",
    "\n",
    "    # ---- init ψ near φ\n",
    "    phi_flat = pnp.array(phi_stage1, requires_grad=False)\n",
    "    psi = pnp.array(np.array(phi_flat) + 0.05*np.random.randn(len(phi_flat)), requires_grad=True)\n",
    "\n",
    "    # ---- loss weights\n",
    "    ALPHA_REC, BETA_TF, GAMMA_TRASH, L_TV, L_ANCH = 1.0, 0.05, 0.5, 0.05, 2e-4\n",
    "    DELTA_TV, DELTA_Z = 0.02, 0.25\n",
    "\n",
    "    # loss on a single window with a specific noise seed\n",
    "    def loss_on_window_seeded(params, clean_values, seed):\n",
    "        v_noisy = pnp.array(ts_add_noise_window_det(clean_values, TARGET_NOISE, seed=seed))\n",
    "        z_all = pnp.array(enc_all(params, v_noisy))\n",
    "        z_sig, z_tr = z_all[:n_latent], z_all[n_latent:]\n",
    "        zD = pnp.array(denoise_all(params, v_noisy))\n",
    "        v_hat = Z_to_values_autograd(zD)\n",
    "\n",
    "        L_rec = pnp.mean((pnp.array(clean_values) - v_hat)**2)\n",
    "        z_t_sig = pnp.array(teacher_lat(clean_values))\n",
    "        L_tf = pnp.mean(huber(z_t_sig - z_sig, DELTA_Z))\n",
    "        L_tr = pnp.mean(p1_from_expZ(z_tr))\n",
    "        L_tv = pnp.mean(huber(first_diff(clean_values) - first_diff(v_hat), DELTA_TV))\n",
    "        L_anchor = pnp.mean((params - phi_flat)**2)\n",
    "        return (ALPHA_REC*L_rec + BETA_TF*L_tf + GAMMA_TRASH*L_tr + L_TV*L_tv + L_ANCH*L_anchor)\n",
    "\n",
    "    # manual Adam\n",
    "    m = pnp.zeros_like(psi); v = pnp.zeros_like(psi)\n",
    "    b1, b2, eps = 0.9, 0.999, 1e-8\n",
    "    t = 0\n",
    "    def adam_step(params, grad, lr):\n",
    "        nonlocal m, v, t\n",
    "        t += 1\n",
    "        m = b1*m + (1-b1)*grad\n",
    "        v = b2*v + (1-b2)*(grad*grad)\n",
    "        mhat = m/(1-b1**t); vhat = v/(1-b2**t)\n",
    "        return params - lr * (mhat/(pnp.sqrt(vhat)+eps))\n",
    "\n",
    "    # batches deterministic per-epoch\n",
    "    def batch_indices(N, B, ep_seed):\n",
    "        rng = np.random.default_rng(ep_seed)\n",
    "        idx = rng.permutation(N)\n",
    "        for s in range(0, N, B):\n",
    "            yield idx[s:s+B]\n",
    "\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve, lr = 0, LR_START\n",
    "    ema = pnp.array(psi, requires_grad=False) if USE_EMA else None\n",
    "\n",
    "    # history buffers (for CSV/reporting)\n",
    "    hist_train, hist_val = [], []\n",
    "    hist_noisy, hist_delta = [], []\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(MAX_EPOCHS):\n",
    "        seed_ep = 88_000 + 100*instance_id + ep  # reproducible shuffling\n",
    "        acc, nb = 0.0, 0\n",
    "        for ix in batch_indices(len(X_train), BATCH, ep_seed=seed_ep):\n",
    "            for k in ix:                                   # k = absolute index in X_train\n",
    "                c = X_train[k]\n",
    "                seed = make_train_seed(instance_id, n_layers, ep, int(k))\n",
    "                L = loss_on_window_seeded(psi, c, seed)\n",
    "                if not pnp.isfinite(L): \n",
    "                    continue\n",
    "                g = qml.grad(lambda p: loss_on_window_seeded(p, c, seed))(psi)\n",
    "                if not pnp.all(pnp.isfinite(g)): \n",
    "                    continue\n",
    "                # clip\n",
    "                gnorm = pnp.linalg.norm(g) + 1e-12\n",
    "                if gnorm > CLIP_NORM:\n",
    "                    g = g * (CLIP_NORM / gnorm)\n",
    "                psi = adam_step(psi, g, lr)\n",
    "                if USE_EMA: \n",
    "                    ema = EMA_DECAY*ema + (1-EMA_DECAY)*psi\n",
    "                acc += float(L); nb += 1\n",
    "\n",
    "        train_loss = acc / max(nb, 1)\n",
    "        eval_params = ema if USE_EMA else psi\n",
    "\n",
    "        # strict value-domain validation at σ=EVAL_SIGMA (deterministic per window)\n",
    "        mN, mD, dV = stage3_val_values_det(eval_params, X_val, sigma=EVAL_SIGMA)\n",
    "        hist_train.append(train_loss); hist_val.append(mD)\n",
    "        hist_noisy.append(mN);        hist_delta.append(dV)\n",
    "\n",
    "        if mD < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = mD, pnp.array(eval_params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % PLATEAU_STEPS) == 0:\n",
    "                lr *= PLATEAU_FACTOR\n",
    "                print(f\"[Stage3] Plateau → LR {lr:.5f}\")\n",
    "\n",
    "        norm_diff = float(pnp.linalg.norm((eval_params - phi_flat)))\n",
    "        print(f\"[Stage3] L={n_layers} ep {ep:03d} | train {train_loss:.5f} | \"\n",
    "              f\"val {mD:.5f} | noisy {mN:.5f} | Δ {dV:+.1f}% | LR {lr:.5f} | ||ψ-φ|| {norm_diff:.3f}\")\n",
    "\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"[Stage3] Early stopping.\"); break\n",
    "\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    epochs_run = len(hist_val)\n",
    "\n",
    "    psi_best = best_params if best_params is not None else (ema if USE_EMA else psi)\n",
    "\n",
    "    return dict(\n",
    "        psi=psi_best, \n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=int(epochs_run),\n",
    "        hist_train=list(map(float, hist_train)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50129aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows built: 197 (W=4, step=1)\n",
      "Split sizes → train=117, val=40, test=40\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# Cell 6 — Build windows & deterministic train/val/test\n",
    "# ===================================================\n",
    "window_size = n_qubits\n",
    "stride = WINDOW_STRIDE\n",
    "\n",
    "X_windows = np.array([y_all[i:i+window_size] for i in range(0, len(y_all)-window_size+1, stride)], dtype=float)\n",
    "print(f\"Total windows built: {len(X_windows)} (W={window_size}, step={stride})\")\n",
    "\n",
    "# 60/20/20 split (deterministic)\n",
    "X_temp, X_test = train_test_split(X_windows, test_size=0.20, random_state=SPLIT_RANDOM_STATE)\n",
    "X_train, X_val = train_test_split(X_temp,   test_size=0.25, random_state=SPLIT_RANDOM_STATE)  # 0.25 of 0.8 = 0.2\n",
    "print(f\"Split sizes → train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6711326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.480592 | val 0.472863 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.475778 | val 0.470310 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.475132 | val 0.467689 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.472342 | val 0.465232 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.468208 | val 0.462826 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.467239 | val 0.460468 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.461400 | val 0.458210 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.458514 | val 0.456005 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.454959 | val 0.453897 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.450856 | val 0.451793 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.446003 | val 0.449721 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.447651 | val 0.447556 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.443245 | val 0.445356 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.437866 | val 0.443128 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.437992 | val 0.440782 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.433880 | val 0.438294 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.431583 | val 0.435558 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.424756 | val 0.432571 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.421004 | val 0.429458 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.419302 | val 0.426029 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.417938 | val 0.422236 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.408403 | val 0.418189 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.408050 | val 0.413654 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.406453 | val 0.409120 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.403024 | val 0.404246 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.396755 | val 0.398857 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.393767 | val 0.392994 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.390690 | val 0.386866 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.388349 | val 0.380540 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.382033 | val 0.373874 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.378960 | val 0.366946 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.377492 | val 0.359744 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.373902 | val 0.352163 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.365641 | val 0.344476 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.357185 | val 0.337114 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.359659 | val 0.329698 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.357701 | val 0.322174 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.351292 | val 0.314796 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.342450 | val 0.307729 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.340705 | val 0.301071 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.328901 | val 0.294919 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.328196 | val 0.289276 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.320651 | val 0.284182 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.317517 | val 0.279879 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.311945 | val 0.276245 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.310658 | val 0.273482 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.304723 | val 0.271334 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.302754 | val 0.269995 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.296679 | val 0.269230 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.294173 | val 0.268919 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.287356 | val 0.269271 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.287585 | val 0.269872 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.281553 | val 0.270560 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.284518 | val 0.271628 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.281859 | val 0.272865 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.283245 | val 0.274103 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.279440 | val 0.275237 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 057 | train 0.278805 | val 0.276121 | LR 0.00500\n",
      "[Stage1] L=1 ep 058 | train 0.276407 | val 0.276101 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.19989 | val 0.01262 | noisy 0.01385 | Δ +8.9% | LR 0.00300 | ||ψ-φ|| 0.273\n",
      "[Stage3] L=1 ep 001 | train 0.18561 | val 0.01169 | noisy 0.01385 | Δ +15.6% | LR 0.00300 | ||ψ-φ|| 0.480\n",
      "[Stage3] L=1 ep 002 | train 0.19813 | val 0.01107 | noisy 0.01385 | Δ +20.0% | LR 0.00300 | ||ψ-φ|| 0.680\n",
      "[Stage3] L=1 ep 003 | train 0.19190 | val 0.01070 | noisy 0.01385 | Δ +22.7% | LR 0.00300 | ||ψ-φ|| 0.841\n",
      "[Stage3] L=1 ep 004 | train 0.19005 | val 0.01041 | noisy 0.01385 | Δ +24.8% | LR 0.00300 | ||ψ-φ|| 0.975\n",
      "[Stage3] L=1 ep 005 | train 0.18716 | val 0.01028 | noisy 0.01385 | Δ +25.7% | LR 0.00300 | ||ψ-φ|| 1.093\n",
      "[Stage3] L=1 ep 006 | train 0.19705 | val 0.01011 | noisy 0.01385 | Δ +27.0% | LR 0.00300 | ||ψ-φ|| 1.196\n",
      "[Stage3] L=1 ep 007 | train 0.20146 | val 0.01006 | noisy 0.01385 | Δ +27.3% | LR 0.00300 | ||ψ-φ|| 1.281\n",
      "[Stage3] L=1 ep 008 | train 0.18965 | val 0.01008 | noisy 0.01385 | Δ +27.2% | LR 0.00300 | ||ψ-φ|| 1.336\n",
      "[Stage3] L=1 ep 009 | train 0.19468 | val 0.00995 | noisy 0.01385 | Δ +28.1% | LR 0.00300 | ||ψ-φ|| 1.380\n",
      "[Stage3] L=1 ep 010 | train 0.19300 | val 0.00989 | noisy 0.01385 | Δ +28.6% | LR 0.00300 | ||ψ-φ|| 1.412\n",
      "[Stage3] L=1 ep 011 | train 0.18795 | val 0.00989 | noisy 0.01385 | Δ +28.6% | LR 0.00300 | ||ψ-φ|| 1.434\n",
      "[Stage3] L=1 ep 012 | train 0.19616 | val 0.00993 | noisy 0.01385 | Δ +28.3% | LR 0.00300 | ||ψ-φ|| 1.459\n",
      "[Stage3] L=1 ep 013 | train 0.18961 | val 0.00993 | noisy 0.01385 | Δ +28.3% | LR 0.00300 | ||ψ-φ|| 1.491\n",
      "[Stage3] L=1 ep 014 | train 0.19844 | val 0.00992 | noisy 0.01385 | Δ +28.4% | LR 0.00300 | ||ψ-φ|| 1.539\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 015 | train 0.19766 | val 0.00994 | noisy 0.01385 | Δ +28.2% | LR 0.00150 | ||ψ-φ|| 1.585\n",
      "[Stage3] L=1 ep 016 | train 0.18328 | val 0.01000 | noisy 0.01385 | Δ +27.8% | LR 0.00150 | ||ψ-φ|| 1.597\n",
      "[Stage3] L=1 ep 017 | train 0.20534 | val 0.00995 | noisy 0.01385 | Δ +28.1% | LR 0.00150 | ||ψ-φ|| 1.612\n",
      "[Stage3] L=1 ep 018 | train 0.19377 | val 0.00993 | noisy 0.01385 | Δ +28.3% | LR 0.00150 | ||ψ-φ|| 1.630\n",
      "[Stage3] L=1 ep 019 | train 0.18631 | val 0.00994 | noisy 0.01385 | Δ +28.2% | LR 0.00150 | ||ψ-φ|| 1.635\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 020 | train 0.19568 | val 0.00996 | noisy 0.01385 | Δ +28.0% | LR 0.00075 | ||ψ-φ|| 1.637\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.515313 | val 0.499705 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.510913 | val 0.495923 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.505201 | val 0.492643 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.502642 | val 0.489678 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.496285 | val 0.487038 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.494655 | val 0.484391 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.491479 | val 0.481726 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.486670 | val 0.479020 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.482673 | val 0.476117 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.480214 | val 0.473052 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.476471 | val 0.469850 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.471865 | val 0.466438 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.469562 | val 0.462913 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.466088 | val 0.459309 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.461219 | val 0.455658 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.455742 | val 0.451957 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.452807 | val 0.448253 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.445011 | val 0.444591 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.442490 | val 0.440815 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.429194 | val 0.437132 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.435486 | val 0.433276 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.424155 | val 0.429419 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.427086 | val 0.425423 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.414100 | val 0.421383 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.413838 | val 0.417263 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.409845 | val 0.413032 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.408955 | val 0.408778 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.403716 | val 0.404177 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.401854 | val 0.399559 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.400352 | val 0.394702 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.396603 | val 0.389613 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.390100 | val 0.384197 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.390371 | val 0.378443 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.385250 | val 0.372525 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.382727 | val 0.366142 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.376787 | val 0.359679 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.376716 | val 0.352682 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.368455 | val 0.345745 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.364413 | val 0.338306 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.359155 | val 0.330996 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.358137 | val 0.323658 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.355130 | val 0.316183 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.349389 | val 0.308830 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.338220 | val 0.302112 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.340024 | val 0.295499 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.334433 | val 0.289344 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.325980 | val 0.283793 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.324898 | val 0.278943 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.317487 | val 0.274736 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.311868 | val 0.271239 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.307420 | val 0.268503 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.304358 | val 0.266366 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.300503 | val 0.264897 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.297429 | val 0.264200 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.291688 | val 0.264146 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.292595 | val 0.264617 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.287563 | val 0.265460 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.285260 | val 0.266526 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.284327 | val 0.267844 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.282626 | val 0.269271 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.279311 | val 0.270970 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.282223 | val 0.272900 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 062 | train 0.277430 | val 0.274444 | LR 0.00500\n",
      "[Stage1] L=1 ep 063 | train 0.275417 | val 0.275380 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.20137 | val 0.01246 | noisy 0.01385 | Δ +10.0% | LR 0.00300 | ||ψ-φ|| 0.292\n",
      "[Stage3] L=1 ep 001 | train 0.18965 | val 0.01159 | noisy 0.01385 | Δ +16.3% | LR 0.00300 | ||ψ-φ|| 0.494\n",
      "[Stage3] L=1 ep 002 | train 0.20210 | val 0.01108 | noisy 0.01385 | Δ +20.0% | LR 0.00300 | ||ψ-φ|| 0.677\n",
      "[Stage3] L=1 ep 003 | train 0.19288 | val 0.01062 | noisy 0.01385 | Δ +23.3% | LR 0.00300 | ||ψ-φ|| 0.873\n",
      "[Stage3] L=1 ep 004 | train 0.18868 | val 0.01039 | noisy 0.01385 | Δ +25.0% | LR 0.00300 | ||ψ-φ|| 1.035\n",
      "[Stage3] L=1 ep 005 | train 0.19083 | val 0.01024 | noisy 0.01385 | Δ +26.0% | LR 0.00300 | ||ψ-φ|| 1.163\n",
      "[Stage3] L=1 ep 006 | train 0.19703 | val 0.01016 | noisy 0.01385 | Δ +26.7% | LR 0.00300 | ||ψ-φ|| 1.268\n",
      "[Stage3] L=1 ep 007 | train 0.19196 | val 0.01006 | noisy 0.01385 | Δ +27.3% | LR 0.00300 | ||ψ-φ|| 1.336\n",
      "[Stage3] L=1 ep 008 | train 0.20055 | val 0.00990 | noisy 0.01385 | Δ +28.5% | LR 0.00300 | ||ψ-φ|| 1.400\n",
      "[Stage3] L=1 ep 009 | train 0.19120 | val 0.00977 | noisy 0.01385 | Δ +29.5% | LR 0.00300 | ||ψ-φ|| 1.459\n",
      "[Stage3] L=1 ep 010 | train 0.19197 | val 0.00973 | noisy 0.01385 | Δ +29.7% | LR 0.00300 | ||ψ-φ|| 1.498\n",
      "[Stage3] L=1 ep 011 | train 0.19748 | val 0.00967 | noisy 0.01385 | Δ +30.1% | LR 0.00300 | ||ψ-φ|| 1.527\n",
      "[Stage3] L=1 ep 012 | train 0.19911 | val 0.00967 | noisy 0.01385 | Δ +30.1% | LR 0.00300 | ||ψ-φ|| 1.552\n",
      "[Stage3] L=1 ep 013 | train 0.19177 | val 0.00969 | noisy 0.01385 | Δ +30.0% | LR 0.00300 | ||ψ-φ|| 1.561\n",
      "[Stage3] L=1 ep 014 | train 0.19198 | val 0.00973 | noisy 0.01385 | Δ +29.7% | LR 0.00300 | ||ψ-φ|| 1.571\n",
      "[Stage3] L=1 ep 015 | train 0.19132 | val 0.00979 | noisy 0.01385 | Δ +29.3% | LR 0.00300 | ||ψ-φ|| 1.585\n",
      "[Stage3] L=1 ep 016 | train 0.19810 | val 0.00979 | noisy 0.01385 | Δ +29.3% | LR 0.00300 | ||ψ-φ|| 1.618\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 017 | train 0.19091 | val 0.00977 | noisy 0.01385 | Δ +29.4% | LR 0.00150 | ||ψ-φ|| 1.647\n",
      "[Stage3] L=1 ep 018 | train 0.19139 | val 0.00980 | noisy 0.01385 | Δ +29.3% | LR 0.00150 | ||ψ-φ|| 1.662\n",
      "[Stage3] L=1 ep 019 | train 0.18838 | val 0.00980 | noisy 0.01385 | Δ +29.2% | LR 0.00150 | ||ψ-φ|| 1.667\n",
      "[Stage3] L=1 ep 020 | train 0.19184 | val 0.00979 | noisy 0.01385 | Δ +29.3% | LR 0.00150 | ||ψ-φ|| 1.671\n",
      "[Stage3] L=1 ep 021 | train 0.19040 | val 0.00981 | noisy 0.01385 | Δ +29.2% | LR 0.00150 | ||ψ-φ|| 1.674\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 022 | train 0.19497 | val 0.00980 | noisy 0.01385 | Δ +29.2% | LR 0.00075 | ||ψ-φ|| 1.687\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.503476 | val 0.486066 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.499550 | val 0.480964 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.492803 | val 0.475490 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.486801 | val 0.469428 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.480769 | val 0.462694 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.475103 | val 0.454824 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.466933 | val 0.445843 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.457266 | val 0.435772 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.449173 | val 0.424628 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.439998 | val 0.412659 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.430085 | val 0.400031 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.415125 | val 0.387191 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.404881 | val 0.374260 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.394522 | val 0.361673 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.381380 | val 0.349715 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.371305 | val 0.338543 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.362036 | val 0.328438 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.353785 | val 0.319217 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.340715 | val 0.311040 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.341613 | val 0.303937 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.331469 | val 0.297676 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.325367 | val 0.292405 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.317103 | val 0.287913 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.316493 | val 0.284287 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.314093 | val 0.281345 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.308749 | val 0.278983 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.303386 | val 0.277232 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.297926 | val 0.276057 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.300629 | val 0.275256 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.297830 | val 0.274811 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.297281 | val 0.274476 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.293226 | val 0.274267 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.295382 | val 0.274231 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.291388 | val 0.274237 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.288086 | val 0.274233 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.290065 | val 0.274304 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.285521 | val 0.274274 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.287722 | val 0.274180 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.285700 | val 0.274215 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.285349 | val 0.274221 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.283327 | val 0.274210 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.285641 | val 0.274470 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.284481 | val 0.274778 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.276638 | val 0.275204 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.276187 | val 0.275709 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 045 | train 0.280264 | val 0.276177 | LR 0.00500\n",
      "[Stage1] L=1 ep 046 | train 0.281185 | val 0.277467 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.20141 | val 0.01292 | noisy 0.01385 | Δ +6.7% | LR 0.00300 | ||ψ-φ|| 0.260\n",
      "[Stage3] L=1 ep 001 | train 0.19336 | val 0.01223 | noisy 0.01385 | Δ +11.7% | LR 0.00300 | ||ψ-φ|| 0.441\n",
      "[Stage3] L=1 ep 002 | train 0.20392 | val 0.01130 | noisy 0.01385 | Δ +18.4% | LR 0.00300 | ||ψ-φ|| 0.653\n",
      "[Stage3] L=1 ep 003 | train 0.19506 | val 0.01060 | noisy 0.01385 | Δ +23.4% | LR 0.00300 | ||ψ-φ|| 0.841\n",
      "[Stage3] L=1 ep 004 | train 0.18409 | val 0.01026 | noisy 0.01385 | Δ +25.9% | LR 0.00300 | ||ψ-φ|| 0.989\n",
      "[Stage3] L=1 ep 005 | train 0.19498 | val 0.01013 | noisy 0.01385 | Δ +26.8% | LR 0.00300 | ||ψ-φ|| 1.090\n",
      "[Stage3] L=1 ep 006 | train 0.18756 | val 0.00996 | noisy 0.01385 | Δ +28.1% | LR 0.00300 | ||ψ-φ|| 1.198\n",
      "[Stage3] L=1 ep 007 | train 0.18977 | val 0.00990 | noisy 0.01385 | Δ +28.5% | LR 0.00300 | ||ψ-φ|| 1.261\n",
      "[Stage3] L=1 ep 008 | train 0.19353 | val 0.00983 | noisy 0.01385 | Δ +29.0% | LR 0.00300 | ||ψ-φ|| 1.317\n",
      "[Stage3] L=1 ep 009 | train 0.18991 | val 0.00985 | noisy 0.01385 | Δ +28.8% | LR 0.00300 | ||ψ-φ|| 1.361\n",
      "[Stage3] L=1 ep 010 | train 0.19303 | val 0.00986 | noisy 0.01385 | Δ +28.8% | LR 0.00300 | ||ψ-φ|| 1.396\n",
      "[Stage3] L=1 ep 011 | train 0.18846 | val 0.00983 | noisy 0.01385 | Δ +29.0% | LR 0.00300 | ||ψ-φ|| 1.428\n",
      "[Stage3] L=1 ep 012 | train 0.18915 | val 0.00982 | noisy 0.01385 | Δ +29.1% | LR 0.00300 | ||ψ-φ|| 1.457\n",
      "[Stage3] L=1 ep 013 | train 0.18764 | val 0.00984 | noisy 0.01385 | Δ +28.9% | LR 0.00300 | ||ψ-φ|| 1.482\n",
      "[Stage3] L=1 ep 014 | train 0.20184 | val 0.00978 | noisy 0.01385 | Δ +29.4% | LR 0.00300 | ||ψ-φ|| 1.481\n",
      "[Stage3] L=1 ep 015 | train 0.19454 | val 0.00977 | noisy 0.01385 | Δ +29.4% | LR 0.00300 | ||ψ-φ|| 1.512\n",
      "[Stage3] L=1 ep 016 | train 0.18730 | val 0.00979 | noisy 0.01385 | Δ +29.3% | LR 0.00300 | ||ψ-φ|| 1.517\n",
      "[Stage3] L=1 ep 017 | train 0.18885 | val 0.00976 | noisy 0.01385 | Δ +29.5% | LR 0.00300 | ||ψ-φ|| 1.505\n",
      "[Stage3] L=1 ep 018 | train 0.19533 | val 0.00977 | noisy 0.01385 | Δ +29.5% | LR 0.00300 | ||ψ-φ|| 1.486\n",
      "[Stage3] L=1 ep 019 | train 0.19225 | val 0.00973 | noisy 0.01385 | Δ +29.7% | LR 0.00300 | ||ψ-φ|| 1.504\n",
      "[Stage3] L=1 ep 020 | train 0.20315 | val 0.00973 | noisy 0.01385 | Δ +29.7% | LR 0.00300 | ||ψ-φ|| 1.515\n",
      "[Stage3] L=1 ep 021 | train 0.19419 | val 0.00976 | noisy 0.01385 | Δ +29.5% | LR 0.00300 | ||ψ-φ|| 1.507\n",
      "[Stage3] L=1 ep 022 | train 0.20026 | val 0.00977 | noisy 0.01385 | Δ +29.4% | LR 0.00300 | ||ψ-φ|| 1.514\n",
      "[Stage3] L=1 ep 023 | train 0.19350 | val 0.00981 | noisy 0.01385 | Δ +29.2% | LR 0.00300 | ||ψ-φ|| 1.526\n",
      "[Stage3] L=1 ep 024 | train 0.19371 | val 0.00981 | noisy 0.01385 | Δ +29.1% | LR 0.00300 | ||ψ-φ|| 1.519\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 025 | train 0.18483 | val 0.00977 | noisy 0.01385 | Δ +29.5% | LR 0.00150 | ||ψ-φ|| 1.525\n",
      "[Stage3] L=1 ep 026 | train 0.19905 | val 0.00972 | noisy 0.01385 | Δ +29.8% | LR 0.00150 | ||ψ-φ|| 1.516\n",
      "[Stage3] L=1 ep 027 | train 0.19517 | val 0.00972 | noisy 0.01385 | Δ +29.8% | LR 0.00150 | ||ψ-φ|| 1.514\n",
      "[Stage3] L=1 ep 028 | train 0.18868 | val 0.00973 | noisy 0.01385 | Δ +29.7% | LR 0.00150 | ||ψ-φ|| 1.511\n",
      "[Stage3] L=1 ep 029 | train 0.19789 | val 0.00976 | noisy 0.01385 | Δ +29.5% | LR 0.00150 | ||ψ-φ|| 1.517\n",
      "[Stage3] L=1 ep 030 | train 0.18591 | val 0.00976 | noisy 0.01385 | Δ +29.5% | LR 0.00150 | ||ψ-φ|| 1.524\n",
      "[Stage3] L=1 ep 031 | train 0.18904 | val 0.00977 | noisy 0.01385 | Δ +29.4% | LR 0.00150 | ||ψ-φ|| 1.518\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 032 | train 0.19549 | val 0.00978 | noisy 0.01385 | Δ +29.4% | LR 0.00075 | ||ψ-φ|| 1.514\n",
      "[Stage3] L=1 ep 033 | train 0.19432 | val 0.00977 | noisy 0.01385 | Δ +29.4% | LR 0.00075 | ||ψ-φ|| 1.510\n",
      "[Stage3] L=1 ep 034 | train 0.19323 | val 0.00978 | noisy 0.01385 | Δ +29.4% | LR 0.00075 | ||ψ-φ|| 1.509\n",
      "[Stage3] L=1 ep 035 | train 0.19164 | val 0.00975 | noisy 0.01385 | Δ +29.6% | LR 0.00075 | ||ψ-φ|| 1.514\n",
      "[Stage3] L=1 ep 036 | train 0.19226 | val 0.00975 | noisy 0.01385 | Δ +29.6% | LR 0.00075 | ||ψ-φ|| 1.511\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=1 ep 037 | train 0.19019 | val 0.00974 | noisy 0.01385 | Δ +29.6% | LR 0.00038 | ||ψ-φ|| 1.515\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.477443 | val 0.477386 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.474633 | val 0.475130 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.473240 | val 0.472720 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.470950 | val 0.470311 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.466382 | val 0.467938 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.461909 | val 0.465581 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.460704 | val 0.463232 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.456791 | val 0.460919 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.446566 | val 0.458671 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.448104 | val 0.456410 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.447575 | val 0.454181 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.445262 | val 0.451914 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.440614 | val 0.449648 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.437811 | val 0.447268 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.435204 | val 0.444683 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.430644 | val 0.441872 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.428332 | val 0.438858 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.423871 | val 0.435569 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.419205 | val 0.431988 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.414635 | val 0.428129 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.413816 | val 0.424064 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.410349 | val 0.419532 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.411790 | val 0.414753 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.407663 | val 0.409802 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.398338 | val 0.404439 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.396348 | val 0.398972 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.393389 | val 0.393068 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.390637 | val 0.386760 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.388101 | val 0.380448 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.385910 | val 0.373779 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.376603 | val 0.366786 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.375862 | val 0.359718 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.370473 | val 0.352515 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.366345 | val 0.345115 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.363724 | val 0.338306 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.359427 | val 0.331276 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.350999 | val 0.324556 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.348634 | val 0.317663 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.346275 | val 0.311078 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.337637 | val 0.304866 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.332218 | val 0.299095 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.328324 | val 0.293439 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.326182 | val 0.288274 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.317516 | val 0.283912 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.317848 | val 0.280114 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.314179 | val 0.276956 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.310029 | val 0.274608 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.302739 | val 0.273022 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.301215 | val 0.271883 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.292350 | val 0.271389 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.295529 | val 0.271342 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.288623 | val 0.271533 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.291796 | val 0.271900 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.279307 | val 0.272759 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.286763 | val 0.273665 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.285933 | val 0.274538 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.282422 | val 0.275068 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.278555 | val 0.275645 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 058 | train 0.281102 | val 0.276443 | LR 0.00500\n",
      "[Stage1] L=1 ep 059 | train 0.282174 | val 0.277162 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.20114 | val 0.01279 | noisy 0.01385 | Δ +7.6% | LR 0.00300 | ||ψ-φ|| 0.276\n",
      "[Stage3] L=1 ep 001 | train 0.19143 | val 0.01176 | noisy 0.01385 | Δ +15.1% | LR 0.00300 | ||ψ-φ|| 0.467\n",
      "[Stage3] L=1 ep 002 | train 0.19809 | val 0.01103 | noisy 0.01385 | Δ +20.3% | LR 0.00300 | ||ψ-φ|| 0.673\n",
      "[Stage3] L=1 ep 003 | train 0.19593 | val 0.01063 | noisy 0.01385 | Δ +23.2% | LR 0.00300 | ||ψ-φ|| 0.836\n",
      "[Stage3] L=1 ep 004 | train 0.19573 | val 0.01034 | noisy 0.01385 | Δ +25.3% | LR 0.00300 | ||ψ-φ|| 0.980\n",
      "[Stage3] L=1 ep 005 | train 0.18600 | val 0.01021 | noisy 0.01385 | Δ +26.3% | LR 0.00300 | ||ψ-φ|| 1.096\n",
      "[Stage3] L=1 ep 006 | train 0.19013 | val 0.01014 | noisy 0.01385 | Δ +26.8% | LR 0.00300 | ||ψ-φ|| 1.188\n",
      "[Stage3] L=1 ep 007 | train 0.20156 | val 0.01008 | noisy 0.01385 | Δ +27.2% | LR 0.00300 | ||ψ-φ|| 1.280\n",
      "[Stage3] L=1 ep 008 | train 0.19867 | val 0.00998 | noisy 0.01385 | Δ +27.9% | LR 0.00300 | ||ψ-φ|| 1.381\n",
      "[Stage3] L=1 ep 009 | train 0.19044 | val 0.01002 | noisy 0.01385 | Δ +27.7% | LR 0.00300 | ||ψ-φ|| 1.454\n",
      "[Stage3] L=1 ep 010 | train 0.20269 | val 0.00994 | noisy 0.01385 | Δ +28.2% | LR 0.00300 | ||ψ-φ|| 1.525\n",
      "[Stage3] L=1 ep 011 | train 0.18747 | val 0.00993 | noisy 0.01385 | Δ +28.3% | LR 0.00300 | ||ψ-φ|| 1.562\n",
      "[Stage3] L=1 ep 012 | train 0.19358 | val 0.01002 | noisy 0.01385 | Δ +27.6% | LR 0.00300 | ||ψ-φ|| 1.595\n",
      "[Stage3] L=1 ep 013 | train 0.19331 | val 0.01001 | noisy 0.01385 | Δ +27.7% | LR 0.00300 | ||ψ-φ|| 1.614\n",
      "[Stage3] L=1 ep 014 | train 0.19649 | val 0.00997 | noisy 0.01385 | Δ +28.0% | LR 0.00300 | ||ψ-φ|| 1.643\n",
      "[Stage3] L=1 ep 015 | train 0.20045 | val 0.00996 | noisy 0.01385 | Δ +28.1% | LR 0.00300 | ||ψ-φ|| 1.697\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 016 | train 0.19688 | val 0.00994 | noisy 0.01385 | Δ +28.2% | LR 0.00150 | ||ψ-φ|| 1.727\n",
      "[Stage3] L=1 ep 017 | train 0.20152 | val 0.00992 | noisy 0.01385 | Δ +28.4% | LR 0.00150 | ||ψ-φ|| 1.749\n",
      "[Stage3] L=1 ep 018 | train 0.19493 | val 0.00988 | noisy 0.01385 | Δ +28.6% | LR 0.00150 | ||ψ-φ|| 1.756\n",
      "[Stage3] L=1 ep 019 | train 0.19310 | val 0.00987 | noisy 0.01385 | Δ +28.7% | LR 0.00150 | ||ψ-φ|| 1.749\n",
      "[Stage3] L=1 ep 020 | train 0.19445 | val 0.00988 | noisy 0.01385 | Δ +28.6% | LR 0.00150 | ||ψ-φ|| 1.741\n",
      "[Stage3] L=1 ep 021 | train 0.19620 | val 0.00988 | noisy 0.01385 | Δ +28.7% | LR 0.00150 | ||ψ-φ|| 1.748\n",
      "[Stage3] L=1 ep 022 | train 0.19607 | val 0.00986 | noisy 0.01385 | Δ +28.8% | LR 0.00150 | ||ψ-φ|| 1.746\n",
      "[Stage3] L=1 ep 023 | train 0.19957 | val 0.00985 | noisy 0.01385 | Δ +28.9% | LR 0.00150 | ||ψ-φ|| 1.741\n",
      "[Stage3] L=1 ep 024 | train 0.19195 | val 0.00988 | noisy 0.01385 | Δ +28.7% | LR 0.00150 | ||ψ-φ|| 1.742\n",
      "[Stage3] L=1 ep 025 | train 0.19122 | val 0.00991 | noisy 0.01385 | Δ +28.4% | LR 0.00150 | ||ψ-φ|| 1.748\n",
      "[Stage3] L=1 ep 026 | train 0.19201 | val 0.00993 | noisy 0.01385 | Δ +28.3% | LR 0.00150 | ||ψ-φ|| 1.739\n",
      "[Stage3] L=1 ep 027 | train 0.19972 | val 0.00995 | noisy 0.01385 | Δ +28.1% | LR 0.00150 | ||ψ-φ|| 1.727\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 028 | train 0.19437 | val 0.00993 | noisy 0.01385 | Δ +28.3% | LR 0.00075 | ||ψ-φ|| 1.723\n",
      "[Stage3] L=1 ep 029 | train 0.18912 | val 0.00992 | noisy 0.01385 | Δ +28.4% | LR 0.00075 | ||ψ-φ|| 1.725\n",
      "[Stage3] L=1 ep 030 | train 0.18608 | val 0.00992 | noisy 0.01385 | Δ +28.3% | LR 0.00075 | ||ψ-φ|| 1.722\n",
      "[Stage3] L=1 ep 031 | train 0.19427 | val 0.00993 | noisy 0.01385 | Δ +28.3% | LR 0.00075 | ||ψ-φ|| 1.721\n",
      "[Stage3] L=1 ep 032 | train 0.19447 | val 0.00991 | noisy 0.01385 | Δ +28.4% | LR 0.00075 | ||ψ-φ|| 1.723\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=1 ep 033 | train 0.18657 | val 0.00990 | noisy 0.01385 | Δ +28.5% | LR 0.00038 | ||ψ-φ|| 1.720\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.480529 | val 0.474307 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.477132 | val 0.470755 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.473960 | val 0.467065 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.470419 | val 0.463497 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.464939 | val 0.460163 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.464207 | val 0.456816 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.460463 | val 0.453637 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.453619 | val 0.450609 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.450273 | val 0.447570 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.446308 | val 0.444400 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.441201 | val 0.441149 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.438618 | val 0.437781 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.439317 | val 0.434184 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.430543 | val 0.430669 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.428297 | val 0.426966 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.424848 | val 0.422931 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.423070 | val 0.418829 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.415740 | val 0.414453 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.411513 | val 0.409732 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.404808 | val 0.405000 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.402432 | val 0.399790 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.400742 | val 0.394556 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.396002 | val 0.389136 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.392880 | val 0.383512 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.392928 | val 0.377500 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.389393 | val 0.371327 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.383033 | val 0.365072 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.380566 | val 0.358527 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.374318 | val 0.351731 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.373112 | val 0.344766 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.371608 | val 0.337885 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.361643 | val 0.330898 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.359310 | val 0.323787 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.353307 | val 0.316776 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.354034 | val 0.309997 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.341024 | val 0.303540 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.339478 | val 0.297452 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.334699 | val 0.291557 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.329551 | val 0.286067 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.328738 | val 0.281075 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.323344 | val 0.276747 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.316097 | val 0.273008 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.311759 | val 0.269868 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.308868 | val 0.267282 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.306931 | val 0.265405 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.302191 | val 0.263995 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.300112 | val 0.263101 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.300686 | val 0.262886 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.287862 | val 0.263092 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.288346 | val 0.263768 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.289926 | val 0.264807 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.285928 | val 0.266280 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.280086 | val 0.267759 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.278946 | val 0.269477 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.282063 | val 0.271343 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 055 | train 0.278571 | val 0.272913 | LR 0.00500\n",
      "[Stage1] L=1 ep 056 | train 0.280097 | val 0.274453 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.19292 | val 0.01271 | noisy 0.01385 | Δ +8.2% | LR 0.00300 | ||ψ-φ|| 0.254\n",
      "[Stage3] L=1 ep 001 | train 0.19061 | val 0.01170 | noisy 0.01385 | Δ +15.5% | LR 0.00300 | ||ψ-φ|| 0.465\n",
      "[Stage3] L=1 ep 002 | train 0.18326 | val 0.01102 | noisy 0.01385 | Δ +20.4% | LR 0.00300 | ||ψ-φ|| 0.646\n",
      "[Stage3] L=1 ep 003 | train 0.19150 | val 0.01066 | noisy 0.01385 | Δ +23.0% | LR 0.00300 | ||ψ-φ|| 0.772\n",
      "[Stage3] L=1 ep 004 | train 0.18934 | val 0.01040 | noisy 0.01385 | Δ +24.9% | LR 0.00300 | ||ψ-φ|| 0.913\n",
      "[Stage3] L=1 ep 005 | train 0.19000 | val 0.01028 | noisy 0.01385 | Δ +25.8% | LR 0.00300 | ||ψ-φ|| 1.025\n",
      "[Stage3] L=1 ep 006 | train 0.19864 | val 0.01009 | noisy 0.01385 | Δ +27.2% | LR 0.00300 | ||ψ-φ|| 1.137\n",
      "[Stage3] L=1 ep 007 | train 0.20048 | val 0.01009 | noisy 0.01385 | Δ +27.1% | LR 0.00300 | ||ψ-φ|| 1.203\n",
      "[Stage3] L=1 ep 008 | train 0.18681 | val 0.01003 | noisy 0.01385 | Δ +27.5% | LR 0.00300 | ||ψ-φ|| 1.272\n",
      "[Stage3] L=1 ep 009 | train 0.19949 | val 0.00997 | noisy 0.01385 | Δ +28.0% | LR 0.00300 | ||ψ-φ|| 1.357\n",
      "[Stage3] L=1 ep 010 | train 0.19642 | val 0.00991 | noisy 0.01385 | Δ +28.5% | LR 0.00300 | ||ψ-φ|| 1.418\n",
      "[Stage3] L=1 ep 011 | train 0.19531 | val 0.00987 | noisy 0.01385 | Δ +28.7% | LR 0.00300 | ||ψ-φ|| 1.464\n",
      "[Stage3] L=1 ep 012 | train 0.19050 | val 0.00991 | noisy 0.01385 | Δ +28.4% | LR 0.00300 | ||ψ-φ|| 1.467\n",
      "[Stage3] L=1 ep 013 | train 0.19544 | val 0.00987 | noisy 0.01385 | Δ +28.8% | LR 0.00300 | ||ψ-φ|| 1.500\n",
      "[Stage3] L=1 ep 014 | train 0.19203 | val 0.00982 | noisy 0.01385 | Δ +29.1% | LR 0.00300 | ||ψ-φ|| 1.517\n",
      "[Stage3] L=1 ep 015 | train 0.18776 | val 0.00985 | noisy 0.01385 | Δ +28.8% | LR 0.00300 | ||ψ-φ|| 1.523\n",
      "[Stage3] L=1 ep 016 | train 0.19416 | val 0.00981 | noisy 0.01385 | Δ +29.2% | LR 0.00300 | ||ψ-φ|| 1.539\n",
      "[Stage3] L=1 ep 017 | train 0.19444 | val 0.00984 | noisy 0.01385 | Δ +28.9% | LR 0.00300 | ||ψ-φ|| 1.547\n",
      "[Stage3] L=1 ep 018 | train 0.19402 | val 0.00979 | noisy 0.01385 | Δ +29.3% | LR 0.00300 | ||ψ-φ|| 1.554\n",
      "[Stage3] L=1 ep 019 | train 0.19010 | val 0.00978 | noisy 0.01385 | Δ +29.4% | LR 0.00300 | ||ψ-φ|| 1.557\n",
      "[Stage3] L=1 ep 020 | train 0.20535 | val 0.00974 | noisy 0.01385 | Δ +29.7% | LR 0.00300 | ||ψ-φ|| 1.567\n",
      "[Stage3] L=1 ep 021 | train 0.19090 | val 0.00973 | noisy 0.01385 | Δ +29.8% | LR 0.00300 | ||ψ-φ|| 1.571\n",
      "[Stage3] L=1 ep 022 | train 0.19890 | val 0.00977 | noisy 0.01385 | Δ +29.4% | LR 0.00300 | ||ψ-φ|| 1.569\n",
      "[Stage3] L=1 ep 023 | train 0.19654 | val 0.00979 | noisy 0.01385 | Δ +29.3% | LR 0.00300 | ||ψ-φ|| 1.575\n",
      "[Stage3] L=1 ep 024 | train 0.18819 | val 0.00981 | noisy 0.01385 | Δ +29.2% | LR 0.00300 | ||ψ-φ|| 1.561\n",
      "[Stage3] L=1 ep 025 | train 0.19653 | val 0.00984 | noisy 0.01385 | Δ +28.9% | LR 0.00300 | ||ψ-φ|| 1.554\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 026 | train 0.19473 | val 0.00982 | noisy 0.01385 | Δ +29.1% | LR 0.00150 | ||ψ-φ|| 1.561\n",
      "[Stage3] L=1 ep 027 | train 0.19143 | val 0.00985 | noisy 0.01385 | Δ +28.9% | LR 0.00150 | ||ψ-φ|| 1.560\n",
      "[Stage3] L=1 ep 028 | train 0.19031 | val 0.00987 | noisy 0.01385 | Δ +28.7% | LR 0.00150 | ||ψ-φ|| 1.558\n",
      "[Stage3] L=1 ep 029 | train 0.19888 | val 0.00988 | noisy 0.01385 | Δ +28.7% | LR 0.00150 | ||ψ-φ|| 1.568\n",
      "[Stage3] L=1 ep 030 | train 0.19048 | val 0.00988 | noisy 0.01385 | Δ +28.7% | LR 0.00150 | ||ψ-φ|| 1.569\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 031 | train 0.19233 | val 0.00987 | noisy 0.01385 | Δ +28.7% | LR 0.00075 | ||ψ-φ|| 1.573\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 1 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.412273 | val 0.403525 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.377406 | val 0.371037 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.351901 | val 0.345337 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.331228 | val 0.324734 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.311978 | val 0.307263 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.298322 | val 0.291510 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.285685 | val 0.277873 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.270142 | val 0.266020 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.263400 | val 0.255949 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.253563 | val 0.246980 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.246355 | val 0.239234 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.237267 | val 0.232062 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.230503 | val 0.225307 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.226640 | val 0.219056 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.218063 | val 0.212951 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.213308 | val 0.206919 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.208537 | val 0.200638 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.201063 | val 0.194488 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.196409 | val 0.188309 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.189216 | val 0.182001 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.182893 | val 0.176195 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.177897 | val 0.170345 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.171743 | val 0.164190 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.164929 | val 0.158193 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.159138 | val 0.152466 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.153672 | val 0.146815 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.147474 | val 0.141257 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.142385 | val 0.136067 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.138726 | val 0.131338 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.132986 | val 0.126750 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.128521 | val 0.122910 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.124753 | val 0.119000 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.120929 | val 0.115444 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.119626 | val 0.112724 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.116747 | val 0.110538 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.114208 | val 0.108563 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.112393 | val 0.106091 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.110463 | val 0.104225 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.108040 | val 0.102942 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.107103 | val 0.101707 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.106025 | val 0.100508 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.103767 | val 0.099204 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.101202 | val 0.098200 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.101622 | val 0.096956 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.100527 | val 0.096182 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.099710 | val 0.095809 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.099384 | val 0.094789 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.097992 | val 0.094088 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.097989 | val 0.093599 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.096990 | val 0.093101 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.095879 | val 0.093034 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.094134 | val 0.092516 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.092715 | val 0.091706 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.093270 | val 0.091559 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.092701 | val 0.091619 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.092361 | val 0.091401 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.092111 | val 0.090915 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.092794 | val 0.090424 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.090618 | val 0.090228 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.093029 | val 0.090536 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.089409 | val 0.090405 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.090485 | val 0.090093 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.090851 | val 0.090131 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.091706 | val 0.090473 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.088948 | val 0.090048 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.088705 | val 0.089821 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.089720 | val 0.089990 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.089773 | val 0.090223 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.091570 | val 0.090460 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.090101 | val 0.090132 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.089620 | val 0.090171 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.089994 | val 0.090007 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.090405 | val 0.090100 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=3 ep 073 | train 0.088227 | val 0.090429 | LR 0.00500\n",
      "[Stage1] L=3 ep 074 | train 0.090198 | val 0.090009 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=3 ep 000 | train 0.09972 | val 0.01178 | noisy 0.01385 | Δ +14.9% | LR 0.00300 | ||ψ-φ|| 0.338\n",
      "[Stage3] L=3 ep 001 | train 0.09252 | val 0.01080 | noisy 0.01385 | Δ +22.0% | LR 0.00300 | ||ψ-φ|| 0.491\n",
      "[Stage3] L=3 ep 002 | train 0.11009 | val 0.01018 | noisy 0.01385 | Δ +26.5% | LR 0.00300 | ||ψ-φ|| 0.628\n",
      "[Stage3] L=3 ep 003 | train 0.09571 | val 0.00982 | noisy 0.01385 | Δ +29.1% | LR 0.00300 | ||ψ-φ|| 0.724\n",
      "[Stage3] L=3 ep 004 | train 0.09744 | val 0.00968 | noisy 0.01385 | Δ +30.1% | LR 0.00300 | ||ψ-φ|| 0.777\n",
      "[Stage3] L=3 ep 005 | train 0.08832 | val 0.00966 | noisy 0.01385 | Δ +30.2% | LR 0.00300 | ||ψ-φ|| 0.826\n",
      "[Stage3] L=3 ep 006 | train 0.09616 | val 0.00950 | noisy 0.01385 | Δ +31.4% | LR 0.00300 | ||ψ-φ|| 0.889\n",
      "[Stage3] L=3 ep 007 | train 0.10084 | val 0.00941 | noisy 0.01385 | Δ +32.0% | LR 0.00300 | ||ψ-φ|| 0.955\n",
      "[Stage3] L=3 ep 008 | train 0.09662 | val 0.00939 | noisy 0.01385 | Δ +32.2% | LR 0.00300 | ||ψ-φ|| 0.988\n",
      "[Stage3] L=3 ep 009 | train 0.09853 | val 0.00937 | noisy 0.01385 | Δ +32.3% | LR 0.00300 | ||ψ-φ|| 1.022\n",
      "[Stage3] L=3 ep 010 | train 0.09486 | val 0.00942 | noisy 0.01385 | Δ +32.0% | LR 0.00300 | ||ψ-φ|| 1.032\n",
      "[Stage3] L=3 ep 011 | train 0.09727 | val 0.00935 | noisy 0.01385 | Δ +32.4% | LR 0.00300 | ||ψ-φ|| 1.089\n",
      "[Stage3] L=3 ep 012 | train 0.09804 | val 0.00934 | noisy 0.01385 | Δ +32.5% | LR 0.00300 | ||ψ-φ|| 1.128\n",
      "[Stage3] L=3 ep 013 | train 0.09121 | val 0.00941 | noisy 0.01385 | Δ +32.0% | LR 0.00300 | ||ψ-φ|| 1.157\n",
      "[Stage3] L=3 ep 014 | train 0.08824 | val 0.00942 | noisy 0.01385 | Δ +31.9% | LR 0.00300 | ||ψ-φ|| 1.164\n",
      "[Stage3] L=3 ep 015 | train 0.09599 | val 0.00942 | noisy 0.01385 | Δ +32.0% | LR 0.00300 | ||ψ-φ|| 1.180\n",
      "[Stage3] L=3 ep 016 | train 0.10167 | val 0.00942 | noisy 0.01385 | Δ +32.0% | LR 0.00300 | ||ψ-φ|| 1.203\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 017 | train 0.09403 | val 0.00946 | noisy 0.01385 | Δ +31.7% | LR 0.00150 | ||ψ-φ|| 1.230\n",
      "[Stage3] L=3 ep 018 | train 0.09501 | val 0.00948 | noisy 0.01385 | Δ +31.5% | LR 0.00150 | ||ψ-φ|| 1.235\n",
      "[Stage3] L=3 ep 019 | train 0.08737 | val 0.00949 | noisy 0.01385 | Δ +31.5% | LR 0.00150 | ||ψ-φ|| 1.240\n",
      "[Stage3] L=3 ep 020 | train 0.08971 | val 0.00951 | noisy 0.01385 | Δ +31.3% | LR 0.00150 | ||ψ-φ|| 1.247\n",
      "[Stage3] L=3 ep 021 | train 0.09859 | val 0.00952 | noisy 0.01385 | Δ +31.3% | LR 0.00150 | ||ψ-φ|| 1.250\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 022 | train 0.09626 | val 0.00952 | noisy 0.01385 | Δ +31.3% | LR 0.00075 | ||ψ-φ|| 1.259\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.502667 | val 0.492886 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.465963 | val 0.456300 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.432255 | val 0.423846 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.402618 | val 0.395178 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.370197 | val 0.368469 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.345091 | val 0.343420 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.318562 | val 0.320691 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.298386 | val 0.300528 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.275756 | val 0.282394 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.260979 | val 0.265900 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.247632 | val 0.250717 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.231848 | val 0.236137 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.221466 | val 0.221979 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.206849 | val 0.208071 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.201478 | val 0.195090 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.184502 | val 0.183172 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.177420 | val 0.173191 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.171555 | val 0.164758 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.166685 | val 0.157545 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.166873 | val 0.151338 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.154492 | val 0.145792 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.154407 | val 0.140938 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.145045 | val 0.136537 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.145625 | val 0.132724 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.141150 | val 0.129268 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.138160 | val 0.126197 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.131484 | val 0.123533 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.130960 | val 0.121117 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.127351 | val 0.119005 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.124449 | val 0.117242 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.122627 | val 0.115511 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.123000 | val 0.114117 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.119394 | val 0.112862 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.117271 | val 0.111880 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.116309 | val 0.110875 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.114901 | val 0.110009 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.114724 | val 0.109150 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.113007 | val 0.108322 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.114382 | val 0.107419 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.111519 | val 0.106602 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.111426 | val 0.105812 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.110361 | val 0.105088 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.111088 | val 0.104407 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.106463 | val 0.103476 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.107922 | val 0.102351 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.105911 | val 0.100881 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.104761 | val 0.099574 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.101353 | val 0.098163 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.099490 | val 0.096591 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.097107 | val 0.094991 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.095317 | val 0.093476 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.093621 | val 0.092057 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.092059 | val 0.090625 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.090444 | val 0.089688 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.087963 | val 0.089048 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.086903 | val 0.088394 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.086893 | val 0.087911 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.084407 | val 0.087279 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.084889 | val 0.086851 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.084203 | val 0.086675 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.082586 | val 0.086653 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.083344 | val 0.086509 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.080601 | val 0.086108 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.080713 | val 0.085729 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.081576 | val 0.085800 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.080347 | val 0.085826 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.081882 | val 0.086301 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.082961 | val 0.086293 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.080252 | val 0.086090 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.080952 | val 0.086002 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.081454 | val 0.085800 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=3 ep 071 | train 0.081087 | val 0.086179 | LR 0.00500\n",
      "[Stage1] L=3 ep 072 | train 0.082695 | val 0.086544 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=3 ep 000 | train 0.09888 | val 0.01206 | noisy 0.01385 | Δ +12.9% | LR 0.00300 | ||ψ-φ|| 0.291\n",
      "[Stage3] L=3 ep 001 | train 0.10100 | val 0.01045 | noisy 0.01385 | Δ +24.5% | LR 0.00300 | ||ψ-φ|| 0.478\n",
      "[Stage3] L=3 ep 002 | train 0.09367 | val 0.00994 | noisy 0.01385 | Δ +28.2% | LR 0.00300 | ||ψ-φ|| 0.564\n",
      "[Stage3] L=3 ep 003 | train 0.09776 | val 0.00965 | noisy 0.01385 | Δ +30.3% | LR 0.00300 | ||ψ-φ|| 0.621\n",
      "[Stage3] L=3 ep 004 | train 0.10167 | val 0.00954 | noisy 0.01385 | Δ +31.1% | LR 0.00300 | ||ψ-φ|| 0.679\n",
      "[Stage3] L=3 ep 005 | train 0.09926 | val 0.00941 | noisy 0.01385 | Δ +32.0% | LR 0.00300 | ||ψ-φ|| 0.735\n",
      "[Stage3] L=3 ep 006 | train 0.10212 | val 0.00934 | noisy 0.01385 | Δ +32.5% | LR 0.00300 | ||ψ-φ|| 0.765\n",
      "[Stage3] L=3 ep 007 | train 0.09334 | val 0.00923 | noisy 0.01385 | Δ +33.4% | LR 0.00300 | ||ψ-φ|| 0.816\n",
      "[Stage3] L=3 ep 008 | train 0.09214 | val 0.00920 | noisy 0.01385 | Δ +33.5% | LR 0.00300 | ||ψ-φ|| 0.855\n",
      "[Stage3] L=3 ep 009 | train 0.09792 | val 0.00917 | noisy 0.01385 | Δ +33.8% | LR 0.00300 | ||ψ-φ|| 0.894\n",
      "[Stage3] L=3 ep 010 | train 0.09503 | val 0.00920 | noisy 0.01385 | Δ +33.6% | LR 0.00300 | ||ψ-φ|| 0.931\n",
      "[Stage3] L=3 ep 011 | train 0.09481 | val 0.00920 | noisy 0.01385 | Δ +33.6% | LR 0.00300 | ||ψ-φ|| 0.983\n",
      "[Stage3] L=3 ep 012 | train 0.09080 | val 0.00910 | noisy 0.01385 | Δ +34.3% | LR 0.00300 | ||ψ-φ|| 1.035\n",
      "[Stage3] L=3 ep 013 | train 0.08970 | val 0.00896 | noisy 0.01385 | Δ +35.3% | LR 0.00300 | ||ψ-φ|| 1.046\n",
      "[Stage3] L=3 ep 014 | train 0.09355 | val 0.00898 | noisy 0.01385 | Δ +35.1% | LR 0.00300 | ||ψ-φ|| 1.049\n",
      "[Stage3] L=3 ep 015 | train 0.09201 | val 0.00899 | noisy 0.01385 | Δ +35.1% | LR 0.00300 | ||ψ-φ|| 1.074\n",
      "[Stage3] L=3 ep 016 | train 0.09923 | val 0.00894 | noisy 0.01385 | Δ +35.5% | LR 0.00300 | ||ψ-φ|| 1.123\n",
      "[Stage3] L=3 ep 017 | train 0.09464 | val 0.00895 | noisy 0.01385 | Δ +35.4% | LR 0.00300 | ||ψ-φ|| 1.130\n",
      "[Stage3] L=3 ep 018 | train 0.09693 | val 0.00902 | noisy 0.01385 | Δ +34.9% | LR 0.00300 | ||ψ-φ|| 1.148\n",
      "[Stage3] L=3 ep 019 | train 0.09354 | val 0.00905 | noisy 0.01385 | Δ +34.6% | LR 0.00300 | ||ψ-φ|| 1.172\n",
      "[Stage3] L=3 ep 020 | train 0.09002 | val 0.00903 | noisy 0.01385 | Δ +34.8% | LR 0.00300 | ||ψ-φ|| 1.180\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 021 | train 0.08931 | val 0.00910 | noisy 0.01385 | Δ +34.3% | LR 0.00150 | ||ψ-φ|| 1.169\n",
      "[Stage3] L=3 ep 022 | train 0.09571 | val 0.00909 | noisy 0.01385 | Δ +34.4% | LR 0.00150 | ||ψ-φ|| 1.177\n",
      "[Stage3] L=3 ep 023 | train 0.09132 | val 0.00908 | noisy 0.01385 | Δ +34.4% | LR 0.00150 | ||ψ-φ|| 1.171\n",
      "[Stage3] L=3 ep 024 | train 0.08786 | val 0.00908 | noisy 0.01385 | Δ +34.4% | LR 0.00150 | ||ψ-φ|| 1.170\n",
      "[Stage3] L=3 ep 025 | train 0.09052 | val 0.00903 | noisy 0.01385 | Δ +34.8% | LR 0.00150 | ||ψ-φ|| 1.180\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 026 | train 0.09716 | val 0.00907 | noisy 0.01385 | Δ +34.5% | LR 0.00075 | ||ψ-φ|| 1.189\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.470644 | val 0.412214 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.439223 | val 0.387911 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.415394 | val 0.367582 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.394819 | val 0.347824 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.374256 | val 0.328477 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.351915 | val 0.310550 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.333681 | val 0.294040 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.309294 | val 0.278822 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.301276 | val 0.265729 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.277351 | val 0.252618 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.262108 | val 0.239845 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.244009 | val 0.226454 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.230425 | val 0.213985 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.221304 | val 0.203164 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.208981 | val 0.193665 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.199575 | val 0.186358 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.197619 | val 0.181356 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.193707 | val 0.176013 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.186702 | val 0.171095 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.186150 | val 0.167674 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.184605 | val 0.164833 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.177368 | val 0.162651 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.178142 | val 0.160986 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.176475 | val 0.159521 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.175908 | val 0.158213 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.172760 | val 0.156874 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.171983 | val 0.155852 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.168279 | val 0.154831 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.170352 | val 0.153840 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.168084 | val 0.152839 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.167096 | val 0.151804 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.164259 | val 0.150873 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.165235 | val 0.149949 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.165046 | val 0.149091 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.164301 | val 0.148233 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.163262 | val 0.147513 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.162182 | val 0.146788 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.161295 | val 0.145985 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.161142 | val 0.145170 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.159851 | val 0.144329 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.157986 | val 0.143422 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.158100 | val 0.142863 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.159656 | val 0.141959 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.153789 | val 0.141030 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.152512 | val 0.139872 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.153930 | val 0.139241 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.154342 | val 0.138376 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.150018 | val 0.137566 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.151114 | val 0.136514 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.148575 | val 0.135331 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.146382 | val 0.134164 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.144621 | val 0.132937 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.141156 | val 0.131800 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.138356 | val 0.130599 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.137413 | val 0.129341 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.134240 | val 0.128005 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.133194 | val 0.126116 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.130167 | val 0.124441 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.129068 | val 0.122346 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.125295 | val 0.120271 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.121628 | val 0.117981 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.117675 | val 0.115576 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.117225 | val 0.113063 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.114233 | val 0.110859 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.110653 | val 0.108462 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.107781 | val 0.106020 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.106179 | val 0.104005 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.104269 | val 0.102087 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.102540 | val 0.100635 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.102524 | val 0.098708 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.100890 | val 0.096943 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.098559 | val 0.095364 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.097028 | val 0.093976 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.095437 | val 0.092341 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.092869 | val 0.090414 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.089472 | val 0.088407 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.087878 | val 0.086578 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.085766 | val 0.084155 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.083299 | val 0.081553 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.082078 | val 0.078647 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.078435 | val 0.075983 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.074565 | val 0.073847 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.072462 | val 0.072143 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.071638 | val 0.070982 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.069269 | val 0.070014 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.070231 | val 0.069735 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.068762 | val 0.069518 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.068886 | val 0.069654 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.069096 | val 0.069634 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.068265 | val 0.069363 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.068524 | val 0.069474 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.068533 | val 0.069510 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.068745 | val 0.069685 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.068930 | val 0.069511 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.068232 | val 0.069428 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.068493 | val 0.069529 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.069377 | val 0.069557 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=3 ep 097 | train 0.067032 | val 0.069450 | LR 0.00500\n",
      "[Stage1] L=3 ep 098 | train 0.069444 | val 0.069289 | LR 0.00500\n",
      "[Stage1] L=3 ep 099 | train 0.067806 | val 0.069155 | LR 0.00500\n",
      "[Stage1] L=3 ep 100 | train 0.068328 | val 0.069382 | LR 0.00500\n",
      "[Stage1] L=3 ep 101 | train 0.068845 | val 0.069387 | LR 0.00500\n",
      "[Stage1] L=3 ep 102 | train 0.067242 | val 0.069486 | LR 0.00500\n",
      "[Stage1] L=3 ep 103 | train 0.068201 | val 0.069632 | LR 0.00500\n",
      "[Stage1] L=3 ep 104 | train 0.068906 | val 0.069659 | LR 0.00500\n",
      "[Stage1] L=3 ep 105 | train 0.067699 | val 0.069617 | LR 0.00500\n",
      "[Stage1] L=3 ep 106 | train 0.067084 | val 0.069504 | LR 0.00500\n",
      "[Stage1] ↓ LR → 0.00250\n",
      "[Stage1] L=3 ep 107 | train 0.068163 | val 0.069482 | LR 0.00250\n",
      "[Stage1] L=3 ep 108 | train 0.069236 | val 0.069738 | LR 0.00250\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=3 ep 000 | train 0.09640 | val 0.01186 | noisy 0.01385 | Δ +14.3% | LR 0.00300 | ||ψ-φ|| 0.356\n",
      "[Stage3] L=3 ep 001 | train 0.09894 | val 0.01108 | noisy 0.01385 | Δ +20.0% | LR 0.00300 | ||ψ-φ|| 0.489\n",
      "[Stage3] L=3 ep 002 | train 0.08792 | val 0.01052 | noisy 0.01385 | Δ +24.0% | LR 0.00300 | ||ψ-φ|| 0.630\n",
      "[Stage3] L=3 ep 003 | train 0.09091 | val 0.01039 | noisy 0.01385 | Δ +25.0% | LR 0.00300 | ||ψ-φ|| 0.720\n",
      "[Stage3] L=3 ep 004 | train 0.09410 | val 0.01021 | noisy 0.01385 | Δ +26.2% | LR 0.00300 | ||ψ-φ|| 0.823\n",
      "[Stage3] L=3 ep 005 | train 0.09048 | val 0.01025 | noisy 0.01385 | Δ +25.9% | LR 0.00300 | ||ψ-φ|| 0.900\n",
      "[Stage3] L=3 ep 006 | train 0.09176 | val 0.01027 | noisy 0.01385 | Δ +25.8% | LR 0.00300 | ||ψ-φ|| 0.972\n",
      "[Stage3] L=3 ep 007 | train 0.07741 | val 0.01024 | noisy 0.01385 | Δ +26.0% | LR 0.00300 | ||ψ-φ|| 0.985\n",
      "[Stage3] L=3 ep 008 | train 0.09237 | val 0.01024 | noisy 0.01385 | Δ +26.1% | LR 0.00300 | ||ψ-φ|| 1.014\n",
      "[Stage3] L=3 ep 009 | train 0.09309 | val 0.01020 | noisy 0.01385 | Δ +26.3% | LR 0.00300 | ||ψ-φ|| 1.052\n",
      "[Stage3] L=3 ep 010 | train 0.09053 | val 0.01019 | noisy 0.01385 | Δ +26.4% | LR 0.00300 | ||ψ-φ|| 1.064\n",
      "[Stage3] L=3 ep 011 | train 0.08949 | val 0.01007 | noisy 0.01385 | Δ +27.3% | LR 0.00300 | ||ψ-φ|| 1.081\n",
      "[Stage3] L=3 ep 012 | train 0.08935 | val 0.01018 | noisy 0.01385 | Δ +26.5% | LR 0.00300 | ||ψ-φ|| 1.071\n",
      "[Stage3] L=3 ep 013 | train 0.09757 | val 0.01015 | noisy 0.01385 | Δ +26.7% | LR 0.00300 | ||ψ-φ|| 1.079\n",
      "[Stage3] L=3 ep 014 | train 0.09138 | val 0.01018 | noisy 0.01385 | Δ +26.5% | LR 0.00300 | ||ψ-φ|| 1.106\n",
      "[Stage3] L=3 ep 015 | train 0.09782 | val 0.01012 | noisy 0.01385 | Δ +26.9% | LR 0.00300 | ||ψ-φ|| 1.136\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 016 | train 0.09288 | val 0.01015 | noisy 0.01385 | Δ +26.7% | LR 0.00150 | ||ψ-φ|| 1.166\n",
      "[Stage3] L=3 ep 017 | train 0.09106 | val 0.01014 | noisy 0.01385 | Δ +26.7% | LR 0.00150 | ||ψ-φ|| 1.181\n",
      "[Stage3] L=3 ep 018 | train 0.08468 | val 0.01008 | noisy 0.01385 | Δ +27.2% | LR 0.00150 | ||ψ-φ|| 1.178\n",
      "[Stage3] L=3 ep 019 | train 0.09059 | val 0.01004 | noisy 0.01385 | Δ +27.5% | LR 0.00150 | ||ψ-φ|| 1.164\n",
      "[Stage3] L=3 ep 020 | train 0.10451 | val 0.01002 | noisy 0.01385 | Δ +27.6% | LR 0.00150 | ||ψ-φ|| 1.158\n",
      "[Stage3] L=3 ep 021 | train 0.09471 | val 0.01007 | noisy 0.01385 | Δ +27.2% | LR 0.00150 | ||ψ-φ|| 1.149\n",
      "[Stage3] L=3 ep 022 | train 0.09734 | val 0.01011 | noisy 0.01385 | Δ +27.0% | LR 0.00150 | ||ψ-φ|| 1.149\n",
      "[Stage3] L=3 ep 023 | train 0.08651 | val 0.01007 | noisy 0.01385 | Δ +27.2% | LR 0.00150 | ||ψ-φ|| 1.149\n",
      "[Stage3] L=3 ep 024 | train 0.09604 | val 0.01006 | noisy 0.01385 | Δ +27.3% | LR 0.00150 | ||ψ-φ|| 1.161\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 025 | train 0.09117 | val 0.01006 | noisy 0.01385 | Δ +27.4% | LR 0.00075 | ||ψ-φ|| 1.171\n",
      "[Stage3] L=3 ep 026 | train 0.08713 | val 0.01007 | noisy 0.01385 | Δ +27.3% | LR 0.00075 | ||ψ-φ|| 1.165\n",
      "[Stage3] L=3 ep 027 | train 0.08388 | val 0.01006 | noisy 0.01385 | Δ +27.4% | LR 0.00075 | ||ψ-φ|| 1.164\n",
      "[Stage3] L=3 ep 028 | train 0.09196 | val 0.01005 | noisy 0.01385 | Δ +27.4% | LR 0.00075 | ||ψ-φ|| 1.165\n",
      "[Stage3] L=3 ep 029 | train 0.09415 | val 0.01008 | noisy 0.01385 | Δ +27.2% | LR 0.00075 | ||ψ-φ|| 1.164\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=3 ep 030 | train 0.09300 | val 0.01009 | noisy 0.01385 | Δ +27.1% | LR 0.00038 | ||ψ-φ|| 1.173\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.452843 | val 0.441729 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.423630 | val 0.411817 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.396296 | val 0.385120 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.373551 | val 0.362129 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.351260 | val 0.343016 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.336801 | val 0.327557 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.320315 | val 0.314172 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.307857 | val 0.301942 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.298136 | val 0.291217 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.283895 | val 0.281929 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.273021 | val 0.273649 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.262665 | val 0.265628 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.256542 | val 0.257474 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.244946 | val 0.248937 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.236955 | val 0.239845 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.228331 | val 0.230516 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.216602 | val 0.221192 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.208259 | val 0.212043 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.203126 | val 0.203241 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.196705 | val 0.194937 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.186837 | val 0.187252 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.179385 | val 0.180102 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.172236 | val 0.173653 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.165371 | val 0.167597 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.167330 | val 0.162302 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.160604 | val 0.157803 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.155128 | val 0.153936 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.153018 | val 0.150635 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.149217 | val 0.147848 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.144976 | val 0.145462 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.147492 | val 0.143709 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.143118 | val 0.142191 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.143715 | val 0.140870 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.146924 | val 0.139779 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.137926 | val 0.138621 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.139763 | val 0.137440 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.138836 | val 0.136535 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.138331 | val 0.135754 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.137734 | val 0.134957 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.136865 | val 0.134188 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.134134 | val 0.133258 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.133781 | val 0.132284 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.133577 | val 0.131320 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.130094 | val 0.130404 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.131398 | val 0.129292 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.132390 | val 0.128137 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.132712 | val 0.126922 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.129370 | val 0.125684 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.129973 | val 0.124330 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.124908 | val 0.123234 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.126367 | val 0.121964 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.125036 | val 0.120925 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.125851 | val 0.119841 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.120362 | val 0.119050 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.122941 | val 0.118148 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.123283 | val 0.117428 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.120154 | val 0.116770 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.117354 | val 0.116318 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.119950 | val 0.116142 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.120204 | val 0.115948 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.117190 | val 0.115640 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.115062 | val 0.115114 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.118212 | val 0.114652 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.116592 | val 0.114479 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.117233 | val 0.114088 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.118686 | val 0.113963 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.116566 | val 0.113775 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.115862 | val 0.113710 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.116347 | val 0.113613 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.115859 | val 0.113330 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.113732 | val 0.112857 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.115783 | val 0.112549 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.116932 | val 0.112562 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.115013 | val 0.112516 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.115247 | val 0.112395 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.114440 | val 0.112358 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.114186 | val 0.112316 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.115852 | val 0.112076 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.115219 | val 0.112094 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.116737 | val 0.111977 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.115747 | val 0.111969 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.117758 | val 0.111652 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.113396 | val 0.111518 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.115035 | val 0.111570 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.112902 | val 0.111630 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.115621 | val 0.111655 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.114094 | val 0.111664 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.113895 | val 0.111301 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.116677 | val 0.111505 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.111902 | val 0.111775 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.115037 | val 0.111718 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.113509 | val 0.111495 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.114109 | val 0.111181 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.115518 | val 0.110965 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.112834 | val 0.111001 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.114260 | val 0.111055 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.113366 | val 0.111058 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.113721 | val 0.111153 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.110390 | val 0.111166 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.113875 | val 0.111042 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.115324 | val 0.111081 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=3 ep 101 | train 0.115807 | val 0.111260 | LR 0.00500\n",
      "[Stage1] L=3 ep 102 | train 0.115295 | val 0.111272 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=3 ep 000 | train 0.10083 | val 0.01200 | noisy 0.01385 | Δ +13.3% | LR 0.00300 | ||ψ-φ|| 0.352\n",
      "[Stage3] L=3 ep 001 | train 0.10565 | val 0.01115 | noisy 0.01385 | Δ +19.5% | LR 0.00300 | ||ψ-φ|| 0.495\n",
      "[Stage3] L=3 ep 002 | train 0.10296 | val 0.01072 | noisy 0.01385 | Δ +22.6% | LR 0.00300 | ||ψ-φ|| 0.618\n",
      "[Stage3] L=3 ep 003 | train 0.10344 | val 0.01037 | noisy 0.01385 | Δ +25.1% | LR 0.00300 | ||ψ-φ|| 0.714\n",
      "[Stage3] L=3 ep 004 | train 0.10341 | val 0.01017 | noisy 0.01385 | Δ +26.5% | LR 0.00300 | ||ψ-φ|| 0.795\n",
      "[Stage3] L=3 ep 005 | train 0.11419 | val 0.00998 | noisy 0.01385 | Δ +27.9% | LR 0.00300 | ||ψ-φ|| 0.884\n",
      "[Stage3] L=3 ep 006 | train 0.10125 | val 0.00999 | noisy 0.01385 | Δ +27.8% | LR 0.00300 | ||ψ-φ|| 0.934\n",
      "[Stage3] L=3 ep 007 | train 0.11552 | val 0.00975 | noisy 0.01385 | Δ +29.6% | LR 0.00300 | ||ψ-φ|| 1.001\n",
      "[Stage3] L=3 ep 008 | train 0.10423 | val 0.00951 | noisy 0.01385 | Δ +31.4% | LR 0.00300 | ||ψ-φ|| 1.067\n",
      "[Stage3] L=3 ep 009 | train 0.10421 | val 0.00943 | noisy 0.01385 | Δ +31.9% | LR 0.00300 | ||ψ-φ|| 1.126\n",
      "[Stage3] L=3 ep 010 | train 0.09849 | val 0.00939 | noisy 0.01385 | Δ +32.2% | LR 0.00300 | ||ψ-φ|| 1.152\n",
      "[Stage3] L=3 ep 011 | train 0.10873 | val 0.00934 | noisy 0.01385 | Δ +32.6% | LR 0.00300 | ||ψ-φ|| 1.179\n",
      "[Stage3] L=3 ep 012 | train 0.10640 | val 0.00933 | noisy 0.01385 | Δ +32.6% | LR 0.00300 | ||ψ-φ|| 1.216\n",
      "[Stage3] L=3 ep 013 | train 0.10265 | val 0.00934 | noisy 0.01385 | Δ +32.5% | LR 0.00300 | ||ψ-φ|| 1.222\n",
      "[Stage3] L=3 ep 014 | train 0.09830 | val 0.00935 | noisy 0.01385 | Δ +32.5% | LR 0.00300 | ||ψ-φ|| 1.231\n",
      "[Stage3] L=3 ep 015 | train 0.10752 | val 0.00933 | noisy 0.01385 | Δ +32.6% | LR 0.00300 | ||ψ-φ|| 1.254\n",
      "[Stage3] L=3 ep 016 | train 0.10432 | val 0.00932 | noisy 0.01385 | Δ +32.7% | LR 0.00300 | ||ψ-φ|| 1.245\n",
      "[Stage3] L=3 ep 017 | train 0.10242 | val 0.00934 | noisy 0.01385 | Δ +32.5% | LR 0.00300 | ||ψ-φ|| 1.242\n",
      "[Stage3] L=3 ep 018 | train 0.10224 | val 0.00938 | noisy 0.01385 | Δ +32.3% | LR 0.00300 | ||ψ-φ|| 1.250\n",
      "[Stage3] L=3 ep 019 | train 0.09582 | val 0.00930 | noisy 0.01385 | Δ +32.8% | LR 0.00300 | ||ψ-φ|| 1.253\n",
      "[Stage3] L=3 ep 020 | train 0.10568 | val 0.00933 | noisy 0.01385 | Δ +32.6% | LR 0.00300 | ||ψ-φ|| 1.258\n",
      "[Stage3] L=3 ep 021 | train 0.10550 | val 0.00929 | noisy 0.01385 | Δ +32.9% | LR 0.00300 | ||ψ-φ|| 1.254\n",
      "[Stage3] L=3 ep 022 | train 0.10249 | val 0.00925 | noisy 0.01385 | Δ +33.2% | LR 0.00300 | ||ψ-φ|| 1.251\n",
      "[Stage3] L=3 ep 023 | train 0.10194 | val 0.00927 | noisy 0.01385 | Δ +33.1% | LR 0.00300 | ||ψ-φ|| 1.255\n",
      "[Stage3] L=3 ep 024 | train 0.10029 | val 0.00925 | noisy 0.01385 | Δ +33.2% | LR 0.00300 | ||ψ-φ|| 1.270\n",
      "[Stage3] L=3 ep 025 | train 0.10241 | val 0.00936 | noisy 0.01385 | Δ +32.4% | LR 0.00300 | ||ψ-φ|| 1.273\n",
      "[Stage3] L=3 ep 026 | train 0.10205 | val 0.00936 | noisy 0.01385 | Δ +32.4% | LR 0.00300 | ||ψ-φ|| 1.271\n",
      "[Stage3] L=3 ep 027 | train 0.10115 | val 0.00938 | noisy 0.01385 | Δ +32.3% | LR 0.00300 | ||ψ-φ|| 1.256\n",
      "[Stage3] L=3 ep 028 | train 0.10985 | val 0.00930 | noisy 0.01385 | Δ +32.8% | LR 0.00300 | ||ψ-φ|| 1.259\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 029 | train 0.10879 | val 0.00929 | noisy 0.01385 | Δ +32.9% | LR 0.00150 | ||ψ-φ|| 1.244\n",
      "[Stage3] L=3 ep 030 | train 0.10352 | val 0.00931 | noisy 0.01385 | Δ +32.8% | LR 0.00150 | ||ψ-φ|| 1.242\n",
      "[Stage3] L=3 ep 031 | train 0.10087 | val 0.00932 | noisy 0.01385 | Δ +32.7% | LR 0.00150 | ||ψ-φ|| 1.241\n",
      "[Stage3] L=3 ep 032 | train 0.10503 | val 0.00931 | noisy 0.01385 | Δ +32.8% | LR 0.00150 | ||ψ-φ|| 1.236\n",
      "[Stage3] L=3 ep 033 | train 0.09934 | val 0.00929 | noisy 0.01385 | Δ +32.9% | LR 0.00150 | ||ψ-φ|| 1.232\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 034 | train 0.10337 | val 0.00926 | noisy 0.01385 | Δ +33.1% | LR 0.00075 | ||ψ-φ|| 1.227\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.545602 | val 0.535747 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.519056 | val 0.511322 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.491669 | val 0.485259 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.460651 | val 0.458098 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.430933 | val 0.431248 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.407103 | val 0.406805 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.380297 | val 0.385850 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.359821 | val 0.367742 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.342919 | val 0.351436 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.331214 | val 0.335942 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.317116 | val 0.321455 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.304299 | val 0.308121 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.287176 | val 0.296126 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.276898 | val 0.285576 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.267209 | val 0.275794 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.252454 | val 0.266667 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.246033 | val 0.257710 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.238343 | val 0.248729 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.230259 | val 0.239432 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.222790 | val 0.230260 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.216697 | val 0.220790 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.207321 | val 0.211591 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.200260 | val 0.202755 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.191127 | val 0.194450 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.185853 | val 0.186135 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.180390 | val 0.178032 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.174455 | val 0.170704 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.167267 | val 0.163794 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.167000 | val 0.157723 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.164802 | val 0.151912 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.155461 | val 0.146847 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.154239 | val 0.142343 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.151762 | val 0.138116 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.151565 | val 0.133918 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.146936 | val 0.130229 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.142789 | val 0.127567 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.138828 | val 0.124525 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.138099 | val 0.121826 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.135736 | val 0.119720 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.135070 | val 0.117554 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.133723 | val 0.115623 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.129943 | val 0.113729 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.128205 | val 0.111912 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.129113 | val 0.110286 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.127826 | val 0.108411 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.124423 | val 0.107159 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.125788 | val 0.105815 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.124503 | val 0.104488 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.118880 | val 0.103303 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.119566 | val 0.102098 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.117639 | val 0.100668 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.115982 | val 0.099701 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.113505 | val 0.098516 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.111090 | val 0.097083 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.109569 | val 0.095997 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.108173 | val 0.094439 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.105954 | val 0.093062 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.100531 | val 0.091459 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.103359 | val 0.089681 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.099011 | val 0.088559 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.096476 | val 0.087234 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.094686 | val 0.085433 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.091535 | val 0.082794 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.089430 | val 0.080112 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.087868 | val 0.077639 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.085421 | val 0.076411 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.080928 | val 0.074138 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.079175 | val 0.071601 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.077845 | val 0.069322 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.074188 | val 0.067045 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.072839 | val 0.065312 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.071306 | val 0.063924 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.069367 | val 0.062131 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.066515 | val 0.060480 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.064667 | val 0.059078 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.064609 | val 0.057940 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.062667 | val 0.056532 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.062187 | val 0.055448 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.061609 | val 0.054566 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.059624 | val 0.053672 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.059048 | val 0.053076 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.058555 | val 0.052744 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.058145 | val 0.052290 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.057875 | val 0.052051 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.056951 | val 0.051413 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.056803 | val 0.051084 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.056314 | val 0.051090 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.056463 | val 0.051028 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.056077 | val 0.050628 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.055677 | val 0.050634 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.055475 | val 0.050770 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.055418 | val 0.050627 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.055674 | val 0.050719 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.055358 | val 0.050721 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.054758 | val 0.050528 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.054101 | val 0.050357 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.053910 | val 0.050405 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.055078 | val 0.050234 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.054599 | val 0.050098 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.054022 | val 0.050084 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.053975 | val 0.050267 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.055257 | val 0.050385 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.053880 | val 0.050378 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.054969 | val 0.050437 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.054487 | val 0.050302 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.054800 | val 0.050306 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.054852 | val 0.050506 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=3 ep 107 | train 0.054412 | val 0.050447 | LR 0.00500\n",
      "[Stage1] L=3 ep 108 | train 0.054675 | val 0.050644 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=3 ep 000 | train 0.08719 | val 0.01156 | noisy 0.01385 | Δ +16.5% | LR 0.00300 | ||ψ-φ|| 0.320\n",
      "[Stage3] L=3 ep 001 | train 0.08336 | val 0.01062 | noisy 0.01385 | Δ +23.3% | LR 0.00300 | ||ψ-φ|| 0.452\n",
      "[Stage3] L=3 ep 002 | train 0.08489 | val 0.01010 | noisy 0.01385 | Δ +27.1% | LR 0.00300 | ||ψ-φ|| 0.614\n",
      "[Stage3] L=3 ep 003 | train 0.08119 | val 0.01000 | noisy 0.01385 | Δ +27.8% | LR 0.00300 | ||ψ-φ|| 0.725\n",
      "[Stage3] L=3 ep 004 | train 0.08719 | val 0.00999 | noisy 0.01385 | Δ +27.9% | LR 0.00300 | ||ψ-φ|| 0.856\n",
      "[Stage3] L=3 ep 005 | train 0.09166 | val 0.00992 | noisy 0.01385 | Δ +28.4% | LR 0.00300 | ||ψ-φ|| 0.958\n",
      "[Stage3] L=3 ep 006 | train 0.09102 | val 0.00979 | noisy 0.01385 | Δ +29.3% | LR 0.00300 | ||ψ-φ|| 1.031\n",
      "[Stage3] L=3 ep 007 | train 0.08458 | val 0.00986 | noisy 0.01385 | Δ +28.8% | LR 0.00300 | ||ψ-φ|| 1.064\n",
      "[Stage3] L=3 ep 008 | train 0.08195 | val 0.00997 | noisy 0.01385 | Δ +28.0% | LR 0.00300 | ||ψ-φ|| 1.075\n",
      "[Stage3] L=3 ep 009 | train 0.08338 | val 0.00989 | noisy 0.01385 | Δ +28.6% | LR 0.00300 | ||ψ-φ|| 1.118\n",
      "[Stage3] L=3 ep 010 | train 0.08655 | val 0.00997 | noisy 0.01385 | Δ +28.0% | LR 0.00300 | ||ψ-φ|| 1.145\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 011 | train 0.08076 | val 0.00988 | noisy 0.01385 | Δ +28.6% | LR 0.00150 | ||ψ-φ|| 1.150\n",
      "[Stage3] L=3 ep 012 | train 0.08225 | val 0.00986 | noisy 0.01385 | Δ +28.8% | LR 0.00150 | ||ψ-φ|| 1.146\n",
      "[Stage3] L=3 ep 013 | train 0.07318 | val 0.00988 | noisy 0.01385 | Δ +28.6% | LR 0.00150 | ||ψ-φ|| 1.137\n",
      "[Stage3] L=3 ep 014 | train 0.08451 | val 0.00986 | noisy 0.01385 | Δ +28.8% | LR 0.00150 | ||ψ-φ|| 1.135\n",
      "[Stage3] L=3 ep 015 | train 0.07935 | val 0.00992 | noisy 0.01385 | Δ +28.4% | LR 0.00150 | ||ψ-φ|| 1.128\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 016 | train 0.08736 | val 0.00985 | noisy 0.01385 | Δ +28.8% | LR 0.00075 | ||ψ-φ|| 1.134\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "Completed 10 runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7 — Train runs (instances × layers)\n",
    "# ============================================\n",
    "RUNS = []  # we’ll save each run in the next cell\n",
    "\n",
    "for L in LAYER_OPTIONS:\n",
    "    for inst in INSTANCE_IDS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Instance {inst} | Layers {L}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        s1 = train_stage1(\n",
    "            X_train, X_val,\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=120, batch_size=32,\n",
    "            lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        s3 = train_stage3(\n",
    "            X_train, X_val,\n",
    "            phi_stage1=s1[\"phi\"],\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16,\n",
    "            LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "            CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        RUNS.append({\n",
    "            \"instance_id\": inst,\n",
    "            \"n_layers\": L,\n",
    "            \"stage1\": {\n",
    "                \"phi\": s1[\"phi\"],\n",
    "                \"best_val\": s1[\"best_val\"],\n",
    "                \"hist_train\": s1[\"hist_train\"],\n",
    "                \"hist_val\": s1[\"hist_val\"],\n",
    "                \"hist_lr\": s1[\"hist_lr\"],\n",
    "                \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                \"epochs\": s1.get(\"epochs\"),\n",
    "                \"train_seconds\": float(t1 - t0),\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"psi\": s3[\"psi\"],\n",
    "                \"best_val\": s3[\"best_val\"],\n",
    "                \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                \"epochs\": s3.get(\"epochs\"),\n",
    "                \"hist_train\": s3[\"hist_train\"],\n",
    "                \"hist_val\": s3[\"hist_val\"],\n",
    "                # NEW: capture these so Cell 8 has them\n",
    "                \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": float(t2 - t1),\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c039220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "\n",
      "All runs saved and recorded.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 8 — Save artifacts (JSON) and append a paper-ready CSV per run\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams logged (keep in sync with training cells) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.003\n",
    "S3_MAX_EPOCHS    = 60\n",
    "S3_PATIENCE      = 10\n",
    "S3_PLATEAU_STEPS = 5\n",
    "S3_PLATEAU_FACT  = 0.5\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v2\"  # bump if you change columns\n",
    "\n",
    "# --- ensure dirs ---\n",
    "ensure_dir(OUT_BASE)\n",
    "subroot = ensure_dir(f\"{OUT_BASE}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "\n",
    "# --- CSV path (versioned) ---\n",
    "CSV_PATH = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "\n",
    "# --- header for the full, paper-friendly table ---\n",
    "CSV_HEADER = [\n",
    "    # id / naming\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    # architecture\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    # noise & window\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    # stage-1 hyperparams + outcomes\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    # stage-3 hyperparams + outcomes\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\"s3_patience\",\"s3_plateau_steps\",\"s3_plateau_factor\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    # params (JSON)\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    # totals\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    needs_header = True\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                first_line = f.readline().rstrip(\"\\n\")\n",
    "            needs_header = (first_line != \",\".join(header))\n",
    "        except Exception:\n",
    "            needs_header = True\n",
    "    if needs_header:\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(header)\n",
    "\n",
    "ensure_csv(CSV_PATH, CSV_HEADER)\n",
    "\n",
    "# -------------------------\n",
    "# Robust helper utilities\n",
    "# -------------------------\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def _safe_int(x, default):\n",
    "    \"\"\"Cast to int, handling None/NaN/invalid gracefully.\"\"\"\n",
    "    try:\n",
    "        if x is None:\n",
    "            return int(default)\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return int(default)\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return int(default)\n",
    "\n",
    "def _safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return float(default)\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "# Fallback filename formatter if not provided elsewhere\n",
    "if \"std_instance_name\" not in globals():\n",
    "    def std_instance_name(nq, nl, nt, L, inst):\n",
    "        return f\"{nq}q_{nl}l_{nt}t_L{L}_inst{inst:02d}.json\"\n",
    "\n",
    "# -------------------------\n",
    "# Saver\n",
    "# -------------------------\n",
    "def save_one_run(run):\n",
    "    global stage3_handles  # used if we need to rebuild QNodes for fallback eval\n",
    "\n",
    "    # Architecture context\n",
    "    nq, nl, nt = int(n_qubits), int(n_latent), int(n_trash)\n",
    "\n",
    "    # Try to grab dataset folder/name from globals or default\n",
    "    dataset_folder = globals().get(\"data_folder\", None)\n",
    "    if dataset_folder is None:\n",
    "        dataset_folder = globals().get(\"dataset_folder\", \"unknown_dataset\")\n",
    "\n",
    "    # Try to get scaling info; default to [0,1] if missing\n",
    "    _info = globals().get(\"info\", {}) or {}\n",
    "    scale_low  = float(_info.get(\"scale_low\", 0.0))\n",
    "    scale_high = float(_info.get(\"scale_high\", 1.0))\n",
    "\n",
    "    # Paths & file names\n",
    "    inst = _safe_int(run.get(\"instance_id\"), 0)\n",
    "    L    = _safe_int(run.get(\"n_layers\"), 0)\n",
    "    seed = _safe_int(run.get(\"seed\", inst), inst)\n",
    "\n",
    "    fname = std_instance_name(nq, nl, nt, L, inst)\n",
    "    out_dir = ensure_dir(f\"{subroot}/L{L}\")\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    # Stage dicts\n",
    "    s1 = run.get(\"stage1\", {})\n",
    "    s3 = run.get(\"stage3\", {})\n",
    "\n",
    "    # Stage-1 metrics\n",
    "    s1_hist_val   = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val   = _safe_float(s1.get(\"best_val\"), np.nan)\n",
    "    s1_final_val  = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = _safe_int(s1.get(\"best_epoch\"), _safe_argmin(s1_hist_val))\n",
    "    s1_epochs     = _safe_int(s1.get(\"epochs\"), len(s1_hist_val))\n",
    "    s1_seconds    = _safe_float(s1.get(\"train_seconds\"), np.nan)\n",
    "\n",
    "    # Stage-3 series\n",
    "    s3_hist_val   = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "\n",
    "    s3_best_val   = _safe_float(s3.get(\"best_val\"), np.nan)\n",
    "    s3_final_val  = _safe_last(s3_hist_val)\n",
    "\n",
    "    s3_best_epoch = _safe_int(s3.get(\"best_epoch\"), _safe_argmin(s3_hist_val))\n",
    "    s3_epochs     = _safe_int(s3.get(\"epochs\"), len(s3_hist_val))\n",
    "    s3_seconds    = _safe_float(s3.get(\"train_seconds\"), np.nan)\n",
    "\n",
    "    # --- compute metrics with FALLBACKS if curves are missing ---\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = (float(np.nanmax(s3_hist_delta)) if (len(s3_hist_delta) and np.isfinite(np.nanmax(s3_hist_delta)))\n",
    "                      else np.nan)\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    need_fallback = (not len(s3_hist_noisy)) or (not np.isfinite(noisy_baseline)) or (not np.isfinite(final_delta))\n",
    "\n",
    "    if need_fallback:\n",
    "        # Only attempt fallback if we have the necessary globals and arrays\n",
    "        phi_for_L = np.array(s1.get(\"phi\", []))\n",
    "        psi_params = np.array(s3.get(\"psi\", []))\n",
    "        if phi_for_L.size and psi_params.size and (\"stage3_qnodes\" in globals()) and (\"stage3_val_values_det\" in globals()):\n",
    "            # Rebuild the QNodes for this (L, phi) so we can evaluate psi on X_val\n",
    "            stage3_handles = stage3_qnodes(L, phi_for_L)  # sets the fixed decoder from φ\n",
    "            # X_val must exist for deterministic evaluation\n",
    "            Xv = globals().get(\"X_val\", None)\n",
    "            if Xv is not None:\n",
    "                mN, mD, d_pct = stage3_val_values_det(psi_params, Xv, sigma=globals().get(\"EVAL_SIGMA\", 0.20))\n",
    "                noisy_baseline = float(mN)\n",
    "                final_delta    = float(d_pct)\n",
    "                if not np.isfinite(best_delta):  # if we don't have a curve, use final as best\n",
    "                    best_delta = final_delta\n",
    "\n",
    "    # bundle JSON (parameters + training curves)\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"id\": dataset_folder,\n",
    "            \"scale_low\":  scale_low,\n",
    "            \"scale_high\": scale_high,\n",
    "            \"window_size\": int(nq),\n",
    "            \"window_stride\": int(globals().get(\"WINDOW_STRIDE\", 1)),\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(globals().get(\"EVAL_SIGMA\", np.nan)),\n",
    "            \"sigma_eval\":  float(globals().get(\"EVAL_SIGMA\", np.nan)),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(nq),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(nl),\n",
    "            \"n_trash\":  int(nt),\n",
    "            \"latent_wires\": list(range(int(nl))),\n",
    "            \"trash_wires\":  list(range(int(nl), int(nq))),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"patience\": S3_PATIENCE, \"plateau_steps\": S3_PLATEAU_STEPS, \"plateau_factor\": S3_PLATEAU_FACT,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val,\n",
    "                \"noisy_curve\": s3.get(\"hist_noisy\", []), \"delta_curve\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # assemble CSV row\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        dataset_folder,\n",
    "        inst, seed,\n",
    "        int(nq), int(nl), int(nt), int(L),\n",
    "        f\"{globals().get('EVAL_SIGMA', np.nan):.3f}\", f\"{globals().get('EVAL_SIGMA', np.nan):.3f}\", int(globals().get(\"WINDOW_STRIDE\", 1)),\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS), int(S3_PATIENCE), int(S3_PLATEAU_STEPS), f\"{S3_PLATEAU_FACT:.3f}\",\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert row into CSV\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(CSV_PATH).exists():\n",
    "        df_old = pd.read_csv(CSV_PATH)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(CSV_PATH, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Upserted CSV row  → {CSV_PATH}\")\n",
    "\n",
    "# ---- save all runs from Cell 7 ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved and recorded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "137c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved per-layer summary → ./runs_halfqae/summary_by_layers_v2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4q_2l_2t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009964</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>64.834131</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>28.597628</td>\n",
       "      <td>28.038128</td>\n",
       "      <td>[-0.016312803262711447, -1.986552627047903, 0....</td>\n",
       "      <td>[0.004653935659031972, -2.0253661313694864, -0...</td>\n",
       "      <td>115.960609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4q_2l_2t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009799</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>72.580082</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>30.144129</td>\n",
       "      <td>29.231419</td>\n",
       "      <td>[0.011801558810262056, -1.9347410256049633, 0....</td>\n",
       "      <td>[0.02414285247762957, -2.0094330925634307, -0....</td>\n",
       "      <td>127.678969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4q_2l_2t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009744</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>116.465180</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>29.835796</td>\n",
       "      <td>29.629363</td>\n",
       "      <td>[0.00028094502693164604, 1.096912804007125, 0....</td>\n",
       "      <td>[0.015007652271541886, 1.0230249899218355, 1.3...</td>\n",
       "      <td>156.282630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4q_2l_2t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>105.541854</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>28.856028</td>\n",
       "      <td>28.519410</td>\n",
       "      <td>[0.03214672941676667, -2.0208978324444513, -0....</td>\n",
       "      <td>[-0.019716129742746204, -2.1269129615096083, 0...</td>\n",
       "      <td>155.306374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4q_2l_2t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>103.156757</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>29.760033</td>\n",
       "      <td>28.689862</td>\n",
       "      <td>[0.00033290752957749667, -1.9049135233050598, ...</td>\n",
       "      <td>[0.029558322431367485, -2.0536424211010313, -0...</td>\n",
       "      <td>150.174163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4q_2l_2t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009519</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>177.698407</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>32.522158</td>\n",
       "      <td>31.250727</td>\n",
       "      <td>[1.2278353250797251, -0.029796534519642336, -0...</td>\n",
       "      <td>[1.3993129974539795, -0.02034789510104483, -0....</td>\n",
       "      <td>336.080375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4q_2l_2t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>219.724816</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>35.453481</td>\n",
       "      <td>34.523729</td>\n",
       "      <td>[-0.0043693490827218075, -0.05240699121450067,...</td>\n",
       "      <td>[-0.004682019395181499, 0.011726924508950902, ...</td>\n",
       "      <td>385.865487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4q_2l_2t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>249.047306</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>27.629534</td>\n",
       "      <td>27.134850</td>\n",
       "      <td>[0.7862941374481595, 1.4073252568117587, 0.780...</td>\n",
       "      <td>[0.8407786622971607, 1.5477164981938103, 0.841...</td>\n",
       "      <td>491.122231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4q_2l_2t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009260</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>277.540700</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>33.191473</td>\n",
       "      <td>33.121094</td>\n",
       "      <td>[0.007009495854836537, -0.027214479629045413, ...</td>\n",
       "      <td>[0.038188889968790274, -0.05859270351253336, 1...</td>\n",
       "      <td>503.708784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4q_2l_2t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>mackey_glass_tau30_n200</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009853</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>135.293316</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>29.264122</td>\n",
       "      <td>28.840395</td>\n",
       "      <td>[1.5714525489700095, -0.008458921277344641, 1....</td>\n",
       "      <td>[1.5928106508978992, -0.0319637609603955, 1.46...</td>\n",
       "      <td>368.880210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag           dataset_folder  instance_id  \\\n",
       "0  4q_2l_2t_1ls_01.json  inst1_L1  mackey_glass_tau30_n200            1   \n",
       "1  4q_2l_2t_1ls_02.json  inst2_L1  mackey_glass_tau30_n200            2   \n",
       "2  4q_2l_2t_1ls_03.json  inst3_L1  mackey_glass_tau30_n200            3   \n",
       "3  4q_2l_2t_1ls_04.json  inst4_L1  mackey_glass_tau30_n200            4   \n",
       "4  4q_2l_2t_1ls_05.json  inst5_L1  mackey_glass_tau30_n200            5   \n",
       "5  4q_2l_2t_3ls_01.json  inst1_L3  mackey_glass_tau30_n200            1   \n",
       "6  4q_2l_2t_3ls_02.json  inst2_L3  mackey_glass_tau30_n200            2   \n",
       "7  4q_2l_2t_3ls_03.json  inst3_L3  mackey_glass_tau30_n200            3   \n",
       "8  4q_2l_2t_3ls_04.json  inst4_L3  mackey_glass_tau30_n200            4   \n",
       "9  4q_2l_2t_3ls_05.json  inst5_L3  mackey_glass_tau30_n200            5   \n",
       "\n",
       "   rng_seed  n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  \\\n",
       "0         1         4         2        2         1          0.2  ...   \n",
       "1         2         4         2        2         1          0.2  ...   \n",
       "2         3         4         2        2         1          0.2  ...   \n",
       "3         4         4         2        2         1          0.2  ...   \n",
       "4         5         4         2        2         1          0.2  ...   \n",
       "5         1         4         2        2         3          0.2  ...   \n",
       "6         2         4         2        2         3          0.2  ...   \n",
       "7         3         4         2        2         3          0.2  ...   \n",
       "8         4         4         2        2         3          0.2  ...   \n",
       "9         5         4         2        2         3          0.2  ...   \n",
       "\n",
       "   s3_final_val_mse  s3_best_epoch  s3_epochs  s3_train_seconds  \\\n",
       "0          0.009964             10         21         64.834131   \n",
       "1          0.009799             12         23         72.580082   \n",
       "2          0.009744             27         38        116.465180   \n",
       "3          0.009898             23         34        105.541854   \n",
       "4          0.009874             21         32        103.156757   \n",
       "5          0.009519             12         23        177.698407   \n",
       "6          0.009066             16         27        219.724816   \n",
       "7          0.010089             20         31        249.047306   \n",
       "8          0.009260             24         35        277.540700   \n",
       "9          0.009853              6         17        135.293316   \n",
       "\n",
       "   s3_noisy_baseline_mse  s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0               0.013847          28.597628           28.038128   \n",
       "1               0.013847          30.144129           29.231419   \n",
       "2               0.013847          29.835796           29.629363   \n",
       "3               0.013847          28.856028           28.519410   \n",
       "4               0.013847          29.760033           28.689862   \n",
       "5               0.013847          32.522158           31.250727   \n",
       "6               0.013847          35.453481           34.523729   \n",
       "7               0.013847          27.629534           27.134850   \n",
       "8               0.013847          33.191473           33.121094   \n",
       "9               0.013847          29.264122           28.840395   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [-0.016312803262711447, -1.986552627047903, 0....   \n",
       "1  [0.011801558810262056, -1.9347410256049633, 0....   \n",
       "2  [0.00028094502693164604, 1.096912804007125, 0....   \n",
       "3  [0.03214672941676667, -2.0208978324444513, -0....   \n",
       "4  [0.00033290752957749667, -1.9049135233050598, ...   \n",
       "5  [1.2278353250797251, -0.029796534519642336, -0...   \n",
       "6  [-0.0043693490827218075, -0.05240699121450067,...   \n",
       "7  [0.7862941374481595, 1.4073252568117587, 0.780...   \n",
       "8  [0.007009495854836537, -0.027214479629045413, ...   \n",
       "9  [1.5714525489700095, -0.008458921277344641, 1....   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [0.004653935659031972, -2.0253661313694864, -0...           115.960609  \n",
       "1  [0.02414285247762957, -2.0094330925634307, -0....           127.678969  \n",
       "2  [0.015007652271541886, 1.0230249899218355, 1.3...           156.282630  \n",
       "3  [-0.019716129742746204, -2.1269129615096083, 0...           155.306374  \n",
       "4  [0.029558322431367485, -2.0536424211010313, -0...           150.174163  \n",
       "5  [1.3993129974539795, -0.02034789510104483, -0....           336.080375  \n",
       "6  [-0.004682019395181499, 0.011726924508950902, ...           385.865487  \n",
       "7  [0.8407786622971607, 1.5477164981938103, 0.841...           491.122231  \n",
       "8  [0.038188889968790274, -0.05859270351253336, 1...           503.708784  \n",
       "9  [1.5928106508978992, -0.0319637609603955, 1.46...           368.880210  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.013847 ± 0.000000</td>\n",
       "      <td>0.009770 ± 0.000083</td>\n",
       "      <td>0.009856 ± 0.000077</td>\n",
       "      <td>29.438723 ± 0.600913</td>\n",
       "      <td>28.821636 ± 0.555506</td>\n",
       "      <td>0.268295 ± 0.004261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.013847 ± 0.000000</td>\n",
       "      <td>0.009469 ± 0.000389</td>\n",
       "      <td>0.009558 ± 0.000375</td>\n",
       "      <td>31.612154 ± 2.809009</td>\n",
       "      <td>30.974159 ± 2.704756</td>\n",
       "      <td>0.081151 ± 0.020465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.013847 ± 0.000000     0.009770 ± 0.000083   \n",
       "3            5           0.013847 ± 0.000000     0.009469 ± 0.000389   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.009856 ± 0.000077      29.438723 ± 0.600913   \n",
       "3             0.009558 ± 0.000375      31.612154 ± 2.809009   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1              28.821636 ± 0.555506    0.268295 ± 0.004261  \n",
       "3              30.974159 ± 2.704756    0.081151 ± 0.020465  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results table\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(CSV_PATH).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Run Cell 8 first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- NEW: drop duplicate runs; keep the newest copy (with baseline/delta)\n",
    "if \"filename\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "# Typical numeric casts (safe)\n",
    "for col in [\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\n",
    "    \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "    \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\"\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "\n",
    "clean_path = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "Path(OUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved training-only table → {clean_path}\")\n",
    "\n",
    "# A compact per-layer summary (mean±std); guards against all-NaN\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "metrics = [\n",
    "    (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "    (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "    (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "    (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "    (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "    (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "]\n",
    "\n",
    "grp = df.groupby(\"n_layers\", dropna=False)\n",
    "summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "for col, label in metrics:\n",
    "    if col in df.columns and np.isfinite(df[col]).any():\n",
    "        summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "summary_path = f\"{OUT_BASE}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "summary.to_csv(summary_path, index=True)\n",
    "print(f\"Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
