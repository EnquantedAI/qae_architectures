{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e979dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 0 — Experiment plan & seeds (GLOBAL)\n",
    "# ============================================\n",
    "# We'll run 5 instances (same across all notebooks) and two depths: 1 and 3 layers.\n",
    "INSTANCE_IDS   = [1, 2, 3, 4, 5]   # used in filenames as ..._ls_01.json, ..._ls_02.json, ...\n",
    "LAYER_OPTIONS  = [1, 3]            # train 1-layer first, then 3-layers\n",
    "EVAL_SIGMA     = 0.10              # fixed noise everywhere (train & eval)\n",
    "\n",
    "# where to save artifacts (JSON bundles, instance records, CSV summary)\n",
    "OUT_BASE = \"./runs_halfqae\"        # change if you like; subfolders will be created automatically\n",
    "CSV_PATH = f\"{OUT_BASE}/results_instances.csv\"  # will be appended-to if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458cfa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils import OK\n",
      "Seed/filename utils ready.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Cell 1 — Imports, utils, reproducibility (fixed seed)\n",
    "# =====================================================\n",
    "import os, sys, json, math, random, time, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- repo utils (your existing readers) -----\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "parent_dir = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'Jacob' else current_dir\n",
    "sys.path.insert(0, parent_dir); sys.path.insert(0, '../')\n",
    "try:\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Utils import OK\")\n",
    "except Exception as e:\n",
    "    print(\"Import error:\", e)\n",
    "    qae_utils_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(''))), 'qae_utils')\n",
    "    sys.path.insert(0, os.path.dirname(qae_utils_path))\n",
    "    from qae_utils.Files import read_ts_file, read_json_file\n",
    "    print(\"Absolute path fallback OK\")\n",
    "\n",
    "assert callable(read_ts_file) and callable(read_json_file)\n",
    "\n",
    "# ----- plotting defaults -----\n",
    "np.set_printoptions(suppress=True, precision=6)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4)\n",
    "\n",
    "# ----- reproducibility -----\n",
    "def set_global_seed(instance_id: int):\n",
    "    \"\"\"\n",
    "    Derive all RNGs from a simple instance ID (1..5).\n",
    "    Keep the mapping stable across notebooks.\n",
    "    \"\"\"\n",
    "    base = 10_000 + int(instance_id)  # simple, memorable\n",
    "    random.seed(base + 11)\n",
    "    np.random.seed(base + 22)\n",
    "    try:\n",
    "        pnp.random.seed(base + 33)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Pennylane's default.qubit is deterministic given params; no device seeding needed.\n",
    "    return dict(global_seed=base, numpy_seed=base+22, pnp_seed=base+33)\n",
    "\n",
    "def std_instance_name(nq, n_latent, n_trash, n_layers, instance_id):\n",
    "    \"\"\"\n",
    "    Standardized filename pattern used across the project.\n",
    "    Example: 4q_2l_2t_3ls_01.json\n",
    "    \"\"\"\n",
    "    return f\"{int(nq)}q_{int(n_latent)}l_{int(n_trash)}t_{int(n_layers)}ls_{int(instance_id):02d}.json\"\n",
    "\n",
    "def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "print(\"Seed/filename utils ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a103b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: mackey_glass_n100\n",
      "Loaded 100 samples; scale [0.200,0.800]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 2 — Data loading (deterministic)\n",
    "# =========================================\n",
    "DATA_PATH = '../jacobs_examples/aintern/data'\n",
    "DATA_NAME = 'mackey_glass'  # your folder prefix\n",
    "\n",
    "# fixed split across all instances/layers (so results are comparable)\n",
    "SPLIT_RANDOM_STATE = 42\n",
    "WINDOW_STRIDE = 1\n",
    "\n",
    "# ---- pick most recent MG folder starting with mackey_glass* ----\n",
    "data_folders = [f for f in os.listdir(DATA_PATH) if f.startswith(DATA_NAME)]\n",
    "if not data_folders:\n",
    "    raise FileNotFoundError(\"No Mackey-Glass data found. Generate it first.\")\n",
    "data_folders.sort()\n",
    "data_folder = data_folders[-1]     # take the last one if multiple\n",
    "print(f\"Using data folder: {data_folder}\")\n",
    "\n",
    "# ---- load series + scaling info ----\n",
    "X_idx = read_ts_file(f'{DATA_PATH}/{data_folder}/x_org.arr')   # indices for plotting, not used\n",
    "y_all = read_ts_file(f'{DATA_PATH}/{data_folder}/y_org.arr')   # values\n",
    "info  = read_json_file(f'{DATA_PATH}/{data_folder}/info.json')\n",
    "\n",
    "print(f\"Loaded {len(y_all)} samples; scale [{info['scale_low']:.3f},{info['scale_high']:.3f}]\")\n",
    "\n",
    "# ---- helper: uniform embed wrapper (works with/without explicit info param)\n",
    "def embed_input(x, info_=None):\n",
    "    \"\"\"\n",
    "    Map value-domain window x (in [lo,hi]) to RY(π·v01).\n",
    "    Accepts optional info to match Stage-3 call signatures.\n",
    "    \"\"\"\n",
    "    if info_ is None:\n",
    "        info_ = info\n",
    "    lo, hi = info_['scale_low'], info_['scale_high']\n",
    "    xn = (pnp.array(x) - lo) / max(hi - lo, 1e-12)   # -> [0,1]\n",
    "    for i, v in enumerate(xn):\n",
    "        qml.RY(v * pnp.pi, wires=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2caea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture set: 4q (2 latent and 2 trash).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — Architecture (do NOT change your brick/entanglers)\n",
    "# ============================================================\n",
    "# This notebook = 4 qubits, 2 latent, 2 trash.\n",
    "n_qubits = 4\n",
    "n_latent = 2\n",
    "n_trash  = n_qubits - n_latent\n",
    "trash_wires = [2, 3]            # your original choice\n",
    "signal_wires = list(range(4))   # Stage-1 diagnostics use all wires\n",
    "\n",
    "# --- device factory (simple; default.qubit) ---\n",
    "def make_device(nq): \n",
    "    return qml.device('default.qubit', wires=nq)\n",
    "\n",
    "# --- Stage-1 encoder template (unchanged architecture) ---\n",
    "def encoder_template(params, n_layers):\n",
    "    \"\"\"RX/RY/RZ per qubit + ring CNOT per layer.\"\"\"\n",
    "    assert len(params) == n_layers * n_qubits * 3\n",
    "    for l in range(n_layers):\n",
    "        # local rotations\n",
    "        for q in range(n_qubits):\n",
    "            idx = l * n_qubits * 3 + q * 3\n",
    "            qml.RX(params[idx + 0], wires=q)\n",
    "            qml.RY(params[idx + 1], wires=q)\n",
    "            qml.RZ(params[idx + 2], wires=q)\n",
    "        # ring entanglers\n",
    "        for q in range(n_qubits-1):\n",
    "            qml.CNOT(wires=[q, q+1])\n",
    "        qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "print(\"Architecture set: 4q (2 latent and 2 trash).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539c5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Cell 4 — Stage-1 QNodes, loss, and training (seeded)\n",
    "# ====================================================\n",
    "def stage1_qnodes(n_layers):\n",
    "    dev = make_device(n_qubits)\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def trash_expectations(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in trash_wires]\n",
    "\n",
    "    @qml.qnode(dev, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def recon_EdagE(phi, x_clean):\n",
    "        embed_input(x_clean)\n",
    "        encoder_template(phi, n_layers)\n",
    "        qml.adjoint(encoder_template)(phi, n_layers)   # E†\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return trash_expectations, recon_EdagE\n",
    "\n",
    "def stage1_batch_loss(trash_expectations, phi, clean_batch):\n",
    "    # L = mean_{batch,trash} P(|1>) = (1 - Z)/2\n",
    "    zs = []\n",
    "    for c in clean_batch:\n",
    "        z = pnp.array(trash_expectations(phi, c))  # shape (n_trash,)\n",
    "        zs.append(z)\n",
    "    zs = pnp.stack(zs, axis=0)\n",
    "    prob_one = (1.0 - zs) * 0.5\n",
    "    return pnp.mean(prob_one)\n",
    "\n",
    "def train_stage1(X_train, X_val, n_layers, instance_id, \n",
    "                 n_epochs=120, batch_size=32, lr_init=0.010,\n",
    "                 patience=10, lr_patience=8, min_delta=1e-6):\n",
    "    set_global_seed(instance_id)\n",
    "    # init\n",
    "    enc_shape = n_layers * n_qubits * 3\n",
    "    phi = pnp.array(np.random.normal(0, 0.5, enc_shape), requires_grad=True)\n",
    "    opt = qml.AdamOptimizer(stepsize=lr_init)\n",
    "    lr = lr_init\n",
    "\n",
    "    trash_expectations, recon_EdagE = stage1_qnodes(n_layers)\n",
    "\n",
    "    # helper\n",
    "    def minibatches(N, B, rng_seed=123456):\n",
    "        rng = np.random.default_rng(rng_seed)  # fixed per-epoch seed below\n",
    "        idx = rng.permutation(N)\n",
    "        for i in range(0, N, B):\n",
    "            yield idx[i:i+B]\n",
    "\n",
    "    train_hist, val_hist, lr_hist = [], [], []\n",
    "    best_phi, best_val = None, float(\"inf\")\n",
    "    no_improve = 0\n",
    "    for ep in range(n_epochs):\n",
    "        # batch order deterministic per-epoch per-instance\n",
    "        seed_ep = 77_000 + 100*instance_id + ep\n",
    "        acc = 0.0; nb = 0\n",
    "        for ix in minibatches(len(X_train), batch_size, rng_seed=seed_ep):\n",
    "            clean_batch = X_train[ix]\n",
    "            def loss_fn(p): return stage1_batch_loss(trash_expectations, p, clean_batch)\n",
    "            phi, cost = opt.step_and_cost(loss_fn, phi)\n",
    "            acc += float(cost); nb += 1\n",
    "        train_cost = acc / max(nb, 1)\n",
    "\n",
    "        # validation\n",
    "        v_costs = []\n",
    "        for c in X_val:\n",
    "            v_costs.append(stage1_batch_loss(trash_expectations, phi, pnp.array([c])))\n",
    "        val_cost = float(pnp.mean(pnp.stack(v_costs)))\n",
    "\n",
    "        train_hist.append(train_cost); val_hist.append(val_cost); lr_hist.append(lr)\n",
    "\n",
    "        if val_cost + min_delta < best_val:\n",
    "            best_val, best_phi = val_cost, pnp.array(phi, requires_grad=False); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % lr_patience) == 0:\n",
    "                lr = max(lr * 0.5, 1e-4)\n",
    "                opt = qml.AdamOptimizer(stepsize=lr)\n",
    "                print(f\"[Stage1] ↓ LR → {lr:.5f}\")\n",
    "            if no_improve >= patience:\n",
    "                print(\"[Stage1] Early stopping.\"); break\n",
    "\n",
    "        print(f\"[Stage1] L={n_layers} ep {ep:03d} | train {train_cost:.6f} | val {val_cost:.6f} | LR {lr:.5f}\")\n",
    "\n",
    "    phi_best = best_phi if best_phi is not None else phi\n",
    "    return dict(\n",
    "        phi=phi_best, best_val=float(best_val),\n",
    "        hist_train=list(map(float, train_hist)),\n",
    "        hist_val=list(map(float, val_hist)),\n",
    "        hist_lr=list(map(float, lr_hist)),\n",
    "        recon_EdagE=recon_EdagE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3943eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Cell 5 — Stage-3 (ψ) with fixed decoder = adjoint(Stage-1 encoder φ)\n",
    "#          (records hist_noisy, hist_delta, best_epoch, epochs, seconds)\n",
    "# ======================================================================\n",
    "import time\n",
    "\n",
    "def stage3_qnodes(n_layers, phi_stage1):\n",
    "    dev3 = make_device(n_qubits)\n",
    "    theta_fixed = pnp.array(phi_stage1, requires_grad=False).reshape((n_layers, n_qubits, 3))\n",
    "\n",
    "    def encoder_fixed_body(theta):\n",
    "        for l in range(n_layers):\n",
    "            for q in range(n_qubits):\n",
    "                qml.RX(theta[l, q, 0], wires=q)\n",
    "                qml.RY(theta[l, q, 1], wires=q)\n",
    "                qml.RZ(theta[l, q, 2], wires=q)\n",
    "            for q in range(n_qubits-1):\n",
    "                qml.CNOT(wires=[q, q+1])\n",
    "            qml.CNOT(wires=[n_qubits-1, 0])\n",
    "\n",
    "    def decoder_fixed():\n",
    "        qml.adjoint(encoder_fixed_body)(theta_fixed)\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def encoder_only_expZ_all(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def teacher_code_latents(x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_fixed_body(theta_fixed)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def student_code_latents(flat_params, x_in):\n",
    "        embed_input(x_in)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_latent)]\n",
    "\n",
    "    @qml.qnode(dev3, interface=\"autograd\", diff_method=\"backprop\")\n",
    "    def denoiser_qnode_all(flat_params, x_noisy):\n",
    "        embed_input(x_noisy)\n",
    "        encoder_template(flat_params, n_layers)\n",
    "        decoder_fixed()\n",
    "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
    "\n",
    "    return dict(\n",
    "        theta_fixed=theta_fixed,\n",
    "        encoder_only_expZ_all=encoder_only_expZ_all,\n",
    "        teacher_code_latents=teacher_code_latents,\n",
    "        student_code_latents=student_code_latents,\n",
    "        denoiser_qnode_all=denoiser_qnode_all\n",
    "    )\n",
    "\n",
    "# ----- value readout helpers (unchanged)\n",
    "def Z_to_values_autograd(z_all):\n",
    "    z_all = pnp.clip(pnp.asarray(z_all), -0.999999, 0.999999)\n",
    "    v01 = pnp.arccos(z_all) / pnp.pi\n",
    "    return v01 * (info[\"scale_high\"] - info[\"scale_low\"]) + info[\"scale_low\"]\n",
    "\n",
    "def first_diff(x): \n",
    "    x = pnp.array(x); return x[1:] - x[:-1]\n",
    "\n",
    "def p1_from_expZ(z): \n",
    "    return (1 - pnp.asarray(z)) * 0.5\n",
    "\n",
    "# ----- deterministic noisy window (shared with eval)\n",
    "def ts_add_noise_window_det(x, sigma, seed):\n",
    "    low, high = float(info[\"scale_low\"]), float(info[\"scale_high\"])\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    noise = rng.normal(0.0, sigma * (high - low), size=np.asarray(x).shape)\n",
    "    return np.clip(np.asarray(x) + noise, low, high)\n",
    "\n",
    "# ----- validation with fixed per-window seeds (unchanged)\n",
    "VAL_BASE_SEED = 12345\n",
    "\n",
    "def stage3_val_values_det(psi, X_clean, sigma=EVAL_SIGMA):\n",
    "    ms_noisy, ms_deno = [], []\n",
    "    for i, c in enumerate(X_clean):\n",
    "        n = ts_add_noise_window_det(c, sigma, seed=VAL_BASE_SEED + i)\n",
    "        zD = np.array(stage3_handles[\"denoiser_qnode_all\"](psi, n))\n",
    "        v_hat = np.array(Z_to_values_autograd(zD))\n",
    "        ms_noisy.append(np.mean((np.asarray(c) - np.asarray(n))**2))\n",
    "        ms_deno.append(np.mean((np.asarray(c) - v_hat)**2))\n",
    "    mN, mD = float(np.mean(ms_noisy)), float(np.mean(ms_deno))\n",
    "    d_pct = 100.0 * (1.0 - mD / max(mN, 1e-12))\n",
    "    return mN, mD, d_pct\n",
    "\n",
    "# ----- small Huber\n",
    "def huber(residual, delta):\n",
    "    r = pnp.abs(residual)\n",
    "    return pnp.where(r <= delta, 0.5*r**2, delta*(r - 0.5*delta))\n",
    "\n",
    "\n",
    "def train_stage3(X_train, X_val, phi_stage1, n_layers, instance_id,\n",
    "                 TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16, \n",
    "                 LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "                 CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99):\n",
    "\n",
    "    # ---- seeds: varied but reproducible across (instance, layers, epoch, window)\n",
    "    def make_train_seed(instance_id, layers, ep, k, view=0):\n",
    "        return (1_000_003 * (instance_id * 10 + layers) + 97 * ep + 31 * int(k) + view) % 2_147_483_647\n",
    "\n",
    "    set_global_seed(instance_id)\n",
    "\n",
    "    global stage3_handles\n",
    "    stage3_handles = stage3_qnodes(n_layers, phi_stage1)\n",
    "    enc_all = stage3_handles[\"encoder_only_expZ_all\"]\n",
    "    teacher_lat = stage3_handles[\"teacher_code_latents\"]\n",
    "    denoise_all = stage3_handles[\"denoiser_qnode_all\"]\n",
    "\n",
    "    # ---- init ψ near φ\n",
    "    phi_flat = pnp.array(phi_stage1, requires_grad=False)\n",
    "    psi = pnp.array(np.array(phi_flat) + 0.05*np.random.randn(len(phi_flat)), requires_grad=True)\n",
    "\n",
    "    # ---- loss weights\n",
    "    ALPHA_REC, BETA_TF, GAMMA_TRASH, L_TV, L_ANCH = 1.0, 0.05, 0.5, 0.05, 2e-4\n",
    "    DELTA_TV, DELTA_Z = 0.02, 0.25\n",
    "\n",
    "    # loss on a single window with a specific noise seed\n",
    "    def loss_on_window_seeded(params, clean_values, seed):\n",
    "        v_noisy = pnp.array(ts_add_noise_window_det(clean_values, TARGET_NOISE, seed=seed))\n",
    "        z_all = pnp.array(enc_all(params, v_noisy))\n",
    "        z_sig, z_tr = z_all[:n_latent], z_all[n_latent:]\n",
    "        zD = pnp.array(denoise_all(params, v_noisy))\n",
    "        v_hat = Z_to_values_autograd(zD)\n",
    "\n",
    "        L_rec = pnp.mean((pnp.array(clean_values) - v_hat)**2)\n",
    "        z_t_sig = pnp.array(teacher_lat(clean_values))\n",
    "        L_tf = pnp.mean(huber(z_t_sig - z_sig, DELTA_Z))\n",
    "        L_tr = pnp.mean(p1_from_expZ(z_tr))\n",
    "        L_tv = pnp.mean(huber(first_diff(clean_values) - first_diff(v_hat), DELTA_TV))\n",
    "        L_anchor = pnp.mean((params - phi_flat)**2)\n",
    "        return (ALPHA_REC*L_rec + BETA_TF*L_tf + GAMMA_TRASH*L_tr + L_TV*L_tv + L_ANCH*L_anchor)\n",
    "\n",
    "    # manual Adam\n",
    "    m = pnp.zeros_like(psi); v = pnp.zeros_like(psi)\n",
    "    b1, b2, eps = 0.9, 0.999, 1e-8\n",
    "    t = 0\n",
    "    def adam_step(params, grad, lr):\n",
    "        nonlocal m, v, t\n",
    "        t += 1\n",
    "        m = b1*m + (1-b1)*grad\n",
    "        v = b2*v + (1-b2)*(grad*grad)\n",
    "        mhat = m/(1-b1**t); vhat = v/(1-b2**t)\n",
    "        return params - lr * (mhat/(pnp.sqrt(vhat)+eps))\n",
    "\n",
    "    # batches deterministic per-epoch\n",
    "    def batch_indices(N, B, ep_seed):\n",
    "        rng = np.random.default_rng(ep_seed)\n",
    "        idx = rng.permutation(N)\n",
    "        for s in range(0, N, B):\n",
    "            yield idx[s:s+B]\n",
    "\n",
    "    best_params, best_val = None, float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    no_improve, lr = 0, LR_START\n",
    "    ema = pnp.array(psi, requires_grad=False) if USE_EMA else None\n",
    "\n",
    "    # history buffers (for CSV/reporting)\n",
    "    hist_train, hist_val = [], []\n",
    "    hist_noisy, hist_delta = [], []\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(MAX_EPOCHS):\n",
    "        seed_ep = 88_000 + 100*instance_id + ep  # reproducible shuffling\n",
    "        acc, nb = 0.0, 0\n",
    "        for ix in batch_indices(len(X_train), BATCH, ep_seed=seed_ep):\n",
    "            for k in ix:                                   # k = absolute index in X_train\n",
    "                c = X_train[k]\n",
    "                seed = make_train_seed(instance_id, n_layers, ep, int(k))\n",
    "                L = loss_on_window_seeded(psi, c, seed)\n",
    "                if not pnp.isfinite(L): \n",
    "                    continue\n",
    "                g = qml.grad(lambda p: loss_on_window_seeded(p, c, seed))(psi)\n",
    "                if not pnp.all(pnp.isfinite(g)): \n",
    "                    continue\n",
    "                # clip\n",
    "                gnorm = pnp.linalg.norm(g) + 1e-12\n",
    "                if gnorm > CLIP_NORM:\n",
    "                    g = g * (CLIP_NORM / gnorm)\n",
    "                psi = adam_step(psi, g, lr)\n",
    "                if USE_EMA: \n",
    "                    ema = EMA_DECAY*ema + (1-EMA_DECAY)*psi\n",
    "                acc += float(L); nb += 1\n",
    "\n",
    "        train_loss = acc / max(nb, 1)\n",
    "        eval_params = ema if USE_EMA else psi\n",
    "\n",
    "        # strict value-domain validation at σ=EVAL_SIGMA (deterministic per window)\n",
    "        mN, mD, dV = stage3_val_values_det(eval_params, X_val, sigma=EVAL_SIGMA)\n",
    "        hist_train.append(train_loss); hist_val.append(mD)\n",
    "        hist_noisy.append(mN);        hist_delta.append(dV)\n",
    "\n",
    "        if mD < best_val - 1e-12:\n",
    "            best_val, best_params, best_epoch, no_improve = mD, pnp.array(eval_params, requires_grad=False), ep, 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if (no_improve % PLATEAU_STEPS) == 0:\n",
    "                lr *= PLATEAU_FACTOR\n",
    "                print(f\"[Stage3] Plateau → LR {lr:.5f}\")\n",
    "\n",
    "        norm_diff = float(pnp.linalg.norm((eval_params - phi_flat)))\n",
    "        print(f\"[Stage3] L={n_layers} ep {ep:03d} | train {train_loss:.5f} | \"\n",
    "              f\"val {mD:.5f} | noisy {mN:.5f} | Δ {dV:+.1f}% | LR {lr:.5f} | ||ψ-φ|| {norm_diff:.3f}\")\n",
    "\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"[Stage3] Early stopping.\"); break\n",
    "\n",
    "    train_seconds = float(time.time() - t0)\n",
    "    epochs_run = len(hist_val)\n",
    "\n",
    "    psi_best = best_params if best_params is not None else (ema if USE_EMA else psi)\n",
    "\n",
    "    return dict(\n",
    "        psi=psi_best, \n",
    "        best_val=float(best_val),\n",
    "        best_epoch=int(best_epoch),\n",
    "        epochs=int(epochs_run),\n",
    "        hist_train=list(map(float, hist_train)),\n",
    "        hist_val=list(map(float, hist_val)),\n",
    "        hist_noisy=list(map(float, hist_noisy)),\n",
    "        hist_delta=list(map(float, hist_delta)),\n",
    "        train_seconds=train_seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50129aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows built: 97 (W=4, step=1)\n",
      "Split sizes → train=57, val=20, test=20\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# Cell 6 — Build windows & deterministic train/val/test\n",
    "# ===================================================\n",
    "window_size = n_qubits\n",
    "stride = WINDOW_STRIDE\n",
    "\n",
    "X_windows = np.array([y_all[i:i+window_size] for i in range(0, len(y_all)-window_size+1, stride)], dtype=float)\n",
    "print(f\"Total windows built: {len(X_windows)} (W={window_size}, step={stride})\")\n",
    "\n",
    "# 60/20/20 split (deterministic)\n",
    "X_temp, X_test = train_test_split(X_windows, test_size=0.20, random_state=SPLIT_RANDOM_STATE)\n",
    "X_train, X_val = train_test_split(X_temp,   test_size=0.25, random_state=SPLIT_RANDOM_STATE)  # 0.25 of 0.8 = 0.2\n",
    "print(f\"Split sizes → train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6711326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Instance 1 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.471965 | val 0.480380 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.471408 | val 0.478677 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.465884 | val 0.476929 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.463119 | val 0.475120 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.462231 | val 0.473211 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.460449 | val 0.471212 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.459320 | val 0.469136 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.454529 | val 0.466994 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.449435 | val 0.464799 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.447411 | val 0.462507 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.443917 | val 0.460158 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.443793 | val 0.457721 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.439415 | val 0.455219 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.438542 | val 0.452655 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.435158 | val 0.450047 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.427667 | val 0.447426 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.428869 | val 0.444730 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.422364 | val 0.441997 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.421034 | val 0.439225 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.421774 | val 0.436390 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.419442 | val 0.433551 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.415177 | val 0.430706 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.411558 | val 0.427795 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.412468 | val 0.424855 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.409612 | val 0.421926 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.406912 | val 0.418963 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.402004 | val 0.416064 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.397276 | val 0.413099 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.397953 | val 0.410114 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.393054 | val 0.407148 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.390734 | val 0.404167 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.387493 | val 0.401251 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.387478 | val 0.398317 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.381241 | val 0.395392 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.378973 | val 0.392435 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.379444 | val 0.389494 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.374572 | val 0.386613 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.377639 | val 0.383705 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.373375 | val 0.380809 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.367834 | val 0.377918 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.368327 | val 0.375005 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.365122 | val 0.372148 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.361643 | val 0.369291 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.363459 | val 0.366418 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.362161 | val 0.363587 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.362778 | val 0.360763 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.355584 | val 0.357943 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.353749 | val 0.355060 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.357496 | val 0.352264 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.358970 | val 0.349557 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.353377 | val 0.346739 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.347930 | val 0.344053 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.349306 | val 0.341309 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.349130 | val 0.338724 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.341826 | val 0.336021 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.351723 | val 0.333394 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.337019 | val 0.330664 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.336275 | val 0.327993 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.337703 | val 0.325462 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.338985 | val 0.322845 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.338196 | val 0.320370 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.335826 | val 0.317798 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.332044 | val 0.315250 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.332378 | val 0.312690 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.332409 | val 0.310239 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.333244 | val 0.307733 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.325729 | val 0.305184 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.326607 | val 0.302696 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.323169 | val 0.300279 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.327029 | val 0.297938 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.328039 | val 0.295465 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.323848 | val 0.293057 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.319465 | val 0.290597 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.316410 | val 0.288175 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.317010 | val 0.285823 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.323955 | val 0.283493 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.314349 | val 0.281114 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.310991 | val 0.278876 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.313181 | val 0.276582 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.313042 | val 0.274324 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.312866 | val 0.272124 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.309586 | val 0.269974 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.304782 | val 0.267809 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.303792 | val 0.265615 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.304034 | val 0.263452 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.300672 | val 0.261479 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.301707 | val 0.259510 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.295632 | val 0.257511 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.291968 | val 0.255716 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.296583 | val 0.253866 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.297608 | val 0.252070 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.289534 | val 0.250383 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.293395 | val 0.248703 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.289693 | val 0.247146 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.287591 | val 0.245680 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.285050 | val 0.244304 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.285933 | val 0.242973 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.281542 | val 0.241702 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.279431 | val 0.240549 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.277642 | val 0.239449 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.276949 | val 0.238448 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.276083 | val 0.237484 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.275786 | val 0.236649 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.273155 | val 0.235952 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.268280 | val 0.235372 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.269583 | val 0.234863 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.265856 | val 0.234502 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.264789 | val 0.234182 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.261764 | val 0.233913 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.261802 | val 0.233732 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.261003 | val 0.233696 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.263243 | val 0.233703 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.262587 | val 0.233800 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.259865 | val 0.233944 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.260047 | val 0.234187 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.262747 | val 0.234368 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.258414 | val 0.234660 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.256910 | val 0.234957 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 118 | train 0.254981 | val 0.235172 | LR 0.00500\n",
      "[Stage1] L=1 ep 119 | train 0.247997 | val 0.234854 | LR 0.00500\n",
      "[Stage3] L=1 ep 000 | train 0.14933 | val 0.00369 | noisy 0.00391 | Δ +5.7% | LR 0.00300 | ||ψ-φ|| 0.161\n",
      "[Stage3] L=1 ep 001 | train 0.15160 | val 0.00364 | noisy 0.00391 | Δ +6.9% | LR 0.00300 | ||ψ-φ|| 0.202\n",
      "[Stage3] L=1 ep 002 | train 0.15166 | val 0.00358 | noisy 0.00391 | Δ +8.5% | LR 0.00300 | ||ψ-φ|| 0.278\n",
      "[Stage3] L=1 ep 003 | train 0.14993 | val 0.00356 | noisy 0.00391 | Δ +8.8% | LR 0.00300 | ||ψ-φ|| 0.369\n",
      "[Stage3] L=1 ep 004 | train 0.15130 | val 0.00355 | noisy 0.00391 | Δ +9.1% | LR 0.00300 | ||ψ-φ|| 0.462\n",
      "[Stage3] L=1 ep 005 | train 0.14848 | val 0.00354 | noisy 0.00391 | Δ +9.5% | LR 0.00300 | ||ψ-φ|| 0.549\n",
      "[Stage3] L=1 ep 006 | train 0.15745 | val 0.00350 | noisy 0.00391 | Δ +10.5% | LR 0.00300 | ||ψ-φ|| 0.638\n",
      "[Stage3] L=1 ep 007 | train 0.15110 | val 0.00346 | noisy 0.00391 | Δ +11.5% | LR 0.00300 | ||ψ-φ|| 0.729\n",
      "[Stage3] L=1 ep 008 | train 0.14818 | val 0.00347 | noisy 0.00391 | Δ +11.2% | LR 0.00300 | ||ψ-φ|| 0.802\n",
      "[Stage3] L=1 ep 009 | train 0.15436 | val 0.00344 | noisy 0.00391 | Δ +11.9% | LR 0.00300 | ||ψ-φ|| 0.859\n",
      "[Stage3] L=1 ep 010 | train 0.15667 | val 0.00346 | noisy 0.00391 | Δ +11.4% | LR 0.00300 | ||ψ-φ|| 0.900\n",
      "[Stage3] L=1 ep 011 | train 0.14221 | val 0.00347 | noisy 0.00391 | Δ +11.2% | LR 0.00300 | ||ψ-φ|| 0.935\n",
      "[Stage3] L=1 ep 012 | train 0.15140 | val 0.00352 | noisy 0.00391 | Δ +9.9% | LR 0.00300 | ||ψ-φ|| 0.959\n",
      "[Stage3] L=1 ep 013 | train 0.15733 | val 0.00360 | noisy 0.00391 | Δ +8.0% | LR 0.00300 | ||ψ-φ|| 0.988\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 014 | train 0.14969 | val 0.00364 | noisy 0.00391 | Δ +6.9% | LR 0.00150 | ||ψ-φ|| 1.023\n",
      "[Stage3] L=1 ep 015 | train 0.14714 | val 0.00365 | noisy 0.00391 | Δ +6.6% | LR 0.00150 | ||ψ-φ|| 1.049\n",
      "[Stage3] L=1 ep 016 | train 0.14492 | val 0.00363 | noisy 0.00391 | Δ +7.0% | LR 0.00150 | ||ψ-φ|| 1.064\n",
      "[Stage3] L=1 ep 017 | train 0.14728 | val 0.00362 | noisy 0.00391 | Δ +7.3% | LR 0.00150 | ||ψ-φ|| 1.076\n",
      "[Stage3] L=1 ep 018 | train 0.14570 | val 0.00360 | noisy 0.00391 | Δ +7.9% | LR 0.00150 | ||ψ-φ|| 1.084\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 019 | train 0.14656 | val 0.00357 | noisy 0.00391 | Δ +8.5% | LR 0.00075 | ||ψ-φ|| 1.091\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.508119 | val 0.503208 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.506659 | val 0.500958 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.504228 | val 0.498765 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.497208 | val 0.496622 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.495231 | val 0.494515 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.490196 | val 0.492447 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.487884 | val 0.490384 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.484610 | val 0.488321 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.481729 | val 0.486234 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.479445 | val 0.484110 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.473932 | val 0.481947 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.469906 | val 0.479705 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.466700 | val 0.477371 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.463593 | val 0.474927 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.458720 | val 0.472395 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.456784 | val 0.469758 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.452702 | val 0.467022 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.450121 | val 0.464190 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.444577 | val 0.461293 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.444116 | val 0.458314 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.434701 | val 0.455327 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.433760 | val 0.452235 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.427675 | val 0.449109 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.426040 | val 0.445909 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.421543 | val 0.442687 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.418218 | val 0.439417 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.418545 | val 0.436094 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.408132 | val 0.432809 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.411756 | val 0.429454 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.404351 | val 0.426101 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.400769 | val 0.422736 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.393029 | val 0.419333 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.395116 | val 0.415916 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.394030 | val 0.412521 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.386989 | val 0.409179 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.389656 | val 0.405846 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.380639 | val 0.402506 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.383528 | val 0.399205 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.376264 | val 0.395907 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.377495 | val 0.392672 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.377782 | val 0.389455 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.371834 | val 0.386309 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.372159 | val 0.383183 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.371146 | val 0.380094 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.362437 | val 0.377028 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.361170 | val 0.374054 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.363074 | val 0.371073 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.362529 | val 0.368129 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.354817 | val 0.365235 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.355001 | val 0.362332 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.344827 | val 0.359526 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.356783 | val 0.356823 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.349091 | val 0.354075 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.354116 | val 0.351436 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.346391 | val 0.348749 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.351876 | val 0.346149 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.348463 | val 0.343551 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.351101 | val 0.340962 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.341064 | val 0.338371 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.338442 | val 0.335933 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.337850 | val 0.333419 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.339028 | val 0.330985 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.338516 | val 0.328625 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.330588 | val 0.326258 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.340032 | val 0.323812 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.331322 | val 0.321338 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.330625 | val 0.318939 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.329723 | val 0.316543 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.329728 | val 0.314194 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.330525 | val 0.311882 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.329196 | val 0.309560 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.332824 | val 0.307212 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.326072 | val 0.304924 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.325706 | val 0.302458 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.322773 | val 0.300142 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.318216 | val 0.297794 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.324632 | val 0.295397 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.318440 | val 0.293099 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.319357 | val 0.290781 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.316799 | val 0.288469 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.318501 | val 0.286159 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.317611 | val 0.283868 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.314350 | val 0.281608 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.318174 | val 0.279365 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.310298 | val 0.277086 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.310717 | val 0.275026 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.311616 | val 0.272858 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.310977 | val 0.270812 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.307395 | val 0.268608 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.309444 | val 0.266438 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.308508 | val 0.264311 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.301598 | val 0.262363 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.303808 | val 0.260415 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.298941 | val 0.258420 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.302934 | val 0.256454 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.298255 | val 0.254562 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.293777 | val 0.252633 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.295471 | val 0.250818 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.295768 | val 0.249080 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.290414 | val 0.247285 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.293512 | val 0.245554 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.288720 | val 0.243933 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.294601 | val 0.242264 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.288737 | val 0.240735 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.286435 | val 0.239218 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.283995 | val 0.237784 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.279304 | val 0.236499 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.280283 | val 0.235275 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.278265 | val 0.234150 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.282117 | val 0.233129 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.274674 | val 0.232144 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.275880 | val 0.231249 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.268211 | val 0.230476 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.271892 | val 0.229830 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.271534 | val 0.229262 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.268627 | val 0.228833 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.265651 | val 0.228442 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.266518 | val 0.228106 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.266087 | val 0.227840 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.263368 | val 0.227774 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.15716 | val 0.00367 | noisy 0.00391 | Δ +6.2% | LR 0.00300 | ||ψ-φ|| 0.158\n",
      "[Stage3] L=1 ep 001 | train 0.14134 | val 0.00363 | noisy 0.00391 | Δ +7.1% | LR 0.00300 | ||ψ-φ|| 0.202\n",
      "[Stage3] L=1 ep 002 | train 0.15335 | val 0.00359 | noisy 0.00391 | Δ +8.1% | LR 0.00300 | ||ψ-φ|| 0.268\n",
      "[Stage3] L=1 ep 003 | train 0.14630 | val 0.00358 | noisy 0.00391 | Δ +8.5% | LR 0.00300 | ||ψ-φ|| 0.349\n",
      "[Stage3] L=1 ep 004 | train 0.15661 | val 0.00351 | noisy 0.00391 | Δ +10.2% | LR 0.00300 | ||ψ-φ|| 0.442\n",
      "[Stage3] L=1 ep 005 | train 0.15291 | val 0.00351 | noisy 0.00391 | Δ +10.3% | LR 0.00300 | ||ψ-φ|| 0.536\n",
      "[Stage3] L=1 ep 006 | train 0.15722 | val 0.00346 | noisy 0.00391 | Δ +11.3% | LR 0.00300 | ||ψ-φ|| 0.632\n",
      "[Stage3] L=1 ep 007 | train 0.14961 | val 0.00348 | noisy 0.00391 | Δ +10.9% | LR 0.00300 | ||ψ-φ|| 0.712\n",
      "[Stage3] L=1 ep 008 | train 0.15236 | val 0.00348 | noisy 0.00391 | Δ +10.8% | LR 0.00300 | ||ψ-φ|| 0.764\n",
      "[Stage3] L=1 ep 009 | train 0.14119 | val 0.00352 | noisy 0.00391 | Δ +10.0% | LR 0.00300 | ||ψ-φ|| 0.813\n",
      "[Stage3] L=1 ep 010 | train 0.14952 | val 0.00353 | noisy 0.00391 | Δ +9.8% | LR 0.00300 | ||ψ-φ|| 0.863\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 011 | train 0.14838 | val 0.00356 | noisy 0.00391 | Δ +9.0% | LR 0.00150 | ||ψ-φ|| 0.905\n",
      "[Stage3] L=1 ep 012 | train 0.15797 | val 0.00357 | noisy 0.00391 | Δ +8.5% | LR 0.00150 | ||ψ-φ|| 0.944\n",
      "[Stage3] L=1 ep 013 | train 0.14496 | val 0.00358 | noisy 0.00391 | Δ +8.5% | LR 0.00150 | ||ψ-φ|| 0.968\n",
      "[Stage3] L=1 ep 014 | train 0.14862 | val 0.00359 | noisy 0.00391 | Δ +8.1% | LR 0.00150 | ||ψ-φ|| 0.985\n",
      "[Stage3] L=1 ep 015 | train 0.14920 | val 0.00360 | noisy 0.00391 | Δ +8.0% | LR 0.00150 | ||ψ-φ|| 0.995\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 016 | train 0.15320 | val 0.00360 | noisy 0.00391 | Δ +7.8% | LR 0.00075 | ||ψ-φ|| 1.005\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.484720 | val 0.487799 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.480619 | val 0.484350 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.478237 | val 0.480908 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.472437 | val 0.477429 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.469176 | val 0.473874 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.462498 | val 0.470237 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.460248 | val 0.466539 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.455284 | val 0.462693 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.449184 | val 0.458755 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.445959 | val 0.454608 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.441595 | val 0.450324 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.438649 | val 0.445774 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.431188 | val 0.441073 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.430096 | val 0.436023 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.422206 | val 0.430692 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.415681 | val 0.425051 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.410718 | val 0.419147 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.406776 | val 0.412899 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.401638 | val 0.406387 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.395634 | val 0.399633 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.388261 | val 0.392674 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.386343 | val 0.385506 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.376291 | val 0.378258 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.367927 | val 0.370917 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.363954 | val 0.363497 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.357909 | val 0.356127 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.350645 | val 0.348829 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.347380 | val 0.341669 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.337643 | val 0.334632 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.335383 | val 0.327768 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.326872 | val 0.321187 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.323172 | val 0.314792 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.320016 | val 0.308686 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.314794 | val 0.302807 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.308140 | val 0.297286 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.306667 | val 0.292008 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.306507 | val 0.287051 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.298777 | val 0.282389 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.288696 | val 0.278082 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.288840 | val 0.274056 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.284232 | val 0.270321 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.278696 | val 0.266843 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.278689 | val 0.263695 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.282302 | val 0.260801 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.276806 | val 0.258136 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.273785 | val 0.255740 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.276373 | val 0.253544 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.274095 | val 0.251479 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.271689 | val 0.249652 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.270565 | val 0.248018 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.269295 | val 0.246489 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.267972 | val 0.245048 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.268959 | val 0.243787 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.265930 | val 0.242634 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.263045 | val 0.241571 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.257553 | val 0.240606 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.259343 | val 0.239723 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.256401 | val 0.238882 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.260389 | val 0.238169 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.259466 | val 0.237474 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.265608 | val 0.236868 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.259782 | val 0.236330 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.259346 | val 0.235794 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.256105 | val 0.235342 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.258087 | val 0.234903 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.256957 | val 0.234561 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.257061 | val 0.234158 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.254206 | val 0.233907 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.256494 | val 0.233617 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.252857 | val 0.233390 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.254896 | val 0.233217 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.250121 | val 0.233063 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.255454 | val 0.232964 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.258500 | val 0.232930 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.260732 | val 0.232908 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.251191 | val 0.232916 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.253612 | val 0.232978 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.249882 | val 0.233004 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.249298 | val 0.233065 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.252287 | val 0.233156 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.250636 | val 0.233284 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.254541 | val 0.233416 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 082 | train 0.254153 | val 0.233621 | LR 0.00500\n",
      "[Stage1] L=1 ep 083 | train 0.253099 | val 0.234643 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.14464 | val 0.00372 | noisy 0.00391 | Δ +4.7% | LR 0.00300 | ||ψ-φ|| 0.170\n",
      "[Stage3] L=1 ep 001 | train 0.14921 | val 0.00372 | noisy 0.00391 | Δ +4.9% | LR 0.00300 | ||ψ-φ|| 0.189\n",
      "[Stage3] L=1 ep 002 | train 0.14425 | val 0.00366 | noisy 0.00391 | Δ +6.3% | LR 0.00300 | ||ψ-φ|| 0.236\n",
      "[Stage3] L=1 ep 003 | train 0.14536 | val 0.00353 | noisy 0.00391 | Δ +9.6% | LR 0.00300 | ||ψ-φ|| 0.307\n",
      "[Stage3] L=1 ep 004 | train 0.15285 | val 0.00341 | noisy 0.00391 | Δ +12.8% | LR 0.00300 | ||ψ-φ|| 0.383\n",
      "[Stage3] L=1 ep 005 | train 0.15067 | val 0.00330 | noisy 0.00391 | Δ +15.6% | LR 0.00300 | ||ψ-φ|| 0.456\n",
      "[Stage3] L=1 ep 006 | train 0.14334 | val 0.00322 | noisy 0.00391 | Δ +17.6% | LR 0.00300 | ||ψ-φ|| 0.525\n",
      "[Stage3] L=1 ep 007 | train 0.15232 | val 0.00314 | noisy 0.00391 | Δ +19.6% | LR 0.00300 | ||ψ-φ|| 0.591\n",
      "[Stage3] L=1 ep 008 | train 0.15139 | val 0.00310 | noisy 0.00391 | Δ +20.6% | LR 0.00300 | ||ψ-φ|| 0.649\n",
      "[Stage3] L=1 ep 009 | train 0.14476 | val 0.00311 | noisy 0.00391 | Δ +20.3% | LR 0.00300 | ||ψ-φ|| 0.691\n",
      "[Stage3] L=1 ep 010 | train 0.14336 | val 0.00312 | noisy 0.00391 | Δ +20.2% | LR 0.00300 | ||ψ-φ|| 0.743\n",
      "[Stage3] L=1 ep 011 | train 0.14662 | val 0.00311 | noisy 0.00391 | Δ +20.5% | LR 0.00300 | ||ψ-φ|| 0.789\n",
      "[Stage3] L=1 ep 012 | train 0.15093 | val 0.00309 | noisy 0.00391 | Δ +20.9% | LR 0.00300 | ||ψ-φ|| 0.814\n",
      "[Stage3] L=1 ep 013 | train 0.14321 | val 0.00310 | noisy 0.00391 | Δ +20.7% | LR 0.00300 | ||ψ-φ|| 0.838\n",
      "[Stage3] L=1 ep 014 | train 0.14440 | val 0.00309 | noisy 0.00391 | Δ +21.0% | LR 0.00300 | ||ψ-φ|| 0.863\n",
      "[Stage3] L=1 ep 015 | train 0.14728 | val 0.00307 | noisy 0.00391 | Δ +21.3% | LR 0.00300 | ||ψ-φ|| 0.876\n",
      "[Stage3] L=1 ep 016 | train 0.14584 | val 0.00307 | noisy 0.00391 | Δ +21.4% | LR 0.00300 | ||ψ-φ|| 0.878\n",
      "[Stage3] L=1 ep 017 | train 0.14536 | val 0.00309 | noisy 0.00391 | Δ +21.0% | LR 0.00300 | ||ψ-φ|| 0.887\n",
      "[Stage3] L=1 ep 018 | train 0.14827 | val 0.00310 | noisy 0.00391 | Δ +20.7% | LR 0.00300 | ||ψ-φ|| 0.897\n",
      "[Stage3] L=1 ep 019 | train 0.14629 | val 0.00310 | noisy 0.00391 | Δ +20.7% | LR 0.00300 | ||ψ-φ|| 0.917\n",
      "[Stage3] L=1 ep 020 | train 0.14656 | val 0.00310 | noisy 0.00391 | Δ +20.8% | LR 0.00300 | ||ψ-φ|| 0.924\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 021 | train 0.15159 | val 0.00308 | noisy 0.00391 | Δ +21.3% | LR 0.00150 | ||ψ-φ|| 0.933\n",
      "[Stage3] L=1 ep 022 | train 0.14373 | val 0.00306 | noisy 0.00391 | Δ +21.6% | LR 0.00150 | ||ψ-φ|| 0.931\n",
      "[Stage3] L=1 ep 023 | train 0.15327 | val 0.00306 | noisy 0.00391 | Δ +21.7% | LR 0.00150 | ||ψ-φ|| 0.929\n",
      "[Stage3] L=1 ep 024 | train 0.14471 | val 0.00305 | noisy 0.00391 | Δ +21.9% | LR 0.00150 | ||ψ-φ|| 0.929\n",
      "[Stage3] L=1 ep 025 | train 0.14384 | val 0.00304 | noisy 0.00391 | Δ +22.1% | LR 0.00150 | ||ψ-φ|| 0.923\n",
      "[Stage3] L=1 ep 026 | train 0.14260 | val 0.00303 | noisy 0.00391 | Δ +22.5% | LR 0.00150 | ||ψ-φ|| 0.920\n",
      "[Stage3] L=1 ep 027 | train 0.14095 | val 0.00301 | noisy 0.00391 | Δ +22.9% | LR 0.00150 | ||ψ-φ|| 0.913\n",
      "[Stage3] L=1 ep 028 | train 0.14474 | val 0.00301 | noisy 0.00391 | Δ +23.1% | LR 0.00150 | ||ψ-φ|| 0.907\n",
      "[Stage3] L=1 ep 029 | train 0.15341 | val 0.00301 | noisy 0.00391 | Δ +22.9% | LR 0.00150 | ||ψ-φ|| 0.907\n",
      "[Stage3] L=1 ep 030 | train 0.15126 | val 0.00301 | noisy 0.00391 | Δ +22.9% | LR 0.00150 | ||ψ-φ|| 0.910\n",
      "[Stage3] L=1 ep 031 | train 0.13926 | val 0.00302 | noisy 0.00391 | Δ +22.6% | LR 0.00150 | ||ψ-φ|| 0.910\n",
      "[Stage3] L=1 ep 032 | train 0.13967 | val 0.00303 | noisy 0.00391 | Δ +22.5% | LR 0.00150 | ||ψ-φ|| 0.908\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 033 | train 0.14827 | val 0.00303 | noisy 0.00391 | Δ +22.3% | LR 0.00075 | ||ψ-φ|| 0.907\n",
      "[Stage3] L=1 ep 034 | train 0.14431 | val 0.00303 | noisy 0.00391 | Δ +22.4% | LR 0.00075 | ||ψ-φ|| 0.905\n",
      "[Stage3] L=1 ep 035 | train 0.14714 | val 0.00303 | noisy 0.00391 | Δ +22.3% | LR 0.00075 | ||ψ-φ|| 0.901\n",
      "[Stage3] L=1 ep 036 | train 0.14398 | val 0.00303 | noisy 0.00391 | Δ +22.4% | LR 0.00075 | ||ψ-φ|| 0.898\n",
      "[Stage3] L=1 ep 037 | train 0.13858 | val 0.00302 | noisy 0.00391 | Δ +22.6% | LR 0.00075 | ||ψ-φ|| 0.897\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=1 ep 038 | train 0.14453 | val 0.00302 | noisy 0.00391 | Δ +22.8% | LR 0.00038 | ||ψ-φ|| 0.895\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.476581 | val 0.482395 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.474063 | val 0.480684 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.469761 | val 0.478904 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.470477 | val 0.477040 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.466639 | val 0.475103 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.464996 | val 0.473063 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.462784 | val 0.470917 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.457811 | val 0.468692 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.459192 | val 0.466361 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.453449 | val 0.463979 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.449782 | val 0.461532 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.449833 | val 0.458982 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.447705 | val 0.456391 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.441552 | val 0.453777 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.442795 | val 0.451096 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.439904 | val 0.448390 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.435115 | val 0.445648 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.431648 | val 0.442882 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.431168 | val 0.440048 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.426189 | val 0.437199 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.420341 | val 0.434360 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.421140 | val 0.431455 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.420458 | val 0.428549 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.413521 | val 0.425642 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.413405 | val 0.422735 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.408593 | val 0.419869 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.406332 | val 0.417017 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.403737 | val 0.414155 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.402433 | val 0.411272 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.396778 | val 0.408412 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.391666 | val 0.405464 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.390065 | val 0.402498 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.386102 | val 0.399545 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.385294 | val 0.396540 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.381097 | val 0.393547 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.383171 | val 0.390534 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.381953 | val 0.387564 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.374459 | val 0.384577 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.375338 | val 0.381639 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.374886 | val 0.378711 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.372043 | val 0.375776 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.367908 | val 0.372806 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.364772 | val 0.369786 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.366977 | val 0.366760 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.364640 | val 0.363833 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.364285 | val 0.361024 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.358010 | val 0.358212 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.355902 | val 0.355462 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.357488 | val 0.352852 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.348683 | val 0.350290 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.351556 | val 0.347605 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.359849 | val 0.344933 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.354452 | val 0.342298 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.345457 | val 0.339675 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.349844 | val 0.337131 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.350330 | val 0.334535 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.346813 | val 0.331891 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.343825 | val 0.329353 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.342494 | val 0.326776 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.338210 | val 0.324172 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.340555 | val 0.321528 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.339302 | val 0.318986 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.336017 | val 0.316381 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.337247 | val 0.313770 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.332725 | val 0.311256 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.328713 | val 0.308714 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.329750 | val 0.306242 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.329826 | val 0.303786 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.327444 | val 0.301448 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.324250 | val 0.299038 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.324504 | val 0.296561 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.320737 | val 0.294301 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.322326 | val 0.292078 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.320514 | val 0.289786 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.321230 | val 0.287592 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.316837 | val 0.285327 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.312731 | val 0.283077 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.317404 | val 0.280859 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.311238 | val 0.278658 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.314739 | val 0.276496 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.307483 | val 0.274399 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.310227 | val 0.272265 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.307891 | val 0.270102 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.307713 | val 0.268039 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.303624 | val 0.265939 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.302911 | val 0.263814 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.298240 | val 0.261754 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.301203 | val 0.259739 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.301511 | val 0.257818 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.295600 | val 0.255955 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.294905 | val 0.254185 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.293516 | val 0.252461 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.294387 | val 0.250793 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.291613 | val 0.249278 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.289343 | val 0.247782 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.292010 | val 0.246374 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.282597 | val 0.245037 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.284550 | val 0.243849 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.277003 | val 0.242747 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.280130 | val 0.241723 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.277365 | val 0.240834 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.274038 | val 0.240119 | LR 0.01000\n",
      "[Stage1] L=1 ep 102 | train 0.271176 | val 0.239449 | LR 0.01000\n",
      "[Stage1] L=1 ep 103 | train 0.272036 | val 0.238807 | LR 0.01000\n",
      "[Stage1] L=1 ep 104 | train 0.273522 | val 0.238328 | LR 0.01000\n",
      "[Stage1] L=1 ep 105 | train 0.270468 | val 0.237974 | LR 0.01000\n",
      "[Stage1] L=1 ep 106 | train 0.266453 | val 0.237620 | LR 0.01000\n",
      "[Stage1] L=1 ep 107 | train 0.263945 | val 0.237406 | LR 0.01000\n",
      "[Stage1] L=1 ep 108 | train 0.265455 | val 0.237229 | LR 0.01000\n",
      "[Stage1] L=1 ep 109 | train 0.262458 | val 0.237055 | LR 0.01000\n",
      "[Stage1] L=1 ep 110 | train 0.261869 | val 0.237029 | LR 0.01000\n",
      "[Stage1] L=1 ep 111 | train 0.259282 | val 0.236971 | LR 0.01000\n",
      "[Stage1] L=1 ep 112 | train 0.255891 | val 0.236941 | LR 0.01000\n",
      "[Stage1] L=1 ep 113 | train 0.259430 | val 0.237025 | LR 0.01000\n",
      "[Stage1] L=1 ep 114 | train 0.267458 | val 0.237122 | LR 0.01000\n",
      "[Stage1] L=1 ep 115 | train 0.258408 | val 0.237177 | LR 0.01000\n",
      "[Stage1] L=1 ep 116 | train 0.257641 | val 0.237280 | LR 0.01000\n",
      "[Stage1] L=1 ep 117 | train 0.255954 | val 0.237362 | LR 0.01000\n",
      "[Stage1] L=1 ep 118 | train 0.257580 | val 0.237496 | LR 0.01000\n",
      "[Stage1] L=1 ep 119 | train 0.255718 | val 0.237513 | LR 0.01000\n",
      "[Stage3] L=1 ep 000 | train 0.15313 | val 0.00384 | noisy 0.00391 | Δ +1.8% | LR 0.00300 | ||ψ-φ|| 0.164\n",
      "[Stage3] L=1 ep 001 | train 0.14935 | val 0.00373 | noisy 0.00391 | Δ +4.6% | LR 0.00300 | ||ψ-φ|| 0.188\n",
      "[Stage3] L=1 ep 002 | train 0.15290 | val 0.00357 | noisy 0.00391 | Δ +8.5% | LR 0.00300 | ||ψ-φ|| 0.249\n",
      "[Stage3] L=1 ep 003 | train 0.14751 | val 0.00346 | noisy 0.00391 | Δ +11.4% | LR 0.00300 | ||ψ-φ|| 0.324\n",
      "[Stage3] L=1 ep 004 | train 0.14988 | val 0.00340 | noisy 0.00391 | Δ +12.9% | LR 0.00300 | ||ψ-φ|| 0.405\n",
      "[Stage3] L=1 ep 005 | train 0.14336 | val 0.00339 | noisy 0.00391 | Δ +13.3% | LR 0.00300 | ||ψ-φ|| 0.479\n",
      "[Stage3] L=1 ep 006 | train 0.14651 | val 0.00336 | noisy 0.00391 | Δ +14.0% | LR 0.00300 | ||ψ-φ|| 0.540\n",
      "[Stage3] L=1 ep 007 | train 0.14440 | val 0.00335 | noisy 0.00391 | Δ +14.2% | LR 0.00300 | ||ψ-φ|| 0.601\n",
      "[Stage3] L=1 ep 008 | train 0.14740 | val 0.00333 | noisy 0.00391 | Δ +14.8% | LR 0.00300 | ||ψ-φ|| 0.662\n",
      "[Stage3] L=1 ep 009 | train 0.15280 | val 0.00332 | noisy 0.00391 | Δ +15.1% | LR 0.00300 | ||ψ-φ|| 0.729\n",
      "[Stage3] L=1 ep 010 | train 0.15392 | val 0.00331 | noisy 0.00391 | Δ +15.3% | LR 0.00300 | ||ψ-φ|| 0.796\n",
      "[Stage3] L=1 ep 011 | train 0.13870 | val 0.00333 | noisy 0.00391 | Δ +14.9% | LR 0.00300 | ||ψ-φ|| 0.836\n",
      "[Stage3] L=1 ep 012 | train 0.15288 | val 0.00337 | noisy 0.00391 | Δ +13.6% | LR 0.00300 | ||ψ-φ|| 0.878\n",
      "[Stage3] L=1 ep 013 | train 0.15113 | val 0.00344 | noisy 0.00391 | Δ +12.0% | LR 0.00300 | ||ψ-φ|| 0.932\n",
      "[Stage3] L=1 ep 014 | train 0.15254 | val 0.00346 | noisy 0.00391 | Δ +11.4% | LR 0.00300 | ||ψ-φ|| 0.985\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 015 | train 0.14450 | val 0.00346 | noisy 0.00391 | Δ +11.6% | LR 0.00150 | ||ψ-φ|| 1.025\n",
      "[Stage3] L=1 ep 016 | train 0.15477 | val 0.00346 | noisy 0.00391 | Δ +11.5% | LR 0.00150 | ||ψ-φ|| 1.053\n",
      "[Stage3] L=1 ep 017 | train 0.14816 | val 0.00346 | noisy 0.00391 | Δ +11.6% | LR 0.00150 | ||ψ-φ|| 1.075\n",
      "[Stage3] L=1 ep 018 | train 0.14639 | val 0.00346 | noisy 0.00391 | Δ +11.3% | LR 0.00150 | ||ψ-φ|| 1.095\n",
      "[Stage3] L=1 ep 019 | train 0.14682 | val 0.00348 | noisy 0.00391 | Δ +11.0% | LR 0.00150 | ||ψ-φ|| 1.107\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 020 | train 0.15051 | val 0.00349 | noisy 0.00391 | Δ +10.8% | LR 0.00075 | ||ψ-φ|| 1.115\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 1\n",
      "==============================\n",
      "[Stage1] L=1 ep 000 | train 0.459794 | val 0.473670 | LR 0.01000\n",
      "[Stage1] L=1 ep 001 | train 0.458902 | val 0.471064 | LR 0.01000\n",
      "[Stage1] L=1 ep 002 | train 0.454007 | val 0.468341 | LR 0.01000\n",
      "[Stage1] L=1 ep 003 | train 0.451847 | val 0.465502 | LR 0.01000\n",
      "[Stage1] L=1 ep 004 | train 0.447398 | val 0.462555 | LR 0.01000\n",
      "[Stage1] L=1 ep 005 | train 0.443997 | val 0.459482 | LR 0.01000\n",
      "[Stage1] L=1 ep 006 | train 0.441112 | val 0.456310 | LR 0.01000\n",
      "[Stage1] L=1 ep 007 | train 0.436865 | val 0.453065 | LR 0.01000\n",
      "[Stage1] L=1 ep 008 | train 0.429720 | val 0.449747 | LR 0.01000\n",
      "[Stage1] L=1 ep 009 | train 0.434001 | val 0.446313 | LR 0.01000\n",
      "[Stage1] L=1 ep 010 | train 0.426402 | val 0.442851 | LR 0.01000\n",
      "[Stage1] L=1 ep 011 | train 0.419830 | val 0.439302 | LR 0.01000\n",
      "[Stage1] L=1 ep 012 | train 0.419800 | val 0.435713 | LR 0.01000\n",
      "[Stage1] L=1 ep 013 | train 0.411288 | val 0.432045 | LR 0.01000\n",
      "[Stage1] L=1 ep 014 | train 0.404530 | val 0.428388 | LR 0.01000\n",
      "[Stage1] L=1 ep 015 | train 0.407664 | val 0.424588 | LR 0.01000\n",
      "[Stage1] L=1 ep 016 | train 0.403431 | val 0.420755 | LR 0.01000\n",
      "[Stage1] L=1 ep 017 | train 0.399719 | val 0.416880 | LR 0.01000\n",
      "[Stage1] L=1 ep 018 | train 0.396543 | val 0.413000 | LR 0.01000\n",
      "[Stage1] L=1 ep 019 | train 0.395618 | val 0.409021 | LR 0.01000\n",
      "[Stage1] L=1 ep 020 | train 0.393287 | val 0.405087 | LR 0.01000\n",
      "[Stage1] L=1 ep 021 | train 0.385851 | val 0.401142 | LR 0.01000\n",
      "[Stage1] L=1 ep 022 | train 0.383296 | val 0.397220 | LR 0.01000\n",
      "[Stage1] L=1 ep 023 | train 0.379556 | val 0.393290 | LR 0.01000\n",
      "[Stage1] L=1 ep 024 | train 0.375031 | val 0.389486 | LR 0.01000\n",
      "[Stage1] L=1 ep 025 | train 0.375105 | val 0.385652 | LR 0.01000\n",
      "[Stage1] L=1 ep 026 | train 0.373918 | val 0.381847 | LR 0.01000\n",
      "[Stage1] L=1 ep 027 | train 0.369929 | val 0.378011 | LR 0.01000\n",
      "[Stage1] L=1 ep 028 | train 0.368316 | val 0.374197 | LR 0.01000\n",
      "[Stage1] L=1 ep 029 | train 0.358831 | val 0.370432 | LR 0.01000\n",
      "[Stage1] L=1 ep 030 | train 0.360513 | val 0.366666 | LR 0.01000\n",
      "[Stage1] L=1 ep 031 | train 0.359791 | val 0.362920 | LR 0.01000\n",
      "[Stage1] L=1 ep 032 | train 0.360621 | val 0.359256 | LR 0.01000\n",
      "[Stage1] L=1 ep 033 | train 0.353892 | val 0.355625 | LR 0.01000\n",
      "[Stage1] L=1 ep 034 | train 0.350353 | val 0.351993 | LR 0.01000\n",
      "[Stage1] L=1 ep 035 | train 0.352173 | val 0.348494 | LR 0.01000\n",
      "[Stage1] L=1 ep 036 | train 0.346502 | val 0.344971 | LR 0.01000\n",
      "[Stage1] L=1 ep 037 | train 0.345651 | val 0.341504 | LR 0.01000\n",
      "[Stage1] L=1 ep 038 | train 0.342692 | val 0.338205 | LR 0.01000\n",
      "[Stage1] L=1 ep 039 | train 0.342086 | val 0.334994 | LR 0.01000\n",
      "[Stage1] L=1 ep 040 | train 0.347446 | val 0.331775 | LR 0.01000\n",
      "[Stage1] L=1 ep 041 | train 0.341693 | val 0.328570 | LR 0.01000\n",
      "[Stage1] L=1 ep 042 | train 0.335678 | val 0.325377 | LR 0.01000\n",
      "[Stage1] L=1 ep 043 | train 0.338823 | val 0.322335 | LR 0.01000\n",
      "[Stage1] L=1 ep 044 | train 0.337216 | val 0.319352 | LR 0.01000\n",
      "[Stage1] L=1 ep 045 | train 0.332600 | val 0.316285 | LR 0.01000\n",
      "[Stage1] L=1 ep 046 | train 0.331924 | val 0.313339 | LR 0.01000\n",
      "[Stage1] L=1 ep 047 | train 0.327266 | val 0.310468 | LR 0.01000\n",
      "[Stage1] L=1 ep 048 | train 0.329978 | val 0.307742 | LR 0.01000\n",
      "[Stage1] L=1 ep 049 | train 0.330929 | val 0.305063 | LR 0.01000\n",
      "[Stage1] L=1 ep 050 | train 0.325708 | val 0.302371 | LR 0.01000\n",
      "[Stage1] L=1 ep 051 | train 0.327862 | val 0.299691 | LR 0.01000\n",
      "[Stage1] L=1 ep 052 | train 0.323527 | val 0.296989 | LR 0.01000\n",
      "[Stage1] L=1 ep 053 | train 0.318398 | val 0.294205 | LR 0.01000\n",
      "[Stage1] L=1 ep 054 | train 0.319534 | val 0.291547 | LR 0.01000\n",
      "[Stage1] L=1 ep 055 | train 0.320995 | val 0.288913 | LR 0.01000\n",
      "[Stage1] L=1 ep 056 | train 0.322583 | val 0.286232 | LR 0.01000\n",
      "[Stage1] L=1 ep 057 | train 0.314379 | val 0.283593 | LR 0.01000\n",
      "[Stage1] L=1 ep 058 | train 0.320168 | val 0.281179 | LR 0.01000\n",
      "[Stage1] L=1 ep 059 | train 0.315865 | val 0.278695 | LR 0.01000\n",
      "[Stage1] L=1 ep 060 | train 0.313138 | val 0.276293 | LR 0.01000\n",
      "[Stage1] L=1 ep 061 | train 0.313833 | val 0.273998 | LR 0.01000\n",
      "[Stage1] L=1 ep 062 | train 0.314685 | val 0.271630 | LR 0.01000\n",
      "[Stage1] L=1 ep 063 | train 0.313093 | val 0.269270 | LR 0.01000\n",
      "[Stage1] L=1 ep 064 | train 0.310695 | val 0.266879 | LR 0.01000\n",
      "[Stage1] L=1 ep 065 | train 0.309092 | val 0.264619 | LR 0.01000\n",
      "[Stage1] L=1 ep 066 | train 0.306254 | val 0.262345 | LR 0.01000\n",
      "[Stage1] L=1 ep 067 | train 0.305742 | val 0.260117 | LR 0.01000\n",
      "[Stage1] L=1 ep 068 | train 0.300428 | val 0.257850 | LR 0.01000\n",
      "[Stage1] L=1 ep 069 | train 0.296764 | val 0.255736 | LR 0.01000\n",
      "[Stage1] L=1 ep 070 | train 0.299050 | val 0.253674 | LR 0.01000\n",
      "[Stage1] L=1 ep 071 | train 0.297606 | val 0.251592 | LR 0.01000\n",
      "[Stage1] L=1 ep 072 | train 0.299203 | val 0.249700 | LR 0.01000\n",
      "[Stage1] L=1 ep 073 | train 0.296827 | val 0.247855 | LR 0.01000\n",
      "[Stage1] L=1 ep 074 | train 0.293977 | val 0.246095 | LR 0.01000\n",
      "[Stage1] L=1 ep 075 | train 0.290819 | val 0.244396 | LR 0.01000\n",
      "[Stage1] L=1 ep 076 | train 0.288445 | val 0.242776 | LR 0.01000\n",
      "[Stage1] L=1 ep 077 | train 0.289432 | val 0.241119 | LR 0.01000\n",
      "[Stage1] L=1 ep 078 | train 0.283372 | val 0.239669 | LR 0.01000\n",
      "[Stage1] L=1 ep 079 | train 0.285429 | val 0.238305 | LR 0.01000\n",
      "[Stage1] L=1 ep 080 | train 0.285453 | val 0.236966 | LR 0.01000\n",
      "[Stage1] L=1 ep 081 | train 0.282006 | val 0.235745 | LR 0.01000\n",
      "[Stage1] L=1 ep 082 | train 0.279791 | val 0.234632 | LR 0.01000\n",
      "[Stage1] L=1 ep 083 | train 0.276625 | val 0.233601 | LR 0.01000\n",
      "[Stage1] L=1 ep 084 | train 0.276022 | val 0.232657 | LR 0.01000\n",
      "[Stage1] L=1 ep 085 | train 0.277248 | val 0.231837 | LR 0.01000\n",
      "[Stage1] L=1 ep 086 | train 0.274947 | val 0.231118 | LR 0.01000\n",
      "[Stage1] L=1 ep 087 | train 0.274879 | val 0.230505 | LR 0.01000\n",
      "[Stage1] L=1 ep 088 | train 0.272111 | val 0.229971 | LR 0.01000\n",
      "[Stage1] L=1 ep 089 | train 0.272961 | val 0.229552 | LR 0.01000\n",
      "[Stage1] L=1 ep 090 | train 0.270132 | val 0.229227 | LR 0.01000\n",
      "[Stage1] L=1 ep 091 | train 0.268689 | val 0.228938 | LR 0.01000\n",
      "[Stage1] L=1 ep 092 | train 0.262121 | val 0.228758 | LR 0.01000\n",
      "[Stage1] L=1 ep 093 | train 0.259216 | val 0.228641 | LR 0.01000\n",
      "[Stage1] L=1 ep 094 | train 0.261635 | val 0.228627 | LR 0.01000\n",
      "[Stage1] L=1 ep 095 | train 0.260806 | val 0.228687 | LR 0.01000\n",
      "[Stage1] L=1 ep 096 | train 0.255384 | val 0.228757 | LR 0.01000\n",
      "[Stage1] L=1 ep 097 | train 0.257737 | val 0.228844 | LR 0.01000\n",
      "[Stage1] L=1 ep 098 | train 0.258662 | val 0.229016 | LR 0.01000\n",
      "[Stage1] L=1 ep 099 | train 0.259730 | val 0.229265 | LR 0.01000\n",
      "[Stage1] L=1 ep 100 | train 0.256033 | val 0.229490 | LR 0.01000\n",
      "[Stage1] L=1 ep 101 | train 0.260210 | val 0.229773 | LR 0.01000\n",
      "[Stage1] ↓ LR → 0.00500\n",
      "[Stage1] L=1 ep 102 | train 0.256159 | val 0.230199 | LR 0.00500\n",
      "[Stage1] L=1 ep 103 | train 0.257864 | val 0.230512 | LR 0.00500\n",
      "[Stage1] Early stopping.\n",
      "[Stage3] L=1 ep 000 | train 0.15583 | val 0.00378 | noisy 0.00391 | Δ +3.2% | LR 0.00300 | ||ψ-φ|| 0.152\n",
      "[Stage3] L=1 ep 001 | train 0.15895 | val 0.00363 | noisy 0.00391 | Δ +7.0% | LR 0.00300 | ||ψ-φ|| 0.195\n",
      "[Stage3] L=1 ep 002 | train 0.15155 | val 0.00353 | noisy 0.00391 | Δ +9.6% | LR 0.00300 | ||ψ-φ|| 0.267\n",
      "[Stage3] L=1 ep 003 | train 0.14917 | val 0.00348 | noisy 0.00391 | Δ +11.0% | LR 0.00300 | ||ψ-φ|| 0.344\n",
      "[Stage3] L=1 ep 004 | train 0.15509 | val 0.00345 | noisy 0.00391 | Δ +11.6% | LR 0.00300 | ||ψ-φ|| 0.419\n",
      "[Stage3] L=1 ep 005 | train 0.14463 | val 0.00344 | noisy 0.00391 | Δ +12.0% | LR 0.00300 | ||ψ-φ|| 0.483\n",
      "[Stage3] L=1 ep 006 | train 0.14880 | val 0.00341 | noisy 0.00391 | Δ +12.8% | LR 0.00300 | ||ψ-φ|| 0.542\n",
      "[Stage3] L=1 ep 007 | train 0.15364 | val 0.00341 | noisy 0.00391 | Δ +12.8% | LR 0.00300 | ||ψ-φ|| 0.605\n",
      "[Stage3] L=1 ep 008 | train 0.15441 | val 0.00344 | noisy 0.00391 | Δ +12.0% | LR 0.00300 | ||ψ-φ|| 0.670\n",
      "[Stage3] L=1 ep 009 | train 0.15404 | val 0.00347 | noisy 0.00391 | Δ +11.1% | LR 0.00300 | ||ψ-φ|| 0.726\n",
      "[Stage3] L=1 ep 010 | train 0.14680 | val 0.00349 | noisy 0.00391 | Δ +10.7% | LR 0.00300 | ||ψ-φ|| 0.770\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=1 ep 011 | train 0.15078 | val 0.00351 | noisy 0.00391 | Δ +10.1% | LR 0.00150 | ||ψ-φ|| 0.811\n",
      "[Stage3] L=1 ep 012 | train 0.15245 | val 0.00354 | noisy 0.00391 | Δ +9.3% | LR 0.00150 | ||ψ-φ|| 0.843\n",
      "[Stage3] L=1 ep 013 | train 0.14669 | val 0.00357 | noisy 0.00391 | Δ +8.6% | LR 0.00150 | ||ψ-φ|| 0.870\n",
      "[Stage3] L=1 ep 014 | train 0.14729 | val 0.00359 | noisy 0.00391 | Δ +8.1% | LR 0.00150 | ||ψ-φ|| 0.891\n",
      "[Stage3] L=1 ep 015 | train 0.14889 | val 0.00359 | noisy 0.00391 | Δ +8.0% | LR 0.00150 | ||ψ-φ|| 0.912\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=1 ep 016 | train 0.14040 | val 0.00360 | noisy 0.00391 | Δ +7.9% | LR 0.00075 | ||ψ-φ|| 0.929\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 1 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.415331 | val 0.399801 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.392250 | val 0.379645 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.374156 | val 0.360909 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.357851 | val 0.343865 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.339436 | val 0.328351 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.325510 | val 0.314234 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.311860 | val 0.301584 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.299209 | val 0.290138 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.289333 | val 0.279719 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.280267 | val 0.270157 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.271564 | val 0.261376 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.261749 | val 0.253213 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.256260 | val 0.245630 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.248885 | val 0.238685 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.242254 | val 0.232282 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.237351 | val 0.226271 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.232809 | val 0.220716 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.227207 | val 0.215407 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.221777 | val 0.210375 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.216109 | val 0.205592 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.211930 | val 0.201121 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.209177 | val 0.196930 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.202742 | val 0.193034 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.198261 | val 0.189494 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.194635 | val 0.186375 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.190874 | val 0.183484 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.188383 | val 0.180787 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.185032 | val 0.178277 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.180040 | val 0.175827 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.177662 | val 0.173338 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.174239 | val 0.170899 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.171591 | val 0.168427 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.166596 | val 0.165905 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.163248 | val 0.163314 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.159568 | val 0.160636 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.155164 | val 0.157956 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.153304 | val 0.155220 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.149296 | val 0.152433 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.146601 | val 0.149656 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.142406 | val 0.146828 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.138997 | val 0.144143 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.134318 | val 0.141381 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.131531 | val 0.138572 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.127948 | val 0.135932 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.125223 | val 0.133399 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.123037 | val 0.130809 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.120180 | val 0.128115 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.118327 | val 0.125674 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.113405 | val 0.123207 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.112452 | val 0.120936 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.109150 | val 0.118720 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.107435 | val 0.116547 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.107123 | val 0.114411 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.104031 | val 0.112364 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.102592 | val 0.110245 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.100718 | val 0.108196 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.096827 | val 0.106082 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.096641 | val 0.104214 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.096452 | val 0.102428 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.094468 | val 0.100782 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.092368 | val 0.099210 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.092022 | val 0.097565 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.090135 | val 0.095972 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.089029 | val 0.094430 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.086657 | val 0.092697 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.087070 | val 0.091136 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.083923 | val 0.089590 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.083854 | val 0.088201 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.080418 | val 0.086936 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.080682 | val 0.085506 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.078938 | val 0.084121 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.078618 | val 0.082939 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.075862 | val 0.081610 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.076007 | val 0.080251 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.074360 | val 0.079070 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.073655 | val 0.078073 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.072156 | val 0.077104 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.070727 | val 0.076024 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.070133 | val 0.075041 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.069530 | val 0.073970 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.068936 | val 0.072972 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.068780 | val 0.072162 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.066365 | val 0.071174 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.065849 | val 0.070182 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.065768 | val 0.069260 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.065693 | val 0.068605 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.064813 | val 0.067855 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.064372 | val 0.067121 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.062866 | val 0.066619 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.062764 | val 0.066141 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.063036 | val 0.065471 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.061555 | val 0.064805 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.061613 | val 0.064166 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.061443 | val 0.063814 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.060630 | val 0.063543 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.059783 | val 0.063262 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.060727 | val 0.062941 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.060501 | val 0.062431 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.059549 | val 0.061948 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.059536 | val 0.061469 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.058486 | val 0.061048 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.058241 | val 0.060529 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.058820 | val 0.060202 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.059288 | val 0.060004 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.058099 | val 0.059905 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.058073 | val 0.059640 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.057943 | val 0.059474 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.057065 | val 0.059190 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.056907 | val 0.058876 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.057180 | val 0.058607 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.055673 | val 0.058395 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.057589 | val 0.058266 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.056634 | val 0.058062 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.057873 | val 0.057892 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.056387 | val 0.057750 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.056950 | val 0.057611 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.056525 | val 0.057576 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.056519 | val 0.057516 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.056364 | val 0.057229 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.054884 | val 0.057033 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.04931 | val 0.00350 | noisy 0.00391 | Δ +10.4% | LR 0.00300 | ||ψ-φ|| 0.269\n",
      "[Stage3] L=3 ep 001 | train 0.04569 | val 0.00345 | noisy 0.00391 | Δ +11.7% | LR 0.00300 | ||ψ-φ|| 0.268\n",
      "[Stage3] L=3 ep 002 | train 0.04881 | val 0.00335 | noisy 0.00391 | Δ +14.3% | LR 0.00300 | ||ψ-φ|| 0.299\n",
      "[Stage3] L=3 ep 003 | train 0.05030 | val 0.00318 | noisy 0.00391 | Δ +18.6% | LR 0.00300 | ||ψ-φ|| 0.367\n",
      "[Stage3] L=3 ep 004 | train 0.04610 | val 0.00305 | noisy 0.00391 | Δ +22.0% | LR 0.00300 | ||ψ-φ|| 0.435\n",
      "[Stage3] L=3 ep 005 | train 0.04603 | val 0.00296 | noisy 0.00391 | Δ +24.3% | LR 0.00300 | ||ψ-φ|| 0.489\n",
      "[Stage3] L=3 ep 006 | train 0.05135 | val 0.00291 | noisy 0.00391 | Δ +25.4% | LR 0.00300 | ||ψ-φ|| 0.539\n",
      "[Stage3] L=3 ep 007 | train 0.05391 | val 0.00286 | noisy 0.00391 | Δ +26.9% | LR 0.00300 | ||ψ-φ|| 0.594\n",
      "[Stage3] L=3 ep 008 | train 0.05025 | val 0.00282 | noisy 0.00391 | Δ +27.9% | LR 0.00300 | ||ψ-φ|| 0.646\n",
      "[Stage3] L=3 ep 009 | train 0.05015 | val 0.00279 | noisy 0.00391 | Δ +28.6% | LR 0.00300 | ||ψ-φ|| 0.679\n",
      "[Stage3] L=3 ep 010 | train 0.04915 | val 0.00278 | noisy 0.00391 | Δ +28.8% | LR 0.00300 | ||ψ-φ|| 0.704\n",
      "[Stage3] L=3 ep 011 | train 0.04857 | val 0.00276 | noisy 0.00391 | Δ +29.5% | LR 0.00300 | ||ψ-φ|| 0.738\n",
      "[Stage3] L=3 ep 012 | train 0.04470 | val 0.00275 | noisy 0.00391 | Δ +29.5% | LR 0.00300 | ||ψ-φ|| 0.751\n",
      "[Stage3] L=3 ep 013 | train 0.04596 | val 0.00274 | noisy 0.00391 | Δ +29.8% | LR 0.00300 | ||ψ-φ|| 0.763\n",
      "[Stage3] L=3 ep 014 | train 0.04704 | val 0.00275 | noisy 0.00391 | Δ +29.5% | LR 0.00300 | ||ψ-φ|| 0.773\n",
      "[Stage3] L=3 ep 015 | train 0.05005 | val 0.00276 | noisy 0.00391 | Δ +29.3% | LR 0.00300 | ||ψ-φ|| 0.779\n",
      "[Stage3] L=3 ep 016 | train 0.04899 | val 0.00275 | noisy 0.00391 | Δ +29.6% | LR 0.00300 | ||ψ-φ|| 0.793\n",
      "[Stage3] L=3 ep 017 | train 0.04905 | val 0.00275 | noisy 0.00391 | Δ +29.6% | LR 0.00300 | ||ψ-φ|| 0.803\n",
      "[Stage3] L=3 ep 018 | train 0.04489 | val 0.00273 | noisy 0.00391 | Δ +30.1% | LR 0.00300 | ||ψ-φ|| 0.810\n",
      "[Stage3] L=3 ep 019 | train 0.04394 | val 0.00271 | noisy 0.00391 | Δ +30.6% | LR 0.00300 | ||ψ-φ|| 0.824\n",
      "[Stage3] L=3 ep 020 | train 0.04607 | val 0.00272 | noisy 0.00391 | Δ +30.5% | LR 0.00300 | ||ψ-φ|| 0.825\n",
      "[Stage3] L=3 ep 021 | train 0.03993 | val 0.00270 | noisy 0.00391 | Δ +30.8% | LR 0.00300 | ||ψ-φ|| 0.832\n",
      "[Stage3] L=3 ep 022 | train 0.04841 | val 0.00272 | noisy 0.00391 | Δ +30.5% | LR 0.00300 | ||ψ-φ|| 0.840\n",
      "[Stage3] L=3 ep 023 | train 0.05159 | val 0.00272 | noisy 0.00391 | Δ +30.4% | LR 0.00300 | ||ψ-φ|| 0.854\n",
      "[Stage3] L=3 ep 024 | train 0.04979 | val 0.00271 | noisy 0.00391 | Δ +30.6% | LR 0.00300 | ||ψ-φ|| 0.870\n",
      "[Stage3] L=3 ep 025 | train 0.04996 | val 0.00271 | noisy 0.00391 | Δ +30.6% | LR 0.00300 | ||ψ-φ|| 0.871\n",
      "[Stage3] L=3 ep 026 | train 0.04565 | val 0.00270 | noisy 0.00391 | Δ +30.8% | LR 0.00300 | ||ψ-φ|| 0.877\n",
      "[Stage3] L=3 ep 027 | train 0.04954 | val 0.00270 | noisy 0.00391 | Δ +31.0% | LR 0.00300 | ||ψ-φ|| 0.885\n",
      "[Stage3] L=3 ep 028 | train 0.05082 | val 0.00267 | noisy 0.00391 | Δ +31.7% | LR 0.00300 | ||ψ-φ|| 0.899\n",
      "[Stage3] L=3 ep 029 | train 0.04498 | val 0.00269 | noisy 0.00391 | Δ +31.2% | LR 0.00300 | ||ψ-φ|| 0.910\n",
      "[Stage3] L=3 ep 030 | train 0.05095 | val 0.00269 | noisy 0.00391 | Δ +31.2% | LR 0.00300 | ||ψ-φ|| 0.915\n",
      "[Stage3] L=3 ep 031 | train 0.04796 | val 0.00271 | noisy 0.00391 | Δ +30.7% | LR 0.00300 | ||ψ-φ|| 0.915\n",
      "[Stage3] L=3 ep 032 | train 0.04710 | val 0.00270 | noisy 0.00391 | Δ +30.8% | LR 0.00300 | ||ψ-φ|| 0.912\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 033 | train 0.05287 | val 0.00268 | noisy 0.00391 | Δ +31.4% | LR 0.00150 | ||ψ-φ|| 0.914\n",
      "[Stage3] L=3 ep 034 | train 0.04758 | val 0.00266 | noisy 0.00391 | Δ +32.0% | LR 0.00150 | ||ψ-φ|| 0.911\n",
      "[Stage3] L=3 ep 035 | train 0.04897 | val 0.00265 | noisy 0.00391 | Δ +32.2% | LR 0.00150 | ||ψ-φ|| 0.910\n",
      "[Stage3] L=3 ep 036 | train 0.04219 | val 0.00265 | noisy 0.00391 | Δ +32.3% | LR 0.00150 | ||ψ-φ|| 0.911\n",
      "[Stage3] L=3 ep 037 | train 0.04215 | val 0.00266 | noisy 0.00391 | Δ +32.0% | LR 0.00150 | ||ψ-φ|| 0.907\n",
      "[Stage3] L=3 ep 038 | train 0.04732 | val 0.00266 | noisy 0.00391 | Δ +31.9% | LR 0.00150 | ||ψ-φ|| 0.907\n",
      "[Stage3] L=3 ep 039 | train 0.04668 | val 0.00269 | noisy 0.00391 | Δ +31.3% | LR 0.00150 | ||ψ-φ|| 0.906\n",
      "[Stage3] L=3 ep 040 | train 0.04547 | val 0.00269 | noisy 0.00391 | Δ +31.2% | LR 0.00150 | ||ψ-φ|| 0.905\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 041 | train 0.04692 | val 0.00269 | noisy 0.00391 | Δ +31.2% | LR 0.00075 | ||ψ-φ|| 0.902\n",
      "[Stage3] L=3 ep 042 | train 0.04459 | val 0.00270 | noisy 0.00391 | Δ +31.0% | LR 0.00075 | ||ψ-φ|| 0.898\n",
      "[Stage3] L=3 ep 043 | train 0.04720 | val 0.00270 | noisy 0.00391 | Δ +30.9% | LR 0.00075 | ||ψ-φ|| 0.896\n",
      "[Stage3] L=3 ep 044 | train 0.04407 | val 0.00270 | noisy 0.00391 | Δ +30.9% | LR 0.00075 | ||ψ-φ|| 0.896\n",
      "[Stage3] L=3 ep 045 | train 0.04597 | val 0.00270 | noisy 0.00391 | Δ +30.9% | LR 0.00075 | ||ψ-φ|| 0.895\n",
      "[Stage3] Plateau → LR 0.00038\n",
      "[Stage3] L=3 ep 046 | train 0.04777 | val 0.00270 | noisy 0.00391 | Δ +30.8% | LR 0.00038 | ||ψ-φ|| 0.895\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 2 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.514125 | val 0.486974 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.499095 | val 0.464996 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.479438 | val 0.443257 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.455405 | val 0.422272 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.436138 | val 0.402456 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.417747 | val 0.383986 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.398821 | val 0.366894 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.381086 | val 0.350962 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.368487 | val 0.335922 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.353689 | val 0.321640 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.335845 | val 0.308056 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.320368 | val 0.295282 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.306946 | val 0.283508 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.292682 | val 0.272855 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.281716 | val 0.263321 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.271943 | val 0.254756 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.259464 | val 0.247046 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.250212 | val 0.239966 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.240325 | val 0.233323 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.231815 | val 0.227123 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.224610 | val 0.221176 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.217517 | val 0.215443 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.210974 | val 0.209776 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.204688 | val 0.204133 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.199493 | val 0.198311 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.191248 | val 0.192458 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.183435 | val 0.186558 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.182950 | val 0.180603 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.174363 | val 0.174805 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.170773 | val 0.169232 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.165229 | val 0.163929 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.162180 | val 0.158904 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.158356 | val 0.154272 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.150487 | val 0.149831 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.148991 | val 0.145877 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.144690 | val 0.142330 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.144520 | val 0.139051 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.140578 | val 0.136111 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.139039 | val 0.133275 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.135009 | val 0.130674 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.133260 | val 0.128256 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.130811 | val 0.125932 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.129018 | val 0.123677 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.127083 | val 0.121471 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.126498 | val 0.119313 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.124949 | val 0.117195 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.122255 | val 0.115015 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.120235 | val 0.112909 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.119555 | val 0.110811 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.118393 | val 0.108739 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.117465 | val 0.106765 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.112198 | val 0.104756 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.112356 | val 0.102784 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.109417 | val 0.100881 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.108557 | val 0.099038 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.105342 | val 0.097202 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.104395 | val 0.095498 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.101947 | val 0.093901 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.102105 | val 0.092403 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.101313 | val 0.091040 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.100494 | val 0.089606 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.098484 | val 0.088211 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.096103 | val 0.086853 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.095498 | val 0.085521 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.093159 | val 0.084181 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.094541 | val 0.083002 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.091990 | val 0.081897 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.090504 | val 0.080840 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.089970 | val 0.079868 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.088743 | val 0.078987 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.087181 | val 0.078142 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.085690 | val 0.077243 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.084009 | val 0.076433 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.084442 | val 0.075550 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.083362 | val 0.074757 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.082972 | val 0.074018 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.081438 | val 0.073200 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.080340 | val 0.072432 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.078873 | val 0.071620 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.078683 | val 0.070752 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.078178 | val 0.069883 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.076849 | val 0.068990 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.074636 | val 0.068032 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.073954 | val 0.067052 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.073892 | val 0.066059 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.071520 | val 0.065100 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.070515 | val 0.063977 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.068206 | val 0.062885 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.068284 | val 0.061659 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.066589 | val 0.060484 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.067220 | val 0.059440 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.063712 | val 0.058486 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.062310 | val 0.057543 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.061406 | val 0.056614 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.061600 | val 0.055656 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.060056 | val 0.054955 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.058688 | val 0.054148 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.058043 | val 0.053345 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.056385 | val 0.052639 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.056528 | val 0.051960 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.055513 | val 0.051407 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.053907 | val 0.050956 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.055940 | val 0.050395 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.054390 | val 0.050023 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.053929 | val 0.049765 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.053703 | val 0.049537 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.051583 | val 0.049348 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.052756 | val 0.049070 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.051845 | val 0.048779 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.052788 | val 0.048392 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.051231 | val 0.048190 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.051811 | val 0.047888 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.051215 | val 0.047718 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.050680 | val 0.047480 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.051014 | val 0.047266 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.052123 | val 0.047212 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.050595 | val 0.047167 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.050910 | val 0.047058 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.050400 | val 0.047023 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.050030 | val 0.047008 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.04856 | val 0.00362 | noisy 0.00391 | Δ +7.3% | LR 0.00300 | ||ψ-φ|| 0.225\n",
      "[Stage3] L=3 ep 001 | train 0.04936 | val 0.00346 | noisy 0.00391 | Δ +11.6% | LR 0.00300 | ||ψ-φ|| 0.241\n",
      "[Stage3] L=3 ep 002 | train 0.04820 | val 0.00321 | noisy 0.00391 | Δ +17.7% | LR 0.00300 | ||ψ-φ|| 0.292\n",
      "[Stage3] L=3 ep 003 | train 0.04953 | val 0.00306 | noisy 0.00391 | Δ +21.6% | LR 0.00300 | ||ψ-φ|| 0.342\n",
      "[Stage3] L=3 ep 004 | train 0.04843 | val 0.00302 | noisy 0.00391 | Δ +22.7% | LR 0.00300 | ||ψ-φ|| 0.371\n",
      "[Stage3] L=3 ep 005 | train 0.04296 | val 0.00302 | noisy 0.00391 | Δ +22.7% | LR 0.00300 | ||ψ-φ|| 0.381\n",
      "[Stage3] L=3 ep 006 | train 0.04853 | val 0.00298 | noisy 0.00391 | Δ +23.9% | LR 0.00300 | ||ψ-φ|| 0.399\n",
      "[Stage3] L=3 ep 007 | train 0.04368 | val 0.00289 | noisy 0.00391 | Δ +25.9% | LR 0.00300 | ||ψ-φ|| 0.431\n",
      "[Stage3] L=3 ep 008 | train 0.04786 | val 0.00289 | noisy 0.00391 | Δ +26.1% | LR 0.00300 | ||ψ-φ|| 0.444\n",
      "[Stage3] L=3 ep 009 | train 0.04395 | val 0.00289 | noisy 0.00391 | Δ +26.0% | LR 0.00300 | ||ψ-φ|| 0.461\n",
      "[Stage3] L=3 ep 010 | train 0.04698 | val 0.00283 | noisy 0.00391 | Δ +27.6% | LR 0.00300 | ||ψ-φ|| 0.500\n",
      "[Stage3] L=3 ep 011 | train 0.04755 | val 0.00280 | noisy 0.00391 | Δ +28.3% | LR 0.00300 | ||ψ-φ|| 0.519\n",
      "[Stage3] L=3 ep 012 | train 0.04251 | val 0.00274 | noisy 0.00391 | Δ +29.8% | LR 0.00300 | ||ψ-φ|| 0.543\n",
      "[Stage3] L=3 ep 013 | train 0.03952 | val 0.00273 | noisy 0.00391 | Δ +30.3% | LR 0.00300 | ||ψ-φ|| 0.562\n",
      "[Stage3] L=3 ep 014 | train 0.04529 | val 0.00269 | noisy 0.00391 | Δ +31.1% | LR 0.00300 | ||ψ-φ|| 0.578\n",
      "[Stage3] L=3 ep 015 | train 0.04265 | val 0.00270 | noisy 0.00391 | Δ +31.0% | LR 0.00300 | ||ψ-φ|| 0.595\n",
      "[Stage3] L=3 ep 016 | train 0.05424 | val 0.00266 | noisy 0.00391 | Δ +31.8% | LR 0.00300 | ||ψ-φ|| 0.624\n",
      "[Stage3] L=3 ep 017 | train 0.04480 | val 0.00262 | noisy 0.00391 | Δ +32.9% | LR 0.00300 | ||ψ-φ|| 0.656\n",
      "[Stage3] L=3 ep 018 | train 0.04697 | val 0.00263 | noisy 0.00391 | Δ +32.8% | LR 0.00300 | ||ψ-φ|| 0.669\n",
      "[Stage3] L=3 ep 019 | train 0.04712 | val 0.00261 | noisy 0.00391 | Δ +33.1% | LR 0.00300 | ||ψ-φ|| 0.688\n",
      "[Stage3] L=3 ep 020 | train 0.04607 | val 0.00259 | noisy 0.00391 | Δ +33.6% | LR 0.00300 | ||ψ-φ|| 0.709\n",
      "[Stage3] L=3 ep 021 | train 0.04271 | val 0.00260 | noisy 0.00391 | Δ +33.5% | LR 0.00300 | ||ψ-φ|| 0.717\n",
      "[Stage3] L=3 ep 022 | train 0.04250 | val 0.00261 | noisy 0.00391 | Δ +33.2% | LR 0.00300 | ||ψ-φ|| 0.723\n",
      "[Stage3] L=3 ep 023 | train 0.04427 | val 0.00259 | noisy 0.00391 | Δ +33.8% | LR 0.00300 | ||ψ-φ|| 0.737\n",
      "[Stage3] L=3 ep 024 | train 0.04272 | val 0.00260 | noisy 0.00391 | Δ +33.4% | LR 0.00300 | ||ψ-φ|| 0.740\n",
      "[Stage3] L=3 ep 025 | train 0.04454 | val 0.00263 | noisy 0.00391 | Δ +32.7% | LR 0.00300 | ||ψ-φ|| 0.743\n",
      "[Stage3] L=3 ep 026 | train 0.04705 | val 0.00264 | noisy 0.00391 | Δ +32.5% | LR 0.00300 | ||ψ-φ|| 0.751\n",
      "[Stage3] L=3 ep 027 | train 0.04469 | val 0.00263 | noisy 0.00391 | Δ +32.7% | LR 0.00300 | ||ψ-φ|| 0.756\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 028 | train 0.04279 | val 0.00261 | noisy 0.00391 | Δ +33.3% | LR 0.00150 | ||ψ-φ|| 0.764\n",
      "[Stage3] L=3 ep 029 | train 0.04611 | val 0.00262 | noisy 0.00391 | Δ +32.8% | LR 0.00150 | ||ψ-φ|| 0.765\n",
      "[Stage3] L=3 ep 030 | train 0.04358 | val 0.00262 | noisy 0.00391 | Δ +33.0% | LR 0.00150 | ||ψ-φ|| 0.765\n",
      "[Stage3] L=3 ep 031 | train 0.05149 | val 0.00261 | noisy 0.00391 | Δ +33.1% | LR 0.00150 | ||ψ-φ|| 0.769\n",
      "[Stage3] L=3 ep 032 | train 0.04223 | val 0.00260 | noisy 0.00391 | Δ +33.5% | LR 0.00150 | ||ψ-φ|| 0.780\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 033 | train 0.04776 | val 0.00261 | noisy 0.00391 | Δ +33.3% | LR 0.00075 | ||ψ-φ|| 0.783\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 3 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.470961 | val 0.439561 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.446998 | val 0.422718 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.434920 | val 0.409497 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.417994 | val 0.398878 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.407542 | val 0.389238 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.396998 | val 0.379303 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.384264 | val 0.368753 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.376293 | val 0.357321 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.362498 | val 0.345306 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.352350 | val 0.332938 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.340083 | val 0.320603 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.331289 | val 0.308379 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.316228 | val 0.296382 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.306630 | val 0.284479 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.294675 | val 0.272676 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.282360 | val 0.261131 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.270089 | val 0.249870 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.261678 | val 0.238968 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.252865 | val 0.228582 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.239962 | val 0.218738 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.228388 | val 0.209166 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.222454 | val 0.199995 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.210104 | val 0.191259 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.198966 | val 0.182859 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.194510 | val 0.175165 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.184486 | val 0.168184 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.177483 | val 0.161997 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.172951 | val 0.156752 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.165908 | val 0.152085 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.161128 | val 0.148030 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.154401 | val 0.144559 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.150718 | val 0.141433 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.147885 | val 0.138737 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.144220 | val 0.136096 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.141276 | val 0.133787 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.138206 | val 0.131521 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.137920 | val 0.129487 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.133524 | val 0.127502 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.128470 | val 0.125781 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.128510 | val 0.124178 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.125135 | val 0.122764 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.122780 | val 0.121453 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.121441 | val 0.120383 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.124703 | val 0.119329 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.122109 | val 0.118340 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.119201 | val 0.117507 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.120683 | val 0.116726 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.119471 | val 0.115945 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.117898 | val 0.115266 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.117292 | val 0.114600 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.116162 | val 0.113926 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.116104 | val 0.113182 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.116891 | val 0.112500 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.115497 | val 0.111849 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.113315 | val 0.111199 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.110840 | val 0.110570 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.111480 | val 0.109932 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.110054 | val 0.109189 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.109888 | val 0.108510 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.109982 | val 0.107742 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.111604 | val 0.107028 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.108217 | val 0.106388 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.108419 | val 0.105625 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.106720 | val 0.104899 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.107060 | val 0.104091 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.106107 | val 0.103375 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.104982 | val 0.102536 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.104474 | val 0.101802 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.103858 | val 0.101010 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.101878 | val 0.100230 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.102760 | val 0.099431 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.101217 | val 0.098656 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.101874 | val 0.097862 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.102496 | val 0.097146 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.101390 | val 0.096470 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.098765 | val 0.095848 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.098615 | val 0.095297 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.098173 | val 0.094704 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.096881 | val 0.094122 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.097516 | val 0.093456 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.096186 | val 0.092854 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.097966 | val 0.092227 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.096684 | val 0.091650 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.096192 | val 0.091107 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.096444 | val 0.090532 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.093335 | val 0.090041 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.095191 | val 0.089436 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.095528 | val 0.088875 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.095210 | val 0.088480 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.093740 | val 0.087960 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.093375 | val 0.087535 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.093290 | val 0.087109 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.094392 | val 0.086643 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.092848 | val 0.086286 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.093747 | val 0.085844 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.092710 | val 0.085463 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.092665 | val 0.085071 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.091543 | val 0.084622 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.091284 | val 0.084299 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.090246 | val 0.084024 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.089394 | val 0.083599 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.089489 | val 0.083175 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.092368 | val 0.082804 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.089260 | val 0.082437 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.090073 | val 0.082090 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.088323 | val 0.081702 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.087318 | val 0.081221 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.087544 | val 0.080659 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.087228 | val 0.080104 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.086973 | val 0.079483 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.087170 | val 0.078782 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.086378 | val 0.078173 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.085331 | val 0.077479 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.085538 | val 0.076679 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.082943 | val 0.075751 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.082207 | val 0.074813 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.081690 | val 0.073752 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.082030 | val 0.072710 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.079993 | val 0.071753 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.077919 | val 0.070642 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.06661 | val 0.00356 | noisy 0.00391 | Δ +9.0% | LR 0.00300 | ||ψ-φ|| 0.292\n",
      "[Stage3] L=3 ep 001 | train 0.06436 | val 0.00353 | noisy 0.00391 | Δ +9.6% | LR 0.00300 | ||ψ-φ|| 0.287\n",
      "[Stage3] L=3 ep 002 | train 0.05393 | val 0.00346 | noisy 0.00391 | Δ +11.4% | LR 0.00300 | ||ψ-φ|| 0.319\n",
      "[Stage3] L=3 ep 003 | train 0.05206 | val 0.00338 | noisy 0.00391 | Δ +13.4% | LR 0.00300 | ||ψ-φ|| 0.383\n",
      "[Stage3] L=3 ep 004 | train 0.05349 | val 0.00331 | noisy 0.00391 | Δ +15.3% | LR 0.00300 | ||ψ-φ|| 0.451\n",
      "[Stage3] L=3 ep 005 | train 0.05361 | val 0.00322 | noisy 0.00391 | Δ +17.6% | LR 0.00300 | ||ψ-φ|| 0.527\n",
      "[Stage3] L=3 ep 006 | train 0.05361 | val 0.00316 | noisy 0.00391 | Δ +19.1% | LR 0.00300 | ||ψ-φ|| 0.605\n",
      "[Stage3] L=3 ep 007 | train 0.05005 | val 0.00312 | noisy 0.00391 | Δ +20.2% | LR 0.00300 | ||ψ-φ|| 0.674\n",
      "[Stage3] L=3 ep 008 | train 0.05175 | val 0.00308 | noisy 0.00391 | Δ +21.1% | LR 0.00300 | ||ψ-φ|| 0.738\n",
      "[Stage3] L=3 ep 009 | train 0.05476 | val 0.00308 | noisy 0.00391 | Δ +21.2% | LR 0.00300 | ||ψ-φ|| 0.796\n",
      "[Stage3] L=3 ep 010 | train 0.04957 | val 0.00309 | noisy 0.00391 | Δ +20.9% | LR 0.00300 | ||ψ-φ|| 0.861\n",
      "[Stage3] L=3 ep 011 | train 0.04629 | val 0.00312 | noisy 0.00391 | Δ +20.1% | LR 0.00300 | ||ψ-φ|| 0.935\n",
      "[Stage3] L=3 ep 012 | train 0.05120 | val 0.00315 | noisy 0.00391 | Δ +19.4% | LR 0.00300 | ||ψ-φ|| 1.016\n",
      "[Stage3] L=3 ep 013 | train 0.04613 | val 0.00321 | noisy 0.00391 | Δ +17.9% | LR 0.00300 | ||ψ-φ|| 1.108\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 014 | train 0.04758 | val 0.00323 | noisy 0.00391 | Δ +17.3% | LR 0.00150 | ||ψ-φ|| 1.209\n",
      "[Stage3] L=3 ep 015 | train 0.04932 | val 0.00327 | noisy 0.00391 | Δ +16.4% | LR 0.00150 | ||ψ-φ|| 1.303\n",
      "[Stage3] L=3 ep 016 | train 0.04428 | val 0.00329 | noisy 0.00391 | Δ +15.7% | LR 0.00150 | ||ψ-φ|| 1.380\n",
      "[Stage3] L=3 ep 017 | train 0.04641 | val 0.00332 | noisy 0.00391 | Δ +15.1% | LR 0.00150 | ||ψ-φ|| 1.448\n",
      "[Stage3] L=3 ep 018 | train 0.03850 | val 0.00334 | noisy 0.00391 | Δ +14.6% | LR 0.00150 | ||ψ-φ|| 1.504\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 019 | train 0.04248 | val 0.00335 | noisy 0.00391 | Δ +14.3% | LR 0.00075 | ||ψ-φ|| 1.552\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 4 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.460850 | val 0.444397 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.444000 | val 0.425579 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.425591 | val 0.407981 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.411755 | val 0.391424 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.393198 | val 0.375888 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.378794 | val 0.361379 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.364196 | val 0.348150 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.351917 | val 0.336418 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.340000 | val 0.326461 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.328556 | val 0.318091 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.320297 | val 0.311070 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.312645 | val 0.305128 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.306476 | val 0.299624 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.299957 | val 0.294115 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.293272 | val 0.288675 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.287620 | val 0.283201 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.283288 | val 0.277792 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.278295 | val 0.272578 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.272625 | val 0.267741 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.267948 | val 0.263020 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.264141 | val 0.258543 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.259782 | val 0.254303 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.255351 | val 0.250151 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.252543 | val 0.245777 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.247887 | val 0.241379 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.243652 | val 0.236805 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.238531 | val 0.231918 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.235038 | val 0.226611 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.229298 | val 0.221071 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.225270 | val 0.215175 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.219983 | val 0.209190 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.215156 | val 0.203122 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.209801 | val 0.197069 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.203754 | val 0.191146 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.198999 | val 0.185351 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.193459 | val 0.179747 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.187988 | val 0.174395 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.184070 | val 0.169254 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.179870 | val 0.164538 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.174624 | val 0.160056 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.171606 | val 0.156010 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.166879 | val 0.152390 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.164601 | val 0.149217 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.161453 | val 0.146436 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.158998 | val 0.143947 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.156219 | val 0.141687 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.154545 | val 0.139712 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.153118 | val 0.137949 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.149575 | val 0.136292 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.148583 | val 0.134796 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.146132 | val 0.133434 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.145745 | val 0.132116 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.144364 | val 0.131038 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.144366 | val 0.130042 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.139993 | val 0.129077 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.138987 | val 0.128081 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.139257 | val 0.127242 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.137392 | val 0.126315 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.137179 | val 0.125352 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.136226 | val 0.124425 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.135422 | val 0.123515 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.132381 | val 0.122775 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.133678 | val 0.121953 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.132310 | val 0.121282 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.132761 | val 0.120633 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.132545 | val 0.119982 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.130808 | val 0.119406 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.131746 | val 0.118829 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.127855 | val 0.118297 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.129732 | val 0.117848 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.131337 | val 0.117439 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.127325 | val 0.116987 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.128705 | val 0.116420 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.126954 | val 0.115908 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.125054 | val 0.115457 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.125646 | val 0.114895 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.122984 | val 0.114339 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.124479 | val 0.113688 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.123799 | val 0.113132 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.123021 | val 0.112495 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.120332 | val 0.111923 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.119868 | val 0.111325 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.119660 | val 0.110799 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.117736 | val 0.110251 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.116672 | val 0.109626 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.115902 | val 0.108857 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.114653 | val 0.108229 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.115892 | val 0.107631 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.114484 | val 0.107025 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.113359 | val 0.106553 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.112018 | val 0.105882 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.111495 | val 0.105220 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.112397 | val 0.104618 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.107871 | val 0.103959 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.108353 | val 0.103181 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.108545 | val 0.102417 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.106355 | val 0.101616 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.105959 | val 0.100808 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.104585 | val 0.099992 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.103849 | val 0.099055 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.101902 | val 0.098183 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.101199 | val 0.097382 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.098132 | val 0.096381 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.097798 | val 0.095327 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.096132 | val 0.094303 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.097182 | val 0.093182 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.094617 | val 0.091949 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.092128 | val 0.090678 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.091181 | val 0.089281 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.089591 | val 0.087945 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.088056 | val 0.086447 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.086036 | val 0.084808 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.083349 | val 0.083266 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.081963 | val 0.081657 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.082588 | val 0.080127 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.080197 | val 0.078691 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.078578 | val 0.077158 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.076867 | val 0.075625 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.075198 | val 0.074222 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.074525 | val 0.072759 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.05568 | val 0.00356 | noisy 0.00391 | Δ +8.8% | LR 0.00300 | ||ψ-φ|| 0.269\n",
      "[Stage3] L=3 ep 001 | train 0.04608 | val 0.00346 | noisy 0.00391 | Δ +11.4% | LR 0.00300 | ||ψ-φ|| 0.283\n",
      "[Stage3] L=3 ep 002 | train 0.05190 | val 0.00325 | noisy 0.00391 | Δ +16.8% | LR 0.00300 | ||ψ-φ|| 0.350\n",
      "[Stage3] L=3 ep 003 | train 0.05120 | val 0.00316 | noisy 0.00391 | Δ +19.3% | LR 0.00300 | ||ψ-φ|| 0.438\n",
      "[Stage3] L=3 ep 004 | train 0.04782 | val 0.00307 | noisy 0.00391 | Δ +21.4% | LR 0.00300 | ||ψ-φ|| 0.536\n",
      "[Stage3] L=3 ep 005 | train 0.05122 | val 0.00302 | noisy 0.00391 | Δ +22.7% | LR 0.00300 | ||ψ-φ|| 0.628\n",
      "[Stage3] L=3 ep 006 | train 0.04944 | val 0.00296 | noisy 0.00391 | Δ +24.4% | LR 0.00300 | ||ψ-φ|| 0.723\n",
      "[Stage3] L=3 ep 007 | train 0.05203 | val 0.00296 | noisy 0.00391 | Δ +24.3% | LR 0.00300 | ||ψ-φ|| 0.818\n",
      "[Stage3] L=3 ep 008 | train 0.04900 | val 0.00296 | noisy 0.00391 | Δ +24.2% | LR 0.00300 | ||ψ-φ|| 0.907\n",
      "[Stage3] L=3 ep 009 | train 0.04794 | val 0.00296 | noisy 0.00391 | Δ +24.4% | LR 0.00300 | ||ψ-φ|| 0.985\n",
      "[Stage3] L=3 ep 010 | train 0.03966 | val 0.00296 | noisy 0.00391 | Δ +24.4% | LR 0.00300 | ||ψ-φ|| 1.045\n",
      "[Stage3] L=3 ep 011 | train 0.04564 | val 0.00295 | noisy 0.00391 | Δ +24.5% | LR 0.00300 | ||ψ-φ|| 1.093\n",
      "[Stage3] L=3 ep 012 | train 0.04404 | val 0.00296 | noisy 0.00391 | Δ +24.2% | LR 0.00300 | ||ψ-φ|| 1.137\n",
      "[Stage3] L=3 ep 013 | train 0.03975 | val 0.00295 | noisy 0.00391 | Δ +24.4% | LR 0.00300 | ||ψ-φ|| 1.174\n",
      "[Stage3] L=3 ep 014 | train 0.04127 | val 0.00298 | noisy 0.00391 | Δ +23.6% | LR 0.00300 | ||ψ-φ|| 1.216\n",
      "[Stage3] L=3 ep 015 | train 0.04321 | val 0.00301 | noisy 0.00391 | Δ +22.9% | LR 0.00300 | ||ψ-φ|| 1.269\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 016 | train 0.04746 | val 0.00304 | noisy 0.00391 | Δ +22.2% | LR 0.00150 | ||ψ-φ|| 1.315\n",
      "[Stage3] L=3 ep 017 | train 0.03679 | val 0.00303 | noisy 0.00391 | Δ +22.6% | LR 0.00150 | ||ψ-φ|| 1.350\n",
      "[Stage3] L=3 ep 018 | train 0.04003 | val 0.00303 | noisy 0.00391 | Δ +22.4% | LR 0.00150 | ||ψ-φ|| 1.379\n",
      "[Stage3] L=3 ep 019 | train 0.04184 | val 0.00303 | noisy 0.00391 | Δ +22.4% | LR 0.00150 | ||ψ-φ|| 1.400\n",
      "[Stage3] L=3 ep 020 | train 0.04278 | val 0.00303 | noisy 0.00391 | Δ +22.4% | LR 0.00150 | ||ψ-φ|| 1.423\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 021 | train 0.03410 | val 0.00303 | noisy 0.00391 | Δ +22.4% | LR 0.00075 | ||ψ-φ|| 1.441\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "==============================\n",
      "Instance 5 | Layers 3\n",
      "==============================\n",
      "[Stage1] L=3 ep 000 | train 0.539318 | val 0.537965 | LR 0.01000\n",
      "[Stage1] L=3 ep 001 | train 0.526051 | val 0.522523 | LR 0.01000\n",
      "[Stage1] L=3 ep 002 | train 0.510541 | val 0.506683 | LR 0.01000\n",
      "[Stage1] L=3 ep 003 | train 0.497408 | val 0.490595 | LR 0.01000\n",
      "[Stage1] L=3 ep 004 | train 0.480529 | val 0.474275 | LR 0.01000\n",
      "[Stage1] L=3 ep 005 | train 0.466004 | val 0.457578 | LR 0.01000\n",
      "[Stage1] L=3 ep 006 | train 0.448638 | val 0.440798 | LR 0.01000\n",
      "[Stage1] L=3 ep 007 | train 0.435687 | val 0.424272 | LR 0.01000\n",
      "[Stage1] L=3 ep 008 | train 0.418880 | val 0.408351 | LR 0.01000\n",
      "[Stage1] L=3 ep 009 | train 0.403249 | val 0.393489 | LR 0.01000\n",
      "[Stage1] L=3 ep 010 | train 0.391847 | val 0.379792 | LR 0.01000\n",
      "[Stage1] L=3 ep 011 | train 0.378126 | val 0.367240 | LR 0.01000\n",
      "[Stage1] L=3 ep 012 | train 0.366023 | val 0.355762 | LR 0.01000\n",
      "[Stage1] L=3 ep 013 | train 0.356758 | val 0.345268 | LR 0.01000\n",
      "[Stage1] L=3 ep 014 | train 0.346410 | val 0.335503 | LR 0.01000\n",
      "[Stage1] L=3 ep 015 | train 0.338738 | val 0.326375 | LR 0.01000\n",
      "[Stage1] L=3 ep 016 | train 0.330385 | val 0.317683 | LR 0.01000\n",
      "[Stage1] L=3 ep 017 | train 0.321983 | val 0.309381 | LR 0.01000\n",
      "[Stage1] L=3 ep 018 | train 0.315657 | val 0.301528 | LR 0.01000\n",
      "[Stage1] L=3 ep 019 | train 0.307477 | val 0.293942 | LR 0.01000\n",
      "[Stage1] L=3 ep 020 | train 0.300593 | val 0.286693 | LR 0.01000\n",
      "[Stage1] L=3 ep 021 | train 0.294213 | val 0.279774 | LR 0.01000\n",
      "[Stage1] L=3 ep 022 | train 0.287546 | val 0.273188 | LR 0.01000\n",
      "[Stage1] L=3 ep 023 | train 0.281925 | val 0.266948 | LR 0.01000\n",
      "[Stage1] L=3 ep 024 | train 0.276766 | val 0.261010 | LR 0.01000\n",
      "[Stage1] L=3 ep 025 | train 0.269436 | val 0.255443 | LR 0.01000\n",
      "[Stage1] L=3 ep 026 | train 0.264130 | val 0.249903 | LR 0.01000\n",
      "[Stage1] L=3 ep 027 | train 0.259197 | val 0.244423 | LR 0.01000\n",
      "[Stage1] L=3 ep 028 | train 0.251864 | val 0.239234 | LR 0.01000\n",
      "[Stage1] L=3 ep 029 | train 0.248644 | val 0.234009 | LR 0.01000\n",
      "[Stage1] L=3 ep 030 | train 0.241231 | val 0.228822 | LR 0.01000\n",
      "[Stage1] L=3 ep 031 | train 0.237313 | val 0.224199 | LR 0.01000\n",
      "[Stage1] L=3 ep 032 | train 0.230831 | val 0.219659 | LR 0.01000\n",
      "[Stage1] L=3 ep 033 | train 0.225830 | val 0.215090 | LR 0.01000\n",
      "[Stage1] L=3 ep 034 | train 0.221850 | val 0.210670 | LR 0.01000\n",
      "[Stage1] L=3 ep 035 | train 0.216656 | val 0.206362 | LR 0.01000\n",
      "[Stage1] L=3 ep 036 | train 0.215552 | val 0.202131 | LR 0.01000\n",
      "[Stage1] L=3 ep 037 | train 0.209783 | val 0.198335 | LR 0.01000\n",
      "[Stage1] L=3 ep 038 | train 0.205709 | val 0.194504 | LR 0.01000\n",
      "[Stage1] L=3 ep 039 | train 0.200899 | val 0.190776 | LR 0.01000\n",
      "[Stage1] L=3 ep 040 | train 0.194319 | val 0.186957 | LR 0.01000\n",
      "[Stage1] L=3 ep 041 | train 0.193770 | val 0.183150 | LR 0.01000\n",
      "[Stage1] L=3 ep 042 | train 0.190267 | val 0.179527 | LR 0.01000\n",
      "[Stage1] L=3 ep 043 | train 0.183917 | val 0.176180 | LR 0.01000\n",
      "[Stage1] L=3 ep 044 | train 0.179267 | val 0.172820 | LR 0.01000\n",
      "[Stage1] L=3 ep 045 | train 0.178459 | val 0.169382 | LR 0.01000\n",
      "[Stage1] L=3 ep 046 | train 0.176259 | val 0.166122 | LR 0.01000\n",
      "[Stage1] L=3 ep 047 | train 0.173033 | val 0.163324 | LR 0.01000\n",
      "[Stage1] L=3 ep 048 | train 0.169283 | val 0.160199 | LR 0.01000\n",
      "[Stage1] L=3 ep 049 | train 0.163676 | val 0.157472 | LR 0.01000\n",
      "[Stage1] L=3 ep 050 | train 0.161507 | val 0.154913 | LR 0.01000\n",
      "[Stage1] L=3 ep 051 | train 0.159698 | val 0.152409 | LR 0.01000\n",
      "[Stage1] L=3 ep 052 | train 0.157751 | val 0.150091 | LR 0.01000\n",
      "[Stage1] L=3 ep 053 | train 0.155901 | val 0.147801 | LR 0.01000\n",
      "[Stage1] L=3 ep 054 | train 0.153073 | val 0.145810 | LR 0.01000\n",
      "[Stage1] L=3 ep 055 | train 0.150534 | val 0.143516 | LR 0.01000\n",
      "[Stage1] L=3 ep 056 | train 0.148039 | val 0.141754 | LR 0.01000\n",
      "[Stage1] L=3 ep 057 | train 0.146470 | val 0.139983 | LR 0.01000\n",
      "[Stage1] L=3 ep 058 | train 0.143630 | val 0.138156 | LR 0.01000\n",
      "[Stage1] L=3 ep 059 | train 0.143339 | val 0.136655 | LR 0.01000\n",
      "[Stage1] L=3 ep 060 | train 0.141364 | val 0.135140 | LR 0.01000\n",
      "[Stage1] L=3 ep 061 | train 0.138416 | val 0.133584 | LR 0.01000\n",
      "[Stage1] L=3 ep 062 | train 0.137843 | val 0.131990 | LR 0.01000\n",
      "[Stage1] L=3 ep 063 | train 0.133943 | val 0.130302 | LR 0.01000\n",
      "[Stage1] L=3 ep 064 | train 0.132309 | val 0.128660 | LR 0.01000\n",
      "[Stage1] L=3 ep 065 | train 0.131420 | val 0.126870 | LR 0.01000\n",
      "[Stage1] L=3 ep 066 | train 0.130152 | val 0.125316 | LR 0.01000\n",
      "[Stage1] L=3 ep 067 | train 0.127931 | val 0.123822 | LR 0.01000\n",
      "[Stage1] L=3 ep 068 | train 0.127864 | val 0.122144 | LR 0.01000\n",
      "[Stage1] L=3 ep 069 | train 0.126040 | val 0.120490 | LR 0.01000\n",
      "[Stage1] L=3 ep 070 | train 0.123376 | val 0.118825 | LR 0.01000\n",
      "[Stage1] L=3 ep 071 | train 0.124333 | val 0.117205 | LR 0.01000\n",
      "[Stage1] L=3 ep 072 | train 0.120896 | val 0.115529 | LR 0.01000\n",
      "[Stage1] L=3 ep 073 | train 0.119820 | val 0.113835 | LR 0.01000\n",
      "[Stage1] L=3 ep 074 | train 0.117015 | val 0.112205 | LR 0.01000\n",
      "[Stage1] L=3 ep 075 | train 0.115335 | val 0.110529 | LR 0.01000\n",
      "[Stage1] L=3 ep 076 | train 0.113339 | val 0.109008 | LR 0.01000\n",
      "[Stage1] L=3 ep 077 | train 0.114280 | val 0.107265 | LR 0.01000\n",
      "[Stage1] L=3 ep 078 | train 0.110739 | val 0.105544 | LR 0.01000\n",
      "[Stage1] L=3 ep 079 | train 0.108699 | val 0.103764 | LR 0.01000\n",
      "[Stage1] L=3 ep 080 | train 0.108213 | val 0.101925 | LR 0.01000\n",
      "[Stage1] L=3 ep 081 | train 0.106619 | val 0.100114 | LR 0.01000\n",
      "[Stage1] L=3 ep 082 | train 0.102552 | val 0.098153 | LR 0.01000\n",
      "[Stage1] L=3 ep 083 | train 0.102994 | val 0.096292 | LR 0.01000\n",
      "[Stage1] L=3 ep 084 | train 0.101847 | val 0.094445 | LR 0.01000\n",
      "[Stage1] L=3 ep 085 | train 0.098053 | val 0.092472 | LR 0.01000\n",
      "[Stage1] L=3 ep 086 | train 0.097291 | val 0.090669 | LR 0.01000\n",
      "[Stage1] L=3 ep 087 | train 0.095769 | val 0.088854 | LR 0.01000\n",
      "[Stage1] L=3 ep 088 | train 0.093335 | val 0.086998 | LR 0.01000\n",
      "[Stage1] L=3 ep 089 | train 0.091407 | val 0.085287 | LR 0.01000\n",
      "[Stage1] L=3 ep 090 | train 0.089620 | val 0.083490 | LR 0.01000\n",
      "[Stage1] L=3 ep 091 | train 0.088601 | val 0.081601 | LR 0.01000\n",
      "[Stage1] L=3 ep 092 | train 0.085014 | val 0.079729 | LR 0.01000\n",
      "[Stage1] L=3 ep 093 | train 0.082446 | val 0.077768 | LR 0.01000\n",
      "[Stage1] L=3 ep 094 | train 0.082222 | val 0.076060 | LR 0.01000\n",
      "[Stage1] L=3 ep 095 | train 0.079794 | val 0.074191 | LR 0.01000\n",
      "[Stage1] L=3 ep 096 | train 0.077637 | val 0.072236 | LR 0.01000\n",
      "[Stage1] L=3 ep 097 | train 0.077005 | val 0.070178 | LR 0.01000\n",
      "[Stage1] L=3 ep 098 | train 0.075825 | val 0.068162 | LR 0.01000\n",
      "[Stage1] L=3 ep 099 | train 0.074335 | val 0.066242 | LR 0.01000\n",
      "[Stage1] L=3 ep 100 | train 0.071278 | val 0.064264 | LR 0.01000\n",
      "[Stage1] L=3 ep 101 | train 0.071007 | val 0.062338 | LR 0.01000\n",
      "[Stage1] L=3 ep 102 | train 0.069437 | val 0.060614 | LR 0.01000\n",
      "[Stage1] L=3 ep 103 | train 0.067404 | val 0.058818 | LR 0.01000\n",
      "[Stage1] L=3 ep 104 | train 0.064242 | val 0.057049 | LR 0.01000\n",
      "[Stage1] L=3 ep 105 | train 0.064339 | val 0.055379 | LR 0.01000\n",
      "[Stage1] L=3 ep 106 | train 0.062403 | val 0.053567 | LR 0.01000\n",
      "[Stage1] L=3 ep 107 | train 0.060605 | val 0.051773 | LR 0.01000\n",
      "[Stage1] L=3 ep 108 | train 0.058742 | val 0.049991 | LR 0.01000\n",
      "[Stage1] L=3 ep 109 | train 0.056735 | val 0.048071 | LR 0.01000\n",
      "[Stage1] L=3 ep 110 | train 0.054883 | val 0.046438 | LR 0.01000\n",
      "[Stage1] L=3 ep 111 | train 0.054091 | val 0.044757 | LR 0.01000\n",
      "[Stage1] L=3 ep 112 | train 0.051279 | val 0.043079 | LR 0.01000\n",
      "[Stage1] L=3 ep 113 | train 0.049758 | val 0.041325 | LR 0.01000\n",
      "[Stage1] L=3 ep 114 | train 0.048049 | val 0.039627 | LR 0.01000\n",
      "[Stage1] L=3 ep 115 | train 0.046878 | val 0.037983 | LR 0.01000\n",
      "[Stage1] L=3 ep 116 | train 0.045371 | val 0.036438 | LR 0.01000\n",
      "[Stage1] L=3 ep 117 | train 0.043770 | val 0.035020 | LR 0.01000\n",
      "[Stage1] L=3 ep 118 | train 0.042997 | val 0.033642 | LR 0.01000\n",
      "[Stage1] L=3 ep 119 | train 0.040466 | val 0.032433 | LR 0.01000\n",
      "[Stage3] L=3 ep 000 | train 0.04135 | val 0.00363 | noisy 0.00391 | Δ +7.2% | LR 0.00300 | ||ψ-φ|| 0.277\n",
      "[Stage3] L=3 ep 001 | train 0.03982 | val 0.00355 | noisy 0.00391 | Δ +9.2% | LR 0.00300 | ||ψ-φ|| 0.291\n",
      "[Stage3] L=3 ep 002 | train 0.03841 | val 0.00344 | noisy 0.00391 | Δ +12.0% | LR 0.00300 | ||ψ-φ|| 0.365\n",
      "[Stage3] L=3 ep 003 | train 0.03374 | val 0.00329 | noisy 0.00391 | Δ +15.9% | LR 0.00300 | ||ψ-φ|| 0.459\n",
      "[Stage3] L=3 ep 004 | train 0.03412 | val 0.00313 | noisy 0.00391 | Δ +20.0% | LR 0.00300 | ||ψ-φ|| 0.560\n",
      "[Stage3] L=3 ep 005 | train 0.03329 | val 0.00301 | noisy 0.00391 | Δ +23.1% | LR 0.00300 | ||ψ-φ|| 0.655\n",
      "[Stage3] L=3 ep 006 | train 0.03503 | val 0.00291 | noisy 0.00391 | Δ +25.6% | LR 0.00300 | ||ψ-φ|| 0.743\n",
      "[Stage3] L=3 ep 007 | train 0.03460 | val 0.00281 | noisy 0.00391 | Δ +28.0% | LR 0.00300 | ||ψ-φ|| 0.828\n",
      "[Stage3] L=3 ep 008 | train 0.03250 | val 0.00277 | noisy 0.00391 | Δ +29.0% | LR 0.00300 | ||ψ-φ|| 0.905\n",
      "[Stage3] L=3 ep 009 | train 0.03210 | val 0.00276 | noisy 0.00391 | Δ +29.5% | LR 0.00300 | ||ψ-φ|| 0.973\n",
      "[Stage3] L=3 ep 010 | train 0.03342 | val 0.00275 | noisy 0.00391 | Δ +29.7% | LR 0.00300 | ||ψ-φ|| 1.035\n",
      "[Stage3] L=3 ep 011 | train 0.03203 | val 0.00276 | noisy 0.00391 | Δ +29.3% | LR 0.00300 | ||ψ-φ|| 1.080\n",
      "[Stage3] L=3 ep 012 | train 0.03508 | val 0.00276 | noisy 0.00391 | Δ +29.5% | LR 0.00300 | ||ψ-φ|| 1.122\n",
      "[Stage3] L=3 ep 013 | train 0.03228 | val 0.00272 | noisy 0.00391 | Δ +30.3% | LR 0.00300 | ||ψ-φ|| 1.161\n",
      "[Stage3] L=3 ep 014 | train 0.03611 | val 0.00270 | noisy 0.00391 | Δ +30.9% | LR 0.00300 | ||ψ-φ|| 1.200\n",
      "[Stage3] L=3 ep 015 | train 0.03725 | val 0.00267 | noisy 0.00391 | Δ +31.6% | LR 0.00300 | ||ψ-φ|| 1.236\n",
      "[Stage3] L=3 ep 016 | train 0.03457 | val 0.00265 | noisy 0.00391 | Δ +32.3% | LR 0.00300 | ||ψ-φ|| 1.259\n",
      "[Stage3] L=3 ep 017 | train 0.03729 | val 0.00268 | noisy 0.00391 | Δ +31.3% | LR 0.00300 | ||ψ-φ|| 1.276\n",
      "[Stage3] L=3 ep 018 | train 0.03064 | val 0.00271 | noisy 0.00391 | Δ +30.8% | LR 0.00300 | ||ψ-φ|| 1.290\n",
      "[Stage3] L=3 ep 019 | train 0.03671 | val 0.00269 | noisy 0.00391 | Δ +31.3% | LR 0.00300 | ||ψ-φ|| 1.302\n",
      "[Stage3] L=3 ep 020 | train 0.03404 | val 0.00270 | noisy 0.00391 | Δ +30.9% | LR 0.00300 | ||ψ-φ|| 1.320\n",
      "[Stage3] Plateau → LR 0.00150\n",
      "[Stage3] L=3 ep 021 | train 0.03576 | val 0.00269 | noisy 0.00391 | Δ +31.2% | LR 0.00150 | ||ψ-φ|| 1.329\n",
      "[Stage3] L=3 ep 022 | train 0.03253 | val 0.00269 | noisy 0.00391 | Δ +31.3% | LR 0.00150 | ||ψ-φ|| 1.339\n",
      "[Stage3] L=3 ep 023 | train 0.03252 | val 0.00270 | noisy 0.00391 | Δ +30.9% | LR 0.00150 | ||ψ-φ|| 1.342\n",
      "[Stage3] L=3 ep 024 | train 0.03608 | val 0.00272 | noisy 0.00391 | Δ +30.4% | LR 0.00150 | ||ψ-φ|| 1.341\n",
      "[Stage3] L=3 ep 025 | train 0.03817 | val 0.00274 | noisy 0.00391 | Δ +29.8% | LR 0.00150 | ||ψ-φ|| 1.343\n",
      "[Stage3] Plateau → LR 0.00075\n",
      "[Stage3] L=3 ep 026 | train 0.03140 | val 0.00276 | noisy 0.00391 | Δ +29.4% | LR 0.00075 | ||ψ-φ|| 1.348\n",
      "[Stage3] Early stopping.\n",
      "\n",
      "Completed 10 runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7 — Train runs (instances × layers)\n",
    "# ============================================\n",
    "RUNS = []  # we’ll save each run in the next cell\n",
    "\n",
    "for L in LAYER_OPTIONS:\n",
    "    for inst in INSTANCE_IDS:\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Instance {inst} | Layers {L}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        s1 = train_stage1(\n",
    "            X_train, X_val,\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            n_epochs=120, batch_size=32,\n",
    "            lr_init=0.010, patience=10, lr_patience=8, min_delta=1e-6\n",
    "        )\n",
    "        t1 = time.time()\n",
    "\n",
    "        s3 = train_stage3(\n",
    "            X_train, X_val,\n",
    "            phi_stage1=s1[\"phi\"],\n",
    "            n_layers=L,\n",
    "            instance_id=inst,\n",
    "            TARGET_NOISE=EVAL_SIGMA, MAX_EPOCHS=60, BATCH=16,\n",
    "            LR_START=0.003, PATIENCE=10, PLATEAU_STEPS=5, PLATEAU_FACTOR=0.5,\n",
    "            CLIP_NORM=2.0, USE_EMA=True, EMA_DECAY=0.99\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        RUNS.append({\n",
    "            \"instance_id\": inst,\n",
    "            \"n_layers\": L,\n",
    "            \"stage1\": {\n",
    "                \"phi\": s1[\"phi\"],\n",
    "                \"best_val\": s1[\"best_val\"],\n",
    "                \"hist_train\": s1[\"hist_train\"],\n",
    "                \"hist_val\": s1[\"hist_val\"],\n",
    "                \"hist_lr\": s1[\"hist_lr\"],\n",
    "                \"best_epoch\": s1.get(\"best_epoch\"),\n",
    "                \"epochs\": s1.get(\"epochs\"),\n",
    "                \"train_seconds\": float(t1 - t0),\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"psi\": s3[\"psi\"],\n",
    "                \"best_val\": s3[\"best_val\"],\n",
    "                \"best_epoch\": s3.get(\"best_epoch\"),\n",
    "                \"epochs\": s3.get(\"epochs\"),\n",
    "                \"hist_train\": s3[\"hist_train\"],\n",
    "                \"hist_val\": s3[\"hist_val\"],\n",
    "                # NEW: capture these so Cell 8 has them\n",
    "                \"hist_noisy\": s3.get(\"hist_noisy\", []),\n",
    "                \"hist_delta\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": float(t2 - t1),\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nCompleted {len(RUNS)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c039220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L1/4q_2l_2t_1ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_01.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_02.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_03.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_04.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved bundle → ./runs_halfqae/q4_l2t2/L3/4q_2l_2t_3ls_05.json\n",
      "Upserted CSV row  → ./runs_halfqae/all_training_instances_v2.csv\n",
      "\n",
      "All runs saved and recorded.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# Cell 8 — Save artifacts (JSON) and append a paper-ready CSV per run\n",
    "# ======================================================================\n",
    "from pathlib import Path\n",
    "import json, time, os, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- hyperparams logged (keep in sync with training cells) ---\n",
    "S1_LR_INIT       = 0.010\n",
    "S1_MAX_EPOCHS    = 120\n",
    "S1_PATIENCE      = 10\n",
    "S1_LR_PATIENCE   = 8\n",
    "\n",
    "S3_LR_INIT       = 0.003\n",
    "S3_MAX_EPOCHS    = 60\n",
    "S3_PATIENCE      = 10\n",
    "S3_PLATEAU_STEPS = 5\n",
    "S3_PLATEAU_FACT  = 0.5\n",
    "\n",
    "CSV_SCHEMA_VERSION = \"v2\"  # bump if you change columns\n",
    "\n",
    "# --- ensure dirs ---\n",
    "ensure_dir(OUT_BASE)\n",
    "subroot = ensure_dir(f\"{OUT_BASE}/q{n_qubits}_l{n_latent}t{n_trash}\")\n",
    "\n",
    "# --- CSV path (versioned) ---\n",
    "CSV_PATH = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "\n",
    "# --- header for the full, paper-friendly table ---\n",
    "CSV_HEADER = [\n",
    "    # id / naming\n",
    "    \"filename\",\"run_tag\",\"dataset_folder\",\"instance_id\",\"rng_seed\",\n",
    "    # architecture\n",
    "    \"n_qubits\",\"n_latent\",\"n_trash\",\"n_layers\",\n",
    "    # noise & window\n",
    "    \"sigma_train\",\"sigma_eval\",\"window_stride\",\n",
    "    # stage-1 hyperparams + outcomes\n",
    "    \"s1_lr_init\",\"s1_max_epochs\",\"s1_patience\",\"s1_lr_patience\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\"s1_best_epoch\",\"s1_epochs\",\"s1_train_seconds\",\n",
    "    # stage-3 hyperparams + outcomes\n",
    "    \"s3_lr_init\",\"s3_max_epochs\",\"s3_patience\",\"s3_plateau_steps\",\"s3_plateau_factor\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\"s3_best_epoch\",\"s3_epochs\",\"s3_train_seconds\",\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    # params (JSON)\n",
    "    \"phi_params\",\"psi_params\",\n",
    "    # totals\n",
    "    \"total_train_seconds\",\n",
    "]\n",
    "\n",
    "def ensure_csv(path, header):\n",
    "    needs_header = True\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                first_line = f.readline().rstrip(\"\\n\")\n",
    "            needs_header = (first_line != \",\".join(header))\n",
    "        except Exception:\n",
    "            needs_header = True\n",
    "    if needs_header:\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(header)\n",
    "\n",
    "ensure_csv(CSV_PATH, CSV_HEADER)\n",
    "\n",
    "def _safe_argmin(seq):\n",
    "    try:\n",
    "        return int(np.nanargmin(seq)) if len(seq) else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def _safe_last(seq):\n",
    "    return float(seq[-1]) if (isinstance(seq, (list, tuple)) and len(seq)) else np.nan\n",
    "\n",
    "def save_one_run(run):\n",
    "    global stage3_handles  # needed by stage3_val_values_det fallback\n",
    "\n",
    "    inst = int(run[\"instance_id\"])\n",
    "    L    = int(run[\"n_layers\"])\n",
    "    seed = int(run.get(\"seed\", inst))\n",
    "\n",
    "    # standardized filename: 4q_2l_2t_3ls_01.json\n",
    "    fname = std_instance_name(n_qubits, n_latent, n_trash, L, inst)\n",
    "    out_dir = ensure_dir(f\"{subroot}/L{L}\")\n",
    "    bundle_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    # pull stage results (robust to missing keys)\n",
    "    s1 = run[\"stage1\"]; s3 = run[\"stage3\"]\n",
    "\n",
    "    # Stage-1 metrics\n",
    "    s1_hist_val = list(map(float, s1.get(\"hist_val\", [])))\n",
    "    s1_best_val = float(s1.get(\"best_val\", np.nan))\n",
    "    s1_final_val = _safe_last(s1_hist_val)\n",
    "    s1_best_epoch = int(s1.get(\"best_epoch\", _safe_argmin(s1_hist_val)))\n",
    "    s1_epochs = int(s1.get(\"epochs\", len(s1_hist_val)))\n",
    "    s1_seconds = float(s1.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # Stage-3 series\n",
    "    s3_hist_val   = list(map(float, s3.get(\"hist_val\", [])))\n",
    "    s3_hist_noisy = list(map(float, s3.get(\"hist_noisy\", [])))\n",
    "    s3_hist_delta = list(map(float, s3.get(\"hist_delta\", [])))\n",
    "\n",
    "    s3_best_val   = float(s3.get(\"best_val\", np.nan))\n",
    "    s3_final_val  = _safe_last(s3_hist_val)\n",
    "\n",
    "    s3_best_epoch = int(s3.get(\"best_epoch\", _safe_argmin(s3_hist_val)))\n",
    "    s3_epochs     = int(s3.get(\"epochs\", len(s3_hist_val)))\n",
    "    s3_seconds    = float(s3.get(\"train_seconds\", np.nan))\n",
    "\n",
    "    # --- compute metrics with FALLBACKS if curves are missing ---\n",
    "    noisy_baseline = float(np.nanmean(s3_hist_noisy)) if len(s3_hist_noisy) else np.nan\n",
    "    best_delta     = (float(np.nanmax(s3_hist_delta)) if (len(s3_hist_delta) and np.isfinite(np.nanmax(s3_hist_delta)))\n",
    "                      else np.nan)\n",
    "    final_delta    = _safe_last(s3_hist_delta)\n",
    "\n",
    "    need_fallback = (not len(s3_hist_noisy)) or (not np.isfinite(noisy_baseline)) or (not np.isfinite(final_delta))\n",
    "\n",
    "    if need_fallback:\n",
    "        # Rebuild the QNodes for this (L, phi) so we can evaluate psi on X_val\n",
    "        phi_for_L = np.array(s1.get(\"phi\", []))\n",
    "        stage3_handles = stage3_qnodes(L, phi_for_L)  # sets the fixed decoder from φ\n",
    "        psi_params = np.array(s3.get(\"psi\", []))\n",
    "        # Deterministic validation at σ = EVAL_SIGMA\n",
    "        mN, mD, d_pct = stage3_val_values_det(psi_params, X_val, sigma=EVAL_SIGMA)\n",
    "        noisy_baseline = float(mN)\n",
    "        final_delta    = float(d_pct)\n",
    "        if not np.isfinite(best_delta):  # if we don't have a curve, use final as best\n",
    "            best_delta = final_delta\n",
    "\n",
    "    # bundle JSON (parameters + training curves)\n",
    "    bundle = {\n",
    "        \"schema\": {\"name\": \"half_qae_bundle\", \"version\": \"1.0\"},\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": {\n",
    "            \"id\": data_folder,\n",
    "            \"scale_low\":  float(info[\"scale_low\"]),\n",
    "            \"scale_high\": float(info[\"scale_high\"]),\n",
    "            \"window_size\": int(n_qubits),\n",
    "            \"window_stride\": int(WINDOW_STRIDE),\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"tag\": f\"inst{inst}_L{L}\",\n",
    "            \"instance_id\": inst,\n",
    "            \"seed\": seed,\n",
    "            \"sigma_train\": float(EVAL_SIGMA),\n",
    "            \"sigma_eval\":  float(EVAL_SIGMA),\n",
    "        },\n",
    "        \"architecture\": {\n",
    "            \"n_qubits\": int(n_qubits),\n",
    "            \"n_layers\": int(L),\n",
    "            \"n_latent\": int(n_latent),\n",
    "            \"n_trash\":  int(n_trash),\n",
    "            \"latent_wires\": list(range(n_latent)),\n",
    "            \"trash_wires\":  list(range(n_latent, n_qubits)),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"stage1\": {\n",
    "                \"lr_init\": S1_LR_INIT, \"max_epochs\": S1_MAX_EPOCHS,\n",
    "                \"patience\": S1_PATIENCE, \"lr_patience\": S1_LR_PATIENCE,\n",
    "                \"best_val\": s1_best_val, \"final_val\": s1_final_val,\n",
    "                \"best_epoch\": s1_best_epoch, \"epochs\": s1_epochs,\n",
    "                \"train_curve\": s1.get(\"hist_train\", []), \"val_curve\": s1_hist_val, \"lr_curve\": s1.get(\"hist_lr\", []),\n",
    "                \"train_seconds\": s1_seconds,\n",
    "            },\n",
    "            \"stage3\": {\n",
    "                \"lr_init\": S3_LR_INIT, \"max_epochs\": S3_MAX_EPOCHS,\n",
    "                \"patience\": S3_PATIENCE, \"plateau_steps\": S3_PLATEAU_STEPS, \"plateau_factor\": S3_PLATEAU_FACT,\n",
    "                \"best_val_mse\": s3_best_val, \"final_val_mse\": s3_final_val,\n",
    "                \"best_epoch\": s3_best_epoch, \"epochs\": s3_epochs,\n",
    "                \"train_curve\": s3.get(\"hist_train\", []), \"val_curve\": s3_hist_val,\n",
    "                \"noisy_curve\": s3.get(\"hist_noisy\", []), \"delta_curve\": s3.get(\"hist_delta\", []),\n",
    "                \"train_seconds\": s3_seconds,\n",
    "            }\n",
    "        },\n",
    "        \"parameters\": {\n",
    "            \"phi_stage1\": np.array(s1.get(\"phi\", [])).tolist(),\n",
    "            \"psi_stage3\": np.array(s3.get(\"psi\", [])).tolist(),\n",
    "        },\n",
    "    }\n",
    "    with open(bundle_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bundle, f, indent=2)\n",
    "    print(f\"Saved bundle → {bundle_path}\")\n",
    "\n",
    "    # assemble CSV row\n",
    "    phi_params = json.dumps(bundle[\"parameters\"][\"phi_stage1\"])\n",
    "    psi_params = json.dumps(bundle[\"parameters\"][\"psi_stage3\"])\n",
    "    total_seconds = float((0 if np.isnan(s1_seconds) else s1_seconds) + (0 if np.isnan(s3_seconds) else s3_seconds))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(bundle_path),\n",
    "        f\"inst{inst}_L{L}\",\n",
    "        data_folder,\n",
    "        inst, seed,\n",
    "        int(n_qubits), int(n_latent), int(n_trash), int(L),\n",
    "        f\"{EVAL_SIGMA:.3f}\", f\"{EVAL_SIGMA:.3f}\", int(WINDOW_STRIDE),\n",
    "        f\"{S1_LR_INIT:.6f}\", int(S1_MAX_EPOCHS), int(S1_PATIENCE), int(S1_LR_PATIENCE),\n",
    "        f\"{s1_best_val:.8f}\", f\"{s1_final_val:.8f}\", s1_best_epoch, s1_epochs, s1_seconds,\n",
    "        f\"{S3_LR_INIT:.6f}\", int(S3_MAX_EPOCHS), int(S3_PATIENCE), int(S3_PLATEAU_STEPS), f\"{S3_PLATEAU_FACT:.3f}\",\n",
    "        f\"{s3_best_val:.8f}\", f\"{s3_final_val:.8f}\", s3_best_epoch, s3_epochs, s3_seconds,\n",
    "        noisy_baseline, best_delta, final_delta,\n",
    "        phi_params, psi_params,\n",
    "        total_seconds,\n",
    "    ]\n",
    "\n",
    "    # upsert row into CSV\n",
    "    row_df = pd.DataFrame([row], columns=CSV_HEADER)\n",
    "    if Path(CSV_PATH).exists():\n",
    "        df_old = pd.read_csv(CSV_PATH)\n",
    "        key = os.path.basename(bundle_path)\n",
    "        if \"filename\" in df_old.columns:\n",
    "            df_old = df_old[df_old[\"filename\"] != key]\n",
    "        df_new = pd.concat([df_old, row_df], ignore_index=True)\n",
    "        df_new.to_csv(CSV_PATH, index=False)\n",
    "    else:\n",
    "        row_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Upserted CSV row  → {CSV_PATH}\")\n",
    "\n",
    "# ---- save all runs from Cell 7 ----\n",
    "for run in RUNS:\n",
    "    save_one_run(run)\n",
    "\n",
    "print(\"\\nAll runs saved and recorded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "137c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training-only table → ./runs_halfqae/all_training_instances_v2.csv\n",
      "Saved per-layer summary → ./runs_halfqae/summary_by_layers_v2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>dataset_folder</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>rng_seed</th>\n",
       "      <th>n_qubits</th>\n",
       "      <th>n_latent</th>\n",
       "      <th>n_trash</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>sigma_train</th>\n",
       "      <th>...</th>\n",
       "      <th>s3_final_val_mse</th>\n",
       "      <th>s3_best_epoch</th>\n",
       "      <th>s3_epochs</th>\n",
       "      <th>s3_train_seconds</th>\n",
       "      <th>s3_noisy_baseline_mse</th>\n",
       "      <th>s3_best_delta_pct</th>\n",
       "      <th>s3_final_delta_pct</th>\n",
       "      <th>phi_params</th>\n",
       "      <th>psi_params</th>\n",
       "      <th>total_train_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4q_2l_2t_1ls_01.json</td>\n",
       "      <td>inst1_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>32.788465</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>11.878268</td>\n",
       "      <td>11.878268</td>\n",
       "      <td>[0.00017403888647591283, -1.9710404535850785, ...</td>\n",
       "      <td>[-0.0006123296470692364, -2.0262762252890805, ...</td>\n",
       "      <td>86.059745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4q_2l_2t_1ls_02.json</td>\n",
       "      <td>inst2_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>28.605802</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>11.336218</td>\n",
       "      <td>11.336218</td>\n",
       "      <td>[1.8056755714875147e-05, -1.9193147771584536, ...</td>\n",
       "      <td>[-0.005484925176926273, -2.0216083452573383, -...</td>\n",
       "      <td>81.824216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4q_2l_2t_1ls_03.json</td>\n",
       "      <td>inst3_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>64.523617</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>23.067725</td>\n",
       "      <td>23.067725</td>\n",
       "      <td>[0.00021396068902014424, 1.0656310158671813, 0...</td>\n",
       "      <td>[0.0022221163621398794, 1.0046762387673538, 1....</td>\n",
       "      <td>102.940782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4q_2l_2t_1ls_04.json</td>\n",
       "      <td>inst4_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>37.238774</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>15.307742</td>\n",
       "      <td>15.307742</td>\n",
       "      <td>[3.9331420911905245e-05, -2.0191278619373705, ...</td>\n",
       "      <td>[0.0016684085266360368, -2.0860432857437528, 0...</td>\n",
       "      <td>91.399046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4q_2l_2t_1ls_05.json</td>\n",
       "      <td>inst5_L1</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>27.810920</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>12.813947</td>\n",
       "      <td>12.813947</td>\n",
       "      <td>[-3.1875221778984855e-05, -1.909854775842016, ...</td>\n",
       "      <td>[-0.012663044461042546, -2.009391188962638, -0...</td>\n",
       "      <td>75.792944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4q_2l_2t_3ls_01.json</td>\n",
       "      <td>inst1_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>36</td>\n",
       "      <td>47</td>\n",
       "      <td>192.629691</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>32.257238</td>\n",
       "      <td>32.257238</td>\n",
       "      <td>[1.4721530341460392, 0.03237289093849179, -0.1...</td>\n",
       "      <td>[1.5373755944746403, 0.013355311686751007, -0....</td>\n",
       "      <td>328.944969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4q_2l_2t_3ls_02.json</td>\n",
       "      <td>inst2_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>140.302912</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>33.813029</td>\n",
       "      <td>33.813029</td>\n",
       "      <td>[0.019330858162530336, -0.28521034298242903, -...</td>\n",
       "      <td>[-0.008000250494283483, -0.2607754835185739, -...</td>\n",
       "      <td>277.924827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4q_2l_2t_3ls_03.json</td>\n",
       "      <td>inst3_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>83.783044</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>21.239540</td>\n",
       "      <td>21.239540</td>\n",
       "      <td>[0.7013220902762077, 1.1604341975538626, 0.576...</td>\n",
       "      <td>[0.5618612928893267, 1.2522178748846688, 0.577...</td>\n",
       "      <td>222.222205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4q_2l_2t_3ls_04.json</td>\n",
       "      <td>inst4_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>90.352279</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>24.546582</td>\n",
       "      <td>24.546582</td>\n",
       "      <td>[-0.20819083620731277, 0.033144625958318935, 1...</td>\n",
       "      <td>[-0.14742413582161445, 0.06266292593559304, 1....</td>\n",
       "      <td>229.604356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4q_2l_2t_3ls_05.json</td>\n",
       "      <td>inst5_L3</td>\n",
       "      <td>mackey_glass_n100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>107.694295</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>32.257212</td>\n",
       "      <td>32.257212</td>\n",
       "      <td>[1.3362803038788154, 0.03851371435683986, 1.06...</td>\n",
       "      <td>[1.574032281646895, 0.03304526435829487, 1.299...</td>\n",
       "      <td>241.914114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename   run_tag     dataset_folder  instance_id  rng_seed  \\\n",
       "0  4q_2l_2t_1ls_01.json  inst1_L1  mackey_glass_n100            1         1   \n",
       "1  4q_2l_2t_1ls_02.json  inst2_L1  mackey_glass_n100            2         2   \n",
       "2  4q_2l_2t_1ls_03.json  inst3_L1  mackey_glass_n100            3         3   \n",
       "3  4q_2l_2t_1ls_04.json  inst4_L1  mackey_glass_n100            4         4   \n",
       "4  4q_2l_2t_1ls_05.json  inst5_L1  mackey_glass_n100            5         5   \n",
       "5  4q_2l_2t_3ls_01.json  inst1_L3  mackey_glass_n100            1         1   \n",
       "6  4q_2l_2t_3ls_02.json  inst2_L3  mackey_glass_n100            2         2   \n",
       "7  4q_2l_2t_3ls_03.json  inst3_L3  mackey_glass_n100            3         3   \n",
       "8  4q_2l_2t_3ls_04.json  inst4_L3  mackey_glass_n100            4         4   \n",
       "9  4q_2l_2t_3ls_05.json  inst5_L3  mackey_glass_n100            5         5   \n",
       "\n",
       "   n_qubits  n_latent  n_trash  n_layers  sigma_train  ...  s3_final_val_mse  \\\n",
       "0         4         2        2         1          0.1  ...          0.003574   \n",
       "1         4         2        2         1          0.1  ...          0.003603   \n",
       "2         4         2        2         1          0.1  ...          0.003016   \n",
       "3         4         2        2         1          0.1  ...          0.003487   \n",
       "4         4         2        2         1          0.1  ...          0.003599   \n",
       "5         4         2        2         3          0.1  ...          0.002705   \n",
       "6         4         2        2         3          0.1  ...          0.002606   \n",
       "7         4         2        2         3          0.1  ...          0.003348   \n",
       "8         4         2        2         3          0.1  ...          0.003031   \n",
       "9         4         2        2         3          0.1  ...          0.002757   \n",
       "\n",
       "   s3_best_epoch  s3_epochs  s3_train_seconds  s3_noisy_baseline_mse  \\\n",
       "0              9         20         32.788465               0.003908   \n",
       "1              6         17         28.605802               0.003908   \n",
       "2             28         39         64.523617               0.003908   \n",
       "3             10         21         37.238774               0.003908   \n",
       "4              6         17         27.810920               0.003908   \n",
       "5             36         47        192.629691               0.003908   \n",
       "6             23         34        140.302912               0.003908   \n",
       "7              9         20         83.783044               0.003908   \n",
       "8             11         22         90.352279               0.003908   \n",
       "9             16         27        107.694295               0.003908   \n",
       "\n",
       "   s3_best_delta_pct  s3_final_delta_pct  \\\n",
       "0          11.878268           11.878268   \n",
       "1          11.336218           11.336218   \n",
       "2          23.067725           23.067725   \n",
       "3          15.307742           15.307742   \n",
       "4          12.813947           12.813947   \n",
       "5          32.257238           32.257238   \n",
       "6          33.813029           33.813029   \n",
       "7          21.239540           21.239540   \n",
       "8          24.546582           24.546582   \n",
       "9          32.257212           32.257212   \n",
       "\n",
       "                                          phi_params  \\\n",
       "0  [0.00017403888647591283, -1.9710404535850785, ...   \n",
       "1  [1.8056755714875147e-05, -1.9193147771584536, ...   \n",
       "2  [0.00021396068902014424, 1.0656310158671813, 0...   \n",
       "3  [3.9331420911905245e-05, -2.0191278619373705, ...   \n",
       "4  [-3.1875221778984855e-05, -1.909854775842016, ...   \n",
       "5  [1.4721530341460392, 0.03237289093849179, -0.1...   \n",
       "6  [0.019330858162530336, -0.28521034298242903, -...   \n",
       "7  [0.7013220902762077, 1.1604341975538626, 0.576...   \n",
       "8  [-0.20819083620731277, 0.033144625958318935, 1...   \n",
       "9  [1.3362803038788154, 0.03851371435683986, 1.06...   \n",
       "\n",
       "                                          psi_params  total_train_seconds  \n",
       "0  [-0.0006123296470692364, -2.0262762252890805, ...            86.059745  \n",
       "1  [-0.005484925176926273, -2.0216083452573383, -...            81.824216  \n",
       "2  [0.0022221163621398794, 1.0046762387673538, 1....           102.940782  \n",
       "3  [0.0016684085266360368, -2.0860432857437528, 0...            91.399046  \n",
       "4  [-0.012663044461042546, -2.009391188962638, -0...            75.792944  \n",
       "5  [1.5373755944746403, 0.013355311686751007, -0....           328.944969  \n",
       "6  [-0.008000250494283483, -0.2607754835185739, -...           277.924827  \n",
       "7  [0.5618612928893267, 1.2522178748846688, 0.577...           222.222205  \n",
       "8  [-0.14742413582161445, 0.06266292593559304, 1....           229.604356  \n",
       "9  [1.574032281646895, 0.03304526435829487, 1.299...           241.914114  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runs</th>\n",
       "      <th>noisy_baseline_mse (mean±std)</th>\n",
       "      <th>best_val_mse (mean±std)</th>\n",
       "      <th>final_val_mse (mean±std)</th>\n",
       "      <th>best_delta_pct (mean±std)</th>\n",
       "      <th>final_delta_pct (mean±std)</th>\n",
       "      <th>s1_best_val (mean±std)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003908 ± 0.000000</td>\n",
       "      <td>0.003326 ± 0.000169</td>\n",
       "      <td>0.003456 ± 0.000224</td>\n",
       "      <td>14.880780 ± 4.314143</td>\n",
       "      <td>14.880780 ± 4.314143</td>\n",
       "      <td>0.231989 ± 0.003387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003908 ± 0.000000</td>\n",
       "      <td>0.002781 ± 0.000195</td>\n",
       "      <td>0.002889 ± 0.000269</td>\n",
       "      <td>28.822720 ± 4.985676</td>\n",
       "      <td>28.822720 ± 4.985676</td>\n",
       "      <td>0.055975 ± 0.015050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          runs noisy_baseline_mse (mean±std) best_val_mse (mean±std)  \\\n",
       "n_layers                                                               \n",
       "1            5           0.003908 ± 0.000000     0.003326 ± 0.000169   \n",
       "3            5           0.003908 ± 0.000000     0.002781 ± 0.000195   \n",
       "\n",
       "         final_val_mse (mean±std) best_delta_pct (mean±std)  \\\n",
       "n_layers                                                      \n",
       "1             0.003456 ± 0.000224      14.880780 ± 4.314143   \n",
       "3             0.002889 ± 0.000269      28.822720 ± 4.985676   \n",
       "\n",
       "         final_delta_pct (mean±std) s1_best_val (mean±std)  \n",
       "n_layers                                                    \n",
       "1              14.880780 ± 4.314143    0.231989 ± 0.003387  \n",
       "3              28.822720 ± 4.985676    0.055975 ± 0.015050  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 9 — Build & preview the training-only results table\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(CSV_PATH).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Run Cell 8 first.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- NEW: drop duplicate runs; keep the newest copy (with baseline/delta)\n",
    "if \"filename\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=[\"run_tag\",\"instance_id\",\"n_layers\"], keep=\"last\")\n",
    "\n",
    "# Typical numeric casts (safe)\n",
    "for col in [\n",
    "    \"s3_noisy_baseline_mse\",\"s3_best_delta_pct\",\"s3_final_delta_pct\",\n",
    "    \"s3_best_val_mse\",\"s3_final_val_mse\",\n",
    "    \"s1_best_val\",\"s1_final_val\",\n",
    "    \"s1_train_seconds\",\"s3_train_seconds\",\"total_train_seconds\",\n",
    "    \"s1_best_epoch\",\"s1_epochs\",\"s3_best_epoch\",\"s3_epochs\"\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"n_layers\",\"instance_id\"]).reset_index(drop=True)\n",
    "\n",
    "clean_path = f\"{OUT_BASE}/all_training_instances_{CSV_SCHEMA_VERSION}.csv\"\n",
    "Path(OUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"Saved training-only table → {clean_path}\")\n",
    "\n",
    "# A compact per-layer summary (mean±std); guards against all-NaN\n",
    "def mean_std_safe(s: pd.Series) -> str:\n",
    "    v = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0: return \"n/a\"\n",
    "    return f\"{v.mean():.6f} ± {v.std(ddof=0):.6f}\"\n",
    "\n",
    "metrics = [\n",
    "    (\"s3_noisy_baseline_mse\", \"noisy_baseline_mse (mean±std)\"),\n",
    "    (\"s3_best_val_mse\",       \"best_val_mse (mean±std)\"),\n",
    "    (\"s3_final_val_mse\",      \"final_val_mse (mean±std)\"),\n",
    "    (\"s3_best_delta_pct\",     \"best_delta_pct (mean±std)\"),\n",
    "    (\"s3_final_delta_pct\",    \"final_delta_pct (mean±std)\"),\n",
    "    (\"s1_best_val\",           \"s1_best_val (mean±std)\"),\n",
    "]\n",
    "\n",
    "grp = df.groupby(\"n_layers\", dropna=False)\n",
    "summary = pd.DataFrame({\"runs\": grp.size()})\n",
    "for col, label in metrics:\n",
    "    if col in df.columns and np.isfinite(df[col]).any():\n",
    "        summary[label] = grp[col].apply(mean_std_safe)\n",
    "\n",
    "summary_path = f\"{OUT_BASE}/summary_by_layers_{CSV_SCHEMA_VERSION}.csv\"\n",
    "summary.to_csv(summary_path, index=True)\n",
    "print(f\"Saved per-layer summary → {summary_path}\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
