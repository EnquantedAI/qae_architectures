# Jacob's Half-QAE Notebook

This folder contains a fully portable and robust implementation of the Half-QAE (Quantum Autoencoder) for time series analysis.

## ğŸš€ Quick Start

1. **Navigate to this folder**:
   ```bash
   cd qae_architectures/Jacob/
   ```

2. **Open the notebook**:
   ```bash
   jupyter notebook replicating_qae_Jacob.ipynb
   ```
   Or open it directly in VS Code.

3. **Run all cells** - everything should work out of the box!

## ğŸ“ Project Structure Expected

```
qae_architectures/
â”œâ”€â”€ Jacob/
â”‚   â”œâ”€â”€ replicating_qae_Jacob.ipynb  â† Main notebook (this folder)
â”‚   â”œâ”€â”€ qae_utils_stubs.py          â† IDE support file
â”‚   â””â”€â”€ README.md                   â† This file
â”œâ”€â”€ jacobs_examples/aintern/data/   â† Data files (generated by data notebooks)
â”œâ”€â”€ qae_utils/                      â† Utility functions (Files.py, etc.)
â””â”€â”€ requirements.txt                â† Python dependencies
```

## ğŸ”§ Features

- **Fully Portable**: Works on any machine, no hardcoded paths
- **Robust Error Handling**: Clear error messages and debugging info
- **IDE Support**: Proper type hints and IntelliSense support
- **Best Practices**: Includes train/test splitting, proper evaluation metrics
- **Comprehensive Analysis**: Latent vs trash space information analysis

## ğŸ“Š Data Requirements

Before running this notebook, ensure you have generated the Mackey-Glass data:

1. Run the data generation notebook first:
   - Located in `jacobs_examples/` folder
   - This creates the required data files in `jacobs_examples/aintern/data/`

2. The notebook expects:
   - `mackey_glass_n*/x_org.arr` (input sequences)
   - `mackey_glass_n*/y_org.arr` (target values)  
   - `mackey_glass_n*/info.json` (metadata)

## ğŸ§  Quantum Architecture

- **Total Qubits**: 4
- **Latent Space**: 2 qubits (compressed representation)
- **Trash Space**: 2 qubits (discarded information)
- **Architecture**: Half-QAE with encoder-decoder structure

## ğŸ“ˆ Analysis Features

The notebook includes:

1. **Data Preparation**: Sliding window creation, train/test splitting
2. **Model Training**: Quantum circuit parameter optimization
3. **Evaluation Metrics**: Reconstruction error, latent space analysis
4. **Visualizations**: Loss curves, reconstruction quality, information flow
5. **Best Practices**: Comprehensive comparison between train/test performance


## ğŸ¤ Contributing

This notebook serves as a template for quantum machine learning research. Feel free to:

- Extend the architecture (more qubits, layers, etc.)
- Try different datasets
- Experiment with circuit designs
- Add new evaluation metrics
- Improve visualization techniques

The code is designed to be educational and research-friendly!
